{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This example covers creation and training of ME-NeuralFMUs\n",
    "\n",
    "## Motivation\n",
    "This Julia Package is motivated by the application of hybrid modeling. This package enables the user to integrate his simulation model between neural networks (NeuralFMU). For this, the simulation model must be exported as FMU (functional mock-up unit), which corresponds to a widely used standard. The big advantage of a hybrid modeling with artificial neural networks is, that effects, that are difficult to model, can be easily learned by the neural networks. For this purpose, the so-called NeuralFMU is trained with measurement data containing the effect. The final product is a simulation with the mapping of complex effects. Another big advantage of the NeuralFMU is that it works with relatively little data, because the FMU already contains the rough functionality of the simulation and only the missing effects are added.\n",
    "\n",
    "\n",
    "## Introduction to the example\n",
    "In this example, a simplified modeling of a one-dimensional spring pendulum (without friction) is compared to a model of the same system that includes friction. The FMU with the simplified modeling will be named *simpleFmu* in the following and the modeling with the friction will be named *realFmu*. At the beginning, the actual state of both simulations is shown, whereby clear deviations can be seen in the graphs. The *realFmu* serves as a reference graph. The *simpleFmu* is then integrated into a NeuralFMU architecture and a training of the entire network is performed. After the training the final state is compared again to the *realFmu*. It can be clearly seen that by using the NeuralFMU, learning of the friction process has taken place.  \n",
    "\n",
    "\n",
    "## Target group\n",
    "The example is primarily intended for users who work in the field of model building and are interested in hybrid model building. The example wants to show how simple it is to combine FMUs with machine learning and to illustrate the advantages of this approach.\n",
    "\n",
    "\n",
    "## License\n",
    "\n",
    "Copyright (c) 2021 Tobias Thummerer, Lars Mikelsons, Johannes Stoljar\n",
    "\n",
    "Licensed under the MIT license. See LICENSE file in the project root for details.\n",
    "\n",
    "\n",
    "## Getting started\n",
    "\n",
    "### Installation prerequisites\n",
    "|    | Description                       | Command     |  Alternative  |   \n",
    "|:--- |:---                               |:---        |:---|\n",
    "|1.  | Enter Package Manager via         |     ]       |     |\n",
    "|2.  | Install FMI via                   |   add FMI   | add \" https://github.com/ThummeTo/FMI.jl \"   |\n",
    "|3.  | Install FMIFlux via               | add FMIFlux | add \" https://github.com/ThummeTo/FMIFlux.jl \" |\n",
    "|4.  | Install Flux via                  |  add Flux   |     |\n",
    "|5.  | Install DifferentialEquations via | add DifferentialEquations |  |\n",
    "|6.  | Install Plots via                 | add Plots   |     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code section\n",
    "\n",
    "To run the example, the previously installed packages must be included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "using FMI\n",
    "using FMIFlux\n",
    "using Flux\n",
    "using DifferentialEquations: Tsit5\n",
    "import Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the packages, the path to the FMUs (FMU = functional mock-up unit) is set. The FMU is a model from the Functional Mock-up Interface (FMI) Standard. The FMI is a free standard that defines a container and an interface to exchange dynamic models using a combination of XML files, binaries and C code zipped into a single file. Here the path for the [*SpringPendulum1D*](../model/) and the [*SpringFrictionPendulum1D*](../model/) model is set. The structure of the *SpringPendulum1D* (*simpleFmu*) can be seen in the following graphic and corresponds to a simple modeling.\n",
    "\n",
    "<img src=\"./pics/SpringPendulum1D.png\" alt=\"\" width=\"300\"/>\n",
    "\n",
    "\n",
    "In contrast, the model *SpringFrictionPendulum1D* (*realFmu*) is somewhat more accurate, because it includes a friction component. \n",
    "\n",
    "<img src=\"./pics/SpringFrictionPendulum1D.png\" alt=\"\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "simpleFmuPath = joinpath(dirname(@__FILE__), \"../model/SpringPendulum1D.fmu\")\n",
    "realFmuPath = joinpath(dirname(@__FILE__), \"../model/SpringFrictionPendulum1D.fmu\")\n",
    "println(\"SimpleFmu path: \", simpleFmuPath)\n",
    "println(\"RealFmu path: \", realFmuPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the start time and end time of the simulation are set. Finally, a step size is specified to store the results of the simulation at these time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tStart = 0.0\n",
    "tStep = 0.01\n",
    "tStop = 5.0\n",
    "tSave = collect(tStart:tStep:tStop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RealFmu\n",
    "\n",
    "In the next lines of code the FMU of the *realFMU* model is loaded and instantiated. Both the start and end time are set via the *fmiSetupExperiment()* function.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realFmu = fmiLoad(realFmuPath)\n",
    "fmiInstantiate!(realFmu; loggingOn=false)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask Tobi !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmiSetupExperiment(realFmu, tStart, tStop)\n",
    "\n",
    "fmiEnterInitializationMode(realFmu)\n",
    "fmiExitInitializationMode(realFmu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get all continuous states of a FMU by the function *fmiGetContinuousStates()* and this is also done for the *realFmu*. It has two states. The first state is the position of the mass, which here is $0.5m$. The second state is the initial velocity, which is $0\\frac{m}{s}$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xâ‚€ = fmiGetContinuousStates(realFmu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block the *realFmu* is simulated, still specifying which variables are included. After the simulation is finished the result of the *realFmu* can be plotted. This plot also serves as a reference for the other model (*simpleFmu*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrs = [\"mass.s\", \"mass.v\", \"mass.a\", \"mass.f\"]\n",
    "_, realSimData = fmiSimulate(realFmu, tStart, tStop; recordValues=vrs, saveat=tSave, setup=false, reset=false)\n",
    "fmiPlot(realFmu, vrs, realSimData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " After the plots are created, the FMU is unloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmiUnload(realFmu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from the simualtion of the *realFmu*, are divided into position and velocity data. These data will be needed later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velReal = collect(data[2] for data in realSimData.saveval)\n",
    "posReal = collect(data[1] for data in realSimData.saveval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleFmu\n",
    "\n",
    "The following lines also load, instantiate, simulate and plot the *simpleFmu*. The differences can be clearly seen from the plots. In the plots for the *realFmu* it can be seen that the oscillation continues to decrease due to the effect of the friction. If you would simulate here long enough, the oscillation would come to a standstill in a certain time. The oscillation in the *simpleFmu* behaves differently, since the friction was not taken into account here. The oscillation in this model would continue to infinity with the same oscillation amplitude. From this observation the desire of an improvement of this model arises.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleFmu = fmiLoad(simpleFmuPath)\n",
    "\n",
    "fmiInstantiate!(simpleFmu; loggingOn=false)\n",
    "\n",
    "vrs = [\"mass.s\", \"mass.v\", \"mass.a\"]\n",
    "_, simpleSimData = fmiSimulate(simpleFmu, tStart, tStop; recordValues=vrs, saveat=tSave, reset=false)\n",
    "fmiPlot(simpleFmu, vrs, simpleSimData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from the simualtion of the *simpleFmu*, are divided into position and velocity data. These data will be needed later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velSimple = collect(data[2] for data in simpleSimData.saveval)\n",
    "posSimple = collect(data[1] for data in simpleSimData.saveval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralFMU\n",
    "\n",
    "It is not always as easy as in this example to integrate friction into a model. This gave rise to the idea of learning such effects or other effects that are difficult to model by neural networks. The crucial difference is to insert known model knowledge into the neural network in the form of an FMU. This kind of architecture is called NeuralFMU and is shown in the following picture.\n",
    "\n",
    "![](./pics/NeuralFMU.png)\n",
    "\n",
    "The advantage of such an architecture is that the neural network does not have to learn all correlations, but only the missing effects. Thus it gets along with a small amount of data.\n",
    "\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "In order to build such an architecture, a loss function must be implemented. The solver of the NeuralFMU can calculate the gradient descent with the loss function. The gradient descent is needed to adjust the weights in the neural network so that the sum of the error is reduced.\n",
    "\n",
    "The loss function in this implmentation consists of the mean squared error (mse) from the real position of the *realFmu* simulation (posReal) and the position data of the network (posNet).\n",
    "$$ mse = \\frac{1}{n} \\sum\\limits_{i=0}^n (posReal[i] - posNet[i])^2 $$\n",
    "\n",
    "As it is indicated with the comments, one could also additionally consider the mse from the real velocity (velReal) and the velocity from the network (velNet). The error in this case would be calculated from the sum of both errors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for training\n",
    "function lossSum()\n",
    "    solution = neuralFmu(xâ‚€, tStart)\n",
    "\n",
    "    posNet = collect(data[1] for data in solution.u)\n",
    "    #velNet = collect(data[2] for data in solution.u)\n",
    "\n",
    "    Flux.Losses.mse(posReal, posNet) #+ Flux.Losses.mse(velReal, velNet)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callback\n",
    "\n",
    "To output the loss in certain intervals, a callback is implemented as a function in the following. Here a counter is added with each call by one and every tenth pass the loss function is called and the average error is printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback function for training\n",
    "global counter = 0\n",
    "function callb()\n",
    "    global counter += 1\n",
    "\n",
    "    if counter % 10 == 1\n",
    "        avgLoss = lossSum()\n",
    "        @info \"Loss [$counter]: $(round(avgLoss, digits=5))   Avg displacement in data: $(round(sqrt(avgLoss), digits=5))\"\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure of the NeuralFMU\n",
    "\n",
    "In the following, the network of the NeuralFMU is constructed. It consists of an input layer, which then leads into the *simpleFmu* model, where only one timestep in the FMU is determined. Following the *simpleFmu* is a dense layer that has exactly as many inputs as the model has states. The output of this layer consists of 16 output nodes and a *tanh* activation function. The next layer has 16 input and output nodes with the same activation function. The last layer is again a dense layer with 16 input nodes and the number of states as outputs. Here it is important that no activation function follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeuralFMU setup\n",
    "numStates = fmiGetNumberOfStates(simpleFmu)\n",
    "\n",
    "net = Chain(inputs -> fmiDoStepME(simpleFmu, inputs),\n",
    "            Dense(numStates, 16, tanh),\n",
    "            Dense(16, 16, tanh),\n",
    "            Dense(16, numStates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the NeuralFMU\n",
    "\n",
    "The definition of the Model-Exchange-NeuralFMU is done as a one-liner. The FMU (*simpleFmu*), the structure of the network (net), start and end time, the numerical solver (Tsit5()) and the time steps for saving are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralFmu = ME_NeuralFMU(simpleFmu, net, (tStart, tStop), Tsit5(); saveat=tSave)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot before training\n",
    "\n",
    "Here the start state of the *simpleFmu* is plotted to compare it later with the plot after the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionBefore = neuralFmu(xâ‚€, tStart)\n",
    "fmiPlot(simpleFmu, solutionBefore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training of the NeuralFMU\n",
    "\n",
    "For the training of the NeuralFMU the parameters are extracted. The known ADAM optimizer for minimizing the gradient descent is used as further passing parameters. In addition, the previously defined loss and callback function, as well as the number of epochs are passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train it ...\n",
    "paramsNet = Flux.params(neuralFmu)\n",
    "\n",
    "optim = ADAM()\n",
    "# Feel free to increase training steps or epochs for better results\n",
    "Flux.train!(lossSum, paramsNet, Iterators.repeated((), 300), optim; cb=callb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of the plots\n",
    "\n",
    "Here three plots are compared with each other and only the position of the mass is considered. The first plot represents the *simpleFmu*, the second represents the *realFmu* (reference) and the third plot represents the result after training the NeuralFMU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### plot results mass.s\n",
    "solutionAfter = neuralFmu(xâ‚€, tStart)\n",
    "\n",
    "fig = Plots.plot(xlabel=\"t [s]\", ylabel=\"mass position [m]\", linewidth=2,\n",
    "                 xtickfontsize=12, ytickfontsize=12,\n",
    "                 xguidefontsize=12, yguidefontsize=12,\n",
    "                 legendfontsize=12, legend=:bottomright)\n",
    "\n",
    "posNeuralFmu = collect(data[1] for data in solutionAfter.u)\n",
    "\n",
    "Plots.plot!(fig, tSave, posSimple, label=\"SimpleFMU\", linewidth=2)\n",
    "Plots.plot!(fig, tSave, posReal, label=\"RealFMU\", linewidth=2)\n",
    "Plots.plot!(fig, tSave, posNeuralFmu, label=\"NeuralFMU\", linewidth=2)\n",
    "# Plots.savefig(fig, \"exampleResult_s.pdf\")\n",
    "fig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmiUnload(simpleFmu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Based on the plots, it can be seen clearly that the NeuralFMU is able to learn the friction from the *realFmu*. After 300 runs, the curves do not exactly overlap, but this can be achieved by longer training."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "037537ff7419c497b9325f7d495147943224d408cf5d5ed915294a5b960167b0"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": "false",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "MyJuliaKernel 1.6.1",
   "language": "julia",
   "name": "myjuliakernel-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
