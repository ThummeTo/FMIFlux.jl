var documenterSearchIndex = {"docs":
[{"location":"contents/","page":"Contents","title":"Contents","text":"Depth = 2","category":"page"},{"location":"library/#library","page":"Library Functions","title":"Library Functions","text":"","category":"section"},{"location":"library/","page":"Library Functions","title":"Library Functions","text":"","category":"page"},{"location":"library/#FMIFlux-functions","page":"Library Functions","title":"FMIFlux functions","text":"","category":"section"},{"location":"library/","page":"Library Functions","title":"Library Functions","text":"CS_NeuralFMU\nME_NeuralFMU\nNeuralFMU","category":"page"},{"location":"library/#FMIFlux.CS_NeuralFMU","page":"Library Functions","title":"FMIFlux.CS_NeuralFMU","text":"Structure definition for a NeuralFMU, that runs in mode Co-Simulation (CS).\n\n\n\n\n\n","category":"type"},{"location":"library/#FMIFlux.ME_NeuralFMU","page":"Library Functions","title":"FMIFlux.ME_NeuralFMU","text":"Structure definition for a NeuralFMU, that runs in mode Model Exchange (ME).\n\n\n\n\n\n","category":"type"},{"location":"library/#FMIFlux.NeuralFMU","page":"Library Functions","title":"FMIFlux.NeuralFMU","text":"The mutable struct representing an abstract (simulation mode unknown) NeuralFMU.\n\n\n\n\n\n","category":"type"},{"location":"library/#FMI-2-version-dependent-functions","page":"Library Functions","title":"FMI 2 version dependent functions","text":"","category":"section"},{"location":"library/","page":"Library Functions","title":"Library Functions","text":"fmi2DoStepCS\nfmi2EvaluateME\nfmi2InputDoStepCSOutput","category":"page"},{"location":"library/#FMIFlux.fmi2EvaluateME","page":"Library Functions","title":"FMIFlux.fmi2EvaluateME","text":"DEPRECATED:\n\nPerforms something similar to fmiDoStep for ME-FMUs (note, that fmiDoStep is for CS-FMUs only). Event handling (state- and time-events) is supported. If you don't want events to be handled, you can disable event-handling for the NeuralFMU nfmu with the attribute eventHandling = false.\n\nOptional, additional FMU-values can be set via keyword arguments setValueReferences and setValues. Optional, additional FMU-values can be retrieved by keyword argument getValueReferences.\n\nFunction takes the current system state array (\"x\") and returns an array with state derivatives (\"x dot\") and optionally the FMU-values for getValueReferences. Setting the FMU time via argument t is optional, if not set, the current time of the ODE solver around the NeuralFMU is used.\n\n\n\n\n\n","category":"function"},{"location":"library/#FMIFlux.fmi2InputDoStepCSOutput","page":"Library Functions","title":"FMIFlux.fmi2InputDoStepCSOutput","text":"DEPRECATED:\n\nfmi2InputDoStepCSOutput(comp::FMU2Component, \n                        dt::Real, \n                        u::Array{<:Real})\n\nSets all FMU inputs to u, performs a ´´´fmi2DoStep´´´ and returns all FMU outputs.\n\n\n\n\n\n","category":"function"},{"location":"library/#FMI-version-independent-functions","page":"Library Functions","title":"FMI version independent functions","text":"","category":"section"},{"location":"library/","page":"Library Functions","title":"Library Functions","text":"fmiDoStepCS\nfmiEvaluateME\nfmiInputDoStepCSOutput","category":"page"},{"location":"library/#FMIFlux.fmiDoStepCS","page":"Library Functions","title":"FMIFlux.fmiDoStepCS","text":"DEPRECATED:\n\nWrapper. Call fmi2DoStepCS for more information.\n\n\n\n\n\n","category":"function"},{"location":"library/#FMIFlux.fmiEvaluateME","page":"Library Functions","title":"FMIFlux.fmiEvaluateME","text":"DEPRECATED:\n\nWrapper. Call fmi2EvaluateME for more information.\n\n\n\n\n\n","category":"function"},{"location":"library/#FMIFlux.fmiInputDoStepCSOutput","page":"Library Functions","title":"FMIFlux.fmiInputDoStepCSOutput","text":"DEPRECATED:\n\nWrapper. Call fmi2InputDoStepCSOutput for more information.\n\n\n\n\n\n","category":"function"},{"location":"library/#Additional-functions","page":"Library Functions","title":"Additional functions","text":"","category":"section"},{"location":"library/","page":"Library Functions","title":"Library Functions","text":"mse_interpolate\ntransferParams!","category":"page"},{"location":"library/#FMIFlux.mse_interpolate","page":"Library Functions","title":"FMIFlux.mse_interpolate","text":"Compares non-equidistant (or equdistant) datapoints by linear interpolating and comparing at given interpolation points t_comp.  (Zygote-friendly: Zygote can differentiate through via AD.)\n\n\n\n\n\n","category":"function"},{"location":"related/#Related-Publications","page":"Related Publication","title":"Related Publications","text":"","category":"section"},{"location":"related/","page":"Related Publication","title":"Related Publication","text":"Thummerer T, Kircher J and Mikelsons L: Neural FMU: Towards structual integration of FMUs into neural networks (Preprint, accepted 14th International Modelica Conference) pdf|DOI","category":"page"},{"location":"related/","page":"Related Publication","title":"Related Publication","text":"Thummerer T, Tintenherr J, Mikelsons L: Hybrid modeling of the human cardiovascular system using NeuralFMUs (Preprint, accepted 10th International Conference on Mathematical Modeling in Physical Sciences) pdf|DOI","category":"page"},{"location":"examples/mdpi_2022/#Physics-enhanced-NeuralODEs-in-real-world-applications","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Tutorial by Tobias Thummerer based on the paper NeuralFMU: presenting a workflow for integrating hybrid NeuralODEs into real-world applications","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"(Work in progress, last edit: 03.01.2023)","category":"page"},{"location":"examples/mdpi_2022/#Keywords","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Keywords","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"PeNODE, NeuralODE, Universal Differential Equation, Hybrid Modeling, Functional Mock-up Unit, FMU, NeuralFMU","category":"page"},{"location":"examples/mdpi_2022/#License","page":"Physics-enhanced NeuralODEs in real-world applications","title":"License","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# Copyright (c) 2021 Tobias Thummerer, Lars Mikelsons\r\n# Licensed under the MIT license. \r\n# See LICENSE (https://github.com/thummeto/FMIFlux.jl/blob/main/LICENSE) file in the project root for details.","category":"page"},{"location":"examples/mdpi_2022/#Introduction-to-the-example","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Introduction to the example","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"The Julia Package FMIFlux.jl is motivated by the application of hybrid modeling. This package enables the user to integrate his simulation model(s) in form of FMU(s) side-by-side together with artifical neural networks. For more detailed information on FMIFlux.jl, see the introduction page. This tutorial is an easy, code-focussed version of the paper [1]. If this tutorial is useful for your work, please cite the linked paper.","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"In this example, a real-world simulation model is enhanced in terms of accuracy using a so called physics-enhanced neural ordinary differential equation (PeNode). Basically, this is an extension to the NeuralODE concept and looks as can be seen in Fig. 1.","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"(Image: NeuralFMU.svg)","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Fig.1: A possible representation for a physics-enhanced neural ordinary differential equation (PeNODE). PeNODEs that include FMUs instead of symbolic ODEs are called NeuralFMUs.","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Note, that this is only one possible topology of a PeNODE, there could be an additional artifical neural network (ANN) before the ODE (that can also share some connections with the other ANN) and other signals, like FMU state derivatives, inputs and outputs could be connected to the ANN(s). ODEs are in general not very handy, for modeling real applications, a more suitable container for ODEs is needed. The most common model exchange format in industry is the Functional Mock-up interface (FMI), models exported with FMI are called Functional Mock-up unit (FMU). Especially model-exchange FMUs can be seen as containers for ODEs. For more information, see (fmi-standard.org). So if you want to use a real model from your modeling tool (with FMI support), you can simply export an FMU instead of handling large and bulky ODEs. If PeNODEs use FMUs instead of ODEs, they are called NeuralFMUs [2].","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"In this tutorial series, a vehicle longitudinal-dynamics model (VLDM) is extended to a NeuralFMU (PeNODE) to make better consumption predictions.","category":"page"},{"location":"examples/mdpi_2022/#Target-group","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Target group","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"The example is primarily intended for users who work in the field of first principle and/or data driven modeling and are further interested in hybrid model building. The example shows how to combine FMUs with machine learning and illustrates the advantages of this approach.","category":"page"},{"location":"examples/mdpi_2022/#Other-formats","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Other formats","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Besides, this Jupyter Notebook there is also a Julia file with the same name, which contains only the code cells. For the documentation there is a Markdown file corresponding to the notebook.  ","category":"page"},{"location":"examples/mdpi_2022/#Getting-started","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Getting started","text":"","category":"section"},{"location":"examples/mdpi_2022/#Installation-prerequisites","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Installation prerequisites","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":" Description Command\n1. Enter Package Manager via ]\n2. Install FMI via add FMI\n3. Install FMIFlux via add FMIFlux\n4. Install FMIZoo via add FMIZoo\n5. Install Plots via add Plots\n6. Install PlotlyJS via add PlotlyJS\n7. Install Random via add Random\n8. Install JLD2 via add JLD2","category":"page"},{"location":"examples/mdpi_2022/#Part-1:-Loading-the-FMU","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Part 1: Loading the FMU","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"To run the example, the previously installed packages must be included. ","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# Loading in the required libraries\r\nusing FMIFlux   # for NeuralFMUs\r\nusing FMI       # import FMUs into Julia \r\nusing FMIZoo    # a collection of demo models, including the VLDM\r\n\r\nimport FMI.DifferentialEquations: Tsit5     # import the Tsit5-solver\r\nusing JLD2                                  # data format for saving/loading parameters\r\n\r\n# plotting\r\nimport Plots        # default plotting framework\r\nimport PlotlyJS     # plotting (interactive)\r\nPlots.plotlyjs()    # actiavte PlotlyJS as default plotting backend\r\n\r\n# Let's fix the random seed to make our program determinsitic (ANN layers are initialized indeterminsitic otherwise)\r\nimport Random \r\nRandom.seed!(1234)\r\n\r\n# we use the Tsit5 solver for ODEs here \r\nsolver = Tsit5()    ","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Next, we load the FMU from the FMIZoo.jl and have a brief look on its metadata. For a detailed view, see the Modelica model.","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# load our FMU (we take one from the FMIZoo.jl, exported with Dymola 2022x)\r\nfmu = fmiLoad(\"VLDM\", \"Dymola\", \"2022x\"; type=:ME, logLevel=FMI.FMIImport.FMULogLevelInfo)\r\n\r\n# let's have a look on the model meta data\r\nfmiInfo(fmu)","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"As you can see, in section States there are six states listed:","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"ID Value Reference Value Name(s) Description\n1 33554432 \"driver.accelerationPedalController.PI.x\" PI-Controller state (integrated error), accelerating\n2 33554433 \"driver.brakePedalController.PI.x\" PI-Controller state (integrated error), braking\n3 33554434 \"drivingCycle.s\" vehicle position (target)\n4 33554435 \"dynamics.accelerationCalculation.integrator.y\" vehicle position (actual)\n5 33554436 \"dynamics.accelerationCalculation.limiter.u\", \"dynamics.accelerationCalculation.limIntegrator.y\" vehicle velocity (actual)\n6 33554437 \"result.integrator.y\" cumulative consumption * 3600","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Next thing is having a look on the real measurement data, that comes with the FMU. The VLDM and corresponding data are based on the Component Library for Full Vehicle Simulations [3].","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# load data from FMIZoo.jl, gather simulation parameters for FMU\r\ndata = FMIZoo.VLDM(split=:train)\r\ntStart = data.consumption_t[1]\r\ntStop = data.consumption_t[end]\r\ntSave = data.consumption_t\r\n\r\n# have a look on the FMU parameters (these are the file paths to the characteristic maps)\r\ndata.params","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Finally, we do a single simulation run and compare the simulation output to the real data.","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"\r\n# let's run a simulation from `tStart` to `tStop`, use the parameters we just viewed for the simulation run\r\nresultFMU = fmiSimulate(fmu, (tStart, tStop); parameters=data.params) \r\nfig = fmiPlot(resultFMU)                                                                        # Plot it, but this is a bit too much, so ...\r\nfig = fmiPlot(resultFMU; stateIndices=6:6)                                                      # ... only plot the state #6 and ...\r\nfig = fmiPlot(resultFMU; stateIndices=6:6, ylabel=\"Cumulative consumption [Ws]\", label=\"FMU\")   # ... add some helpful labels!\r\n\r\n# further plot the (measurement) data values `consumption_val` and deviation between measurements `consumption_dev`\r\nPlots.plot!(fig, data.consumption_t, data.consumption_val; label=\"Data\", ribbon=data.consumption_dev, fillalpha=0.3)","category":"page"},{"location":"examples/mdpi_2022/#Part-2:-Designing-the-Topology","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Part 2: Designing the Topology","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"After we have successful loaded the FMI, had a look inside its model description and simulated it, we want to build a hybrid model (NeuralFMU) with our FMU as its core. ","category":"page"},{"location":"examples/mdpi_2022/#Part-2a:-Interfaces-between-ANNs-and-FMUs","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Part 2a: Interfaces between ANNs and FMUs","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"What happens between ANNs and FMUs?","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Note, that FMUs and ANNs don't operate within the same numerical ranges. Whereas FMU signals can basically use the entire range of a Float64, ANNs operate the best in a range that suits theire activation functions. Many activation functions saturate their input values. Consider the tanh-activation, that acts almost linear close around 0, but drastically saturates values further away from zero:","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"for i in [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\r\n    println(\"tanh($(i)) = $(tanh(i))\")\r\nend","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Similarly for the opposite direction: Signals from ANNs into FMUs may be too small, because of the limited output of the ANNs. To prevent this issue, an appropriate transformation (like shifting and scaling) between ANNs and FMUs is necessary. In the following code section, the results of ignoring this is shown, together with a fix by using the provided ScaleShift- and ShiftScale-layers from FMIFlux.jl.","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# variable we want to manipulate - why we are picking exactly these three is shown a few lines later ;-)\r\nmanipulatedDerVars = [\"der(dynamics.accelerationCalculation.integrator.y)\",\r\n                      \"der(dynamics.accelerationCalculation.limIntegrator.y)\",\r\n                      \"der(result.integrator.y)\"]\r\n# alternative: manipulatedDerVars = fmu.modelDescription.derivativeValueReferences[4:6]\r\n\r\n# reference simulation to record the derivatives \r\nresultFMU = fmiSimulate(fmu, (tStart, tStop), parameters=data.params, recordValues=:derivatives, saveat=tSave) # [29s]\r\nvals = fmiGetSolutionValue(resultFMU, manipulatedDerVars)\r\n\r\n# what happens without propper transformation between FMU- and ML-domain?\r\nPlots.plot(resultFMU.values.t, vals[1,:][1]; label=\"vehicle velocity\");\r\nPlots.plot!(resultFMU.values.t, tanh.(vals[1,:][1]); label=\"tanh(velocity)\")\r\n\r\n# setup shift/scale layers for pre-processing\r\npreProcess = ShiftScale(vals)\r\n\r\n# check what it's doing now ...\r\ntestVals = collect(preProcess(collect(val[t] for val in vals))[1] for t in 1:length(resultFMU.values.t))\r\nPlots.plot(resultFMU.values.t, testVals; label=\"velocity (pre-processed)\");\r\nPlots.plot!(resultFMU.values.t, tanh.(testVals); label=\"tanh(velocity)\")","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# add some additional \"buffer\"\r\npreProcess.scale[:] *= 0.5 \r\ntestVals = collect(preProcess(collect(val[t] for val in vals))[1] for t in 1:length(resultFMU.values.t))\r\nPlots.plot(resultFMU.values.t, testVals; label=\"velocity (pre-processed)\");\r\nPlots.plot!(resultFMU.values.t, tanh.(testVals); label=\"tanh(velocity)\")","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# ... also check the consumption\r\ntestVals = collect(preProcess(collect(val[t] for val in vals))[3] for t in 1:length(resultFMU.values.t))\r\nPlots.plot(resultFMU.values.t, testVals; label=\"vehicle consumption (pre-processed)\");\r\nPlots.plot!(resultFMU.values.t, tanh.(testVals); label=\"tanh(consumption)\")","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# setup scale/shift layer (inverse transformation) for post-processing\r\n# we don't an inverse transform for the entire preProcess, only for the second element (acceleration)\r\npostProcess = ScaleShift(preProcess; indices=2:2)","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"As a little extra, these shifting and scaling parameters are optimized together with the ANN parameters in the later training process!","category":"page"},{"location":"examples/mdpi_2022/#Part-2b:-ANN-in-and-output","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Part 2b: ANN in- and output","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Besides shifting and scaling, a major question is: What signals should be fed into the ANN and what signals should be output by it? We need to make some considerations:","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"What should be learned? In theory, we could manipulate every interface signal from an to the FMU, but in real life this is not efficient. The more signals we connect, the more partial derivatives need to be determined during training. So if there is something you know about the model (and often there is more than just something) you can use that knowledge to make your hybrid model more efficient. So ask yourself: What should be learned? And right after that: How could it be learned? In mechanical applications the answer will often be: A force (or momentum in rotational systems). Forces result in a change of acceleration, so they can be expressed by an additional acceleration. In mechanical systems, the acceleration is almost always a state derivative (the derivative of the velocity), so in many cases, manipulating the lowest state derivative - the acceleration - is a very good choice. \nHow many ANNs do we need? Technically, you can add an arbitrary number of ANNs around your FMU (you can also use multiple FMUs if you want). But again, one should not use more than a single ANN if there is no good reason to do so. A second ANN before the FMU can be useful for example, if measurment offsets or similar effects should be corrected. Often, a single ANN to modify the state dynamics is sufficient.\nWhat signals from the FMU should be inserted into the ANN? In theory, we could use them all, meaning all states, state derivatives, time, inputs, outputs and other variables that are accessible through FMI. You know what comes next: Using less signals is the better choice, of course. If you know that the physical effect (here: the friction force), you have also an idea what influences this effect or at least you know what values will have no impact and can be neglected to enhance training performance.","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"At this specific example, the following considerations were made:","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":" Consideration Considered inputs for the ANN\n1. we have no inputs, nothing to add \n2. we have no outputs, nothing to add \n3. we have six states, that may influence the effect, add them x_1 x_2 x_3 x_4 x_5 x_6\n4. we have six state derivatives, that may influence the effect, add them x_1 x_2 x_3 x_4 x_5 x_6 dotx_1 dotx_2 dotx_3 dotx_4 dotx_5 dotx_6\n5. the system is modelled as second order ODE, the state x_5 (velocity) equlas the state derivative dotx_4, remove x_5 x_1 x_2 x_3 x_4 x_6 dotx_1 dotx_2 dotx_3 dotx_4 dotx_5 dotx_6\n6. we know that the friction effect is not dependent on the driver controller, remove two states (x_1 and x_2) and state derivatives (dotx_1 and dotx_2) x_3 x_4 x_6 dotx_3 dotx_4 dotx_5 dotx_6\n7. we know that the friction effect is not dependent on the target driving cycle position x_3 or velocity dotx_3, remove them x_4 x_6 dotx_4 dotx_5 dotx_6\n8. we assume that the friction effect is not dependent on the vehicle position x_4, remove x_4 x_6 dotx_4 dotx_5 dotx_6\n9. we assume that the friction effect is not dependent on the accumulated vehicle consumption x_6, remove x_6 dotx_4 dotx_5 dotx_6","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"These considerations lead to the following topology:","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"(Image: usedneuralfmu.svg)","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Fig.2: The used topology for a NeuralFMU in this example. The ANN operates based on the signals dotx_4 dotx_5 dotx_6 from the FMU, but only modifies the signal hatdotx_5.","category":"page"},{"location":"examples/mdpi_2022/#Part-2c:-Translating-topology-to-Julia","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Part 2c: Translating topology to Julia","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"To implement the signal bypass in a layer sequence, two layers named CacheLayer and CacheRetrieveLayer are used to cache and retrieve arbitrary values:","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# setup cache layers \r\ncache = CacheLayer()\r\ncacheRetrieve = CacheRetrieveLayer(cache)\r\n\r\ngates = ScaleSum([1.0, 0.0]) # signal from FMU (#1 = 1.0), signal from ANN (#2 = 0.0)\r\n\r\n# evaluate the FMU by calling it, we are only interessted in `dx` in this case\r\nfunction evalFMU(x)\r\n    y, dx = fmu(; x=x)\r\n    return dx \r\nend\r\n\r\n# setup the NeuralFMU topology\r\nnet = Chain(x -> evalFMU(x),                    # take `x`, put it into the FMU, retrieve `dx`\r\n            dx -> cache(dx),                    # cache `dx`\r\n            dx -> dx[4:6],                      # forward only dx[4, 5, 6]\r\n            preProcess,                         # pre-process `dx`\r\n            Dense(3, 32, tanh),                 # Dense Layer 3 -> 32 with `tanh` activasion\r\n            Dense(32, 1, tanh),                 # Dense Layer 32 -> 1 with `tanh` activasion \r\n            postProcess,                        # post process `dx`\r\n            dx -> cacheRetrieve(5:5, dx),       # dynamics FMU | dynamics ANN\r\n            gates,                              # compute resulting dx from ANN + FMU\r\n            dx -> cacheRetrieve(1:4, dx, 6:6))  # stack together: dx[1,2,3,4] from cache + dx from ANN + dx[6] from cache\r\n\r\n# build NeuralFMU\r\nneuralFMU = ME_NeuralFMU(fmu, net, (tStart, tStop), solver; saveat=tSave)\r\nneuralFMU.modifiedState = false # speed optimization (no ANN before the FMU)\r\n\r\n# get start state vector from data (FMIZoo)\r\nx0 = FMIZoo.getStateVector(data, tStart)\r\n\r\n# simulate and plot the (uninitialized) NeuralFMU\r\nresultNFMU_original = neuralFMU(x0, (tStart, tStop); parameters=data.params, showProgress=true) \r\nfig = fmiPlot(resultNFMU_original; stateIndices=5:5, label=\"NeuralFMU (original)\", ylabel=\"velocity [m/s]\")\r\n\r\n# plot the original FMU and data\r\nfmiPlot!(fig, resultFMU; stateIndices=5:5, values=false)\r\nPlots.plot!(fig, data.speed_t, data.speed_val, label=\"Data\")","category":"page"},{"location":"examples/mdpi_2022/#Part-2d:-Initialization","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Part 2d: Initialization","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"In general, initialization of (phyiscs-enhanced) NeuralODEs is challenging, because ANNs are initialized random by default. In this case we are using a special initialization method introducing two gates, that control how much of the original FMU dynamics and how much of the ANN dynamics is introduced to the final model dynamics. See the paper [1] for a deeper insight.","category":"page"},{"location":"examples/mdpi_2022/#Part-3:-Training","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Part 3: Training","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Finally: The actual training!","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Basically, you can use your custom loss function, batching strategies and optimsation routines with FMIFlux.jl. Because we need to keep it short here, we use some tools already shipped with FMIFlux.jl.","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# prepare training data (array of arrays required)\r\ntrain_data = collect([d] for d in data.consumption_val)\r\ntrain_t = data.consumption_t \r\n\r\n# switch to a more efficient execution configuration, allocate only a singel FMU instance, see:\r\n# https://thummeto.github.io/FMI.jl/dev/features/#Execution-Configuration\r\nfmu.executionConfig = FMI.FMIImport.FMU2_EXECUTION_CONFIGURATION_NOTHING\r\nc, _ = FMIFlux.prepareSolveFMU(neuralFMU.fmu, nothing, neuralFMU.fmu.type, true, false, false, false, true, data.params; x0=x0)\r\n\r\n# batch the data (time, targets), train only on model output index 6, plot batch elements\r\nbatch = batchDataSolution(neuralFMU, t -> FMIZoo.getStateVector(data, t), train_t, train_data;\r\n    batchDuration=10.0, indicesModel=6:6, plot=true, parameters=data.params, showProgress=true)\r\n\r\n# limit the maximum number of solver steps to 1e5 and maximum simulation/training duration to 30 minutes\r\nsolverKwargsTrain = Dict{Symbol, Any}(:maxiters => 1e5, :max_execution_duration => 10.0*60.0)\r\n\r\n# picks a modified MSE, which weights the last time point MSE with 25% and the remaining element MSE with 75%\r\n# this promotes training a continuous function, even when training on batch elements\r\nlossFct = (a, b) -> FMIFlux.Losses.mse_last_element_rel(a, b, 0.5)\r\nlossFct([1.0, 2.0], [0.0, 0.0]) # (1.0)^2 * 0.75 + (2.0)^2 * 0.25\r\n\r\n# initialize a \"worst error growth scheduler\" (updates all batch losses, pick the batch element with largest error increase)\r\nscheduler = LossAccumulationScheduler(neuralFMU, batch, lossFct; applyStep=1, plotStep=5, updateStep=5)\r\nlogLoss = false\r\nupdateScheduler = () -> update!(scheduler)\r\n#scheduler = SequentialScheduler(neuralFMU, batch)\r\n#logLoss = true \r\n\r\n# defines a loss for the entire batch (accumulate error of batch elements)\r\nbatch_loss = p -> FMIFlux.Losses.batch_loss(neuralFMU, batch; \r\n    showProgress=true, p=p, parameters=data.params, update=true, lossFct=lossFct, logLoss=true, solverKwargsTrain...) # [120s]\r\n\r\n# loss for training, take element from the worst element scheduler\r\nloss = p -> FMIFlux.Losses.loss(neuralFMU, batch; \r\n    showProgress=true, p=p, parameters=data.params, lossFct=lossFct, batchIndex=scheduler.elementIndex, logLoss=logLoss, solverKwargsTrain...)\r\n\r\ngates.scale[:] = [0.99, 0.01] \r\n\r\n# gather the parameters from the NeuralFMU\r\nparams = FMIFlux.params(neuralFMU)\r\n\r\nparams[1][end-1] = 0.99\r\nparams[1][end] = 0.01\r\n\r\n# for training, we use the Adam optimizer (with exponential decay) \r\noptim = Adam(1e-3) \r\n\r\n# initialize the scheduler \r\ninitialize!(scheduler; parameters=data.params, p=params[1], showProgress=true) # [120s]","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"Finally, the line we are waiting for so long:","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"batch_loss(params[1])\r\nFMIFlux.train!(loss, params, Iterators.repeated((), length(batch)), optim; chunk_size=length(params[1]), cb=updateScheduler) \r\n\r\noptim.eta *= 0.5\r\nFMIFlux.train!(loss, params, Iterators.repeated((), length(batch)), optim; chunk_size=length(params[1]), cb=updateScheduler) \r\nbatch_loss(params[1])\r\n\r\noptim.eta *= 0.5\r\nFMIFlux.train!(loss, params, Iterators.repeated((), length(batch)), optim; chunk_size=length(params[1]), cb=updateScheduler) \r\nbatch_loss(params[1])","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"After training, it seems a good idea to store the optimized parameters for later use:","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# save the parameters (so we can use them tomorrow again)\r\nparamsPath = joinpath(@__DIR__, \"params_$(scheduler.step)steps.jld2\")\r\nfmiSaveParameters(neuralFMU, paramsPath)\r\n\r\n# switch back to the default execution configuration, see:\r\n# https://thummeto.github.io/FMI.jl/dev/features/#Execution-Configuration\r\nfmu.executionConfig = FMI.FMIImport.FMU2_EXECUTION_CONFIGURATION_NO_RESET\r\nFMIFlux.finishSolveFMU(neuralFMU.fmu, c, false, true)","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"The final question is: Are we better? This can be easily checked by running a simulation and compare it to the training data:","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"# Load parameters \r\n# fmiLoadParameters(neuralFMU, paramsPath)\r\n\r\n# check what had been learned by the NeuralFMU, simulate it ...\r\nresultNFMU_train = neuralFMU(x0, (tStart, tStop); parameters=data.params, showProgress=true, recordValues=manipulatedDerVars, maxiters=1e7) # [120s]\r\n\r\n# are we better?\r\nmse_NFMU = FMIFlux.Losses.mse(data.consumption_val, fmiGetSolutionState(resultNFMU_train, 6; isIndex=true))\r\nmse_FMU  = FMIFlux.Losses.mse(data.consumption_val, fmiGetSolutionState(resultFMU, 6; isIndex=true))\r\n\r\n# ... and plot it\r\nfig = fmiPlot(resultNFMU_nipt; stateIndices=6:6, stateEvents=false, label=\"NeuralFMU (NIPT)\", title=\"Training Data\");\r\nfmiPlot!(fig, resultNFMU_train; stateIndices=6:6, stateEvents=false, values=false, label=\"NeuralFMU (Train)\");\r\nfmiPlot!(fig, resultFMU; stateIndices=6:6, stateEvents=false, values=false, label=\"FMU\");\r\nPlots.plot!(fig, train_t, data.consumption_val, label=\"Data\", ribbon=data.consumption_dev, fillalpha=0.3)","category":"page"},{"location":"examples/mdpi_2022/#Part-4:-Results-discussion","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Part 4: Results discussion","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"WIP: coming soon!","category":"page"},{"location":"examples/mdpi_2022/#Source","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Source","text":"","category":"section"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"[1] Tobias Thummerer, Johannes Stoljar and Lars Mikelsons. 2022. NeuralFMU: presenting a workflow for integrating hybrid NeuralODEs into real-world applications. Electronics 11, 19, 3202. DOI: 10.3390/electronics11193202","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"[2] Tobias Thummerer, Lars Mikelsons and Josef Kircher. 2021. NeuralFMU: towards structural integration of FMUs into neural networks. Martin Sjölund, Lena Buffoni, Adrian Pop and Lennart Ochel (Ed.). Proceedings of 14th Modelica Conference 2021, Linköping, Sweden, September 20-24, 2021. Linköping University Electronic Press, Linköping (Linköping Electronic Conference Proceedings ; 181), 297-306. DOI: 10.3384/ecp21181297","category":"page"},{"location":"examples/mdpi_2022/","page":"Physics-enhanced NeuralODEs in real-world applications","title":"Physics-enhanced NeuralODEs in real-world applications","text":"[3] Danquah, B.; Koch, A.; Weis, T.; Lienkamp, M.; Pinnel, A. 2019. Modular, Open Source Simulation Approach: Application to Design and Analyze Electric Vehicles. In Proceedings of the IEEE 2019 Fourteenth International Conference on Ecological Vehicles and Renewable Energies (EVER), Monte Carlo, Monaco, 8–10 May 2019; pp. 1–8. DOI: 10.1109/EVER.2019.8813568.","category":"page"},{"location":"examples/overview/#Examples-Overview","page":"Overview","title":"Examples - Overview","text":"","category":"section"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"This section discusses the included examples of the FMIFlux.jl library. So you can execute them on your machine and get detailed information about all of the steps. If you require further information about the function calls, see library functions section. For more information related to the setup and simulation of an FMU see FMI.jl library.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"The examples are:","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"Simple CS-NeuralFMU: Showing how to train a Neural CS FMU.\nSimple ME-NeuralFMU: Showing how to train a Neural ME FMU.\nAdvanced ME-NeuralFMU: Showing how to train an Neural ME FMU in an advanced way.\nModelica Conference 2021: Showing how to train a Neural ME FMU.\nPhysics-enhanced NeuralODEs in real-world applications: An example for a NeuralODE in a real world modeling scenario.","category":"page"},{"location":"examples/simple_hybrid_ME/#Creation-and-training-of-ME-NeuralFMUs","page":"Simple ME-NeuralFMU","title":"Creation and training of ME-NeuralFMUs","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Tutorial by Johannes Stoljar, Tobias Thummerer","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Last edit: 03.01.2023","category":"page"},{"location":"examples/simple_hybrid_ME/#License","page":"Simple ME-NeuralFMU","title":"License","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# Copyright (c) 2021 Tobias Thummerer, Lars Mikelsons, Johannes Stoljar\r\n# Licensed under the MIT license. \r\n# See LICENSE (https://github.com/thummeto/FMIFlux.jl/blob/main/LICENSE) file in the project root for details.","category":"page"},{"location":"examples/simple_hybrid_ME/#Motivation","page":"Simple ME-NeuralFMU","title":"Motivation","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The Julia Package FMIFlux.jl is motivated by the application of hybrid modeling. This package enables the user to integrate his simulation model between neural networks (NeuralFMU). For this, the simulation model must be exported as FMU (functional mock-up unit), which corresponds to a widely used standard. The big advantage of hybrid modeling with artificial neural networks is, that effects that are difficult to model (because they might be unknown) can be easily learned by the neural networks. For this purpose, the NeuralFMU is trained with measurement data containing the not modeled physical effect. The final product is a simulation model including the originally not modeled effects. Another big advantage of the NeuralFMU is that it works with little data, because the FMU already contains the characteristic functionality of the simulation and only the missing effects are added.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"NeuralFMUs do not need to be as easy as in this example. Basically a NeuralFMU can combine different ANN topologies that manipulate any FMU-input (system state, system inputs, time) and any FMU-output (system state derivative, system outputs, other system variables). However, for this example a NeuralFMU topology as shown in the following picture is used.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: NeuralFMU.svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"NeuralFMU (ME) from [1].","category":"page"},{"location":"examples/simple_hybrid_ME/#Introduction-to-the-example","page":"Simple ME-NeuralFMU","title":"Introduction to the example","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In this example, simplified modeling of a one-dimensional spring pendulum (without friction) is compared to a model of the same system that includes a nonlinear friction model. The FMU with the simplified model will be named simpleFMU in the following and the model with the friction will be named realFMU. At the beginning, the actual state of both simulations is shown, whereby clear deviations can be seen in the graphs. The realFMU serves as a reference graph. The simpleFMU is then integrated into a NeuralFMU architecture and a training of the entire network is performed. After the training the final state is compared again to the realFMU. It can be clearly seen that by using the NeuralFMU, learning of the friction process has taken place.  ","category":"page"},{"location":"examples/simple_hybrid_ME/#Target-group","page":"Simple ME-NeuralFMU","title":"Target group","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The example is primarily intended for users who work in the field of first principle and/or hybrid modeling and are further interested in hybrid model building. The example wants to show how simple it is to combine FMUs with machine learning and to illustrate the advantages of this approach.","category":"page"},{"location":"examples/simple_hybrid_ME/#Other-formats","page":"Simple ME-NeuralFMU","title":"Other formats","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Besides, this Jupyter Notebook there is also a Julia file with the same name, which contains only the code cells and for the documentation there is a Markdown file corresponding to the notebook.  ","category":"page"},{"location":"examples/simple_hybrid_ME/#Getting-started","page":"Simple ME-NeuralFMU","title":"Getting started","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/#Installation-prerequisites","page":"Simple ME-NeuralFMU","title":"Installation prerequisites","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":" Description Command\n1. Enter Package Manager via ]\n2. Install FMI via add FMI\n3. Install FMIFlux via add FMIFlux\n4. Install FMIZoo via add FMIZoo\n5. Install DifferentialEquations via add DifferentialEquations\n6. Install Plots via add Plots\n7. Install Random via add Random","category":"page"},{"location":"examples/simple_hybrid_ME/#Code-section","page":"Simple ME-NeuralFMU","title":"Code section","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"To run the example, the previously installed packages must be included. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# imports\r\nusing FMI\r\nusing FMIFlux\r\nusing FMIZoo\r\nusing DifferentialEquations: Tsit5\r\nimport Plots\r\n\r\n# set seed\r\nimport Random\r\nRandom.seed!(42);","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"After importing the packages, the path to the Functional Mock-up Units (FMUs) is set. The FMU is a model exported meeting the Functional Mock-up Interface (FMI) Standard. The FMI is a free standard (fmi-standard.org) that defines a container and an interface to exchange dynamic models using a combination of XML files, binaries and C code zipped into a single file. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The object-orientated structure of the SpringPendulum1D (simpleFMU) can be seen in the following graphic and corresponds to a simple modeling.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In contrast, the model SpringFrictionPendulum1D (realFMU) is somewhat more accurate, because it includes a friction component. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Next, the start time and end time of the simulation are set. Finally, a step size is specified to store the results of the simulation at these time steps.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"tStart = 0.0\r\ntStep = 0.01\r\ntStop = 5.0\r\ntSave = collect(tStart:tStep:tStop)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"501-element Vector{Float64}:\r\n 0.0\r\n 0.01\r\n 0.02\r\n 0.03\r\n 0.04\r\n 0.05\r\n 0.06\r\n 0.07\r\n 0.08\r\n 0.09\r\n 0.1\r\n 0.11\r\n 0.12\r\n ⋮\r\n 4.89\r\n 4.9\r\n 4.91\r\n 4.92\r\n 4.93\r\n 4.94\r\n 4.95\r\n 4.96\r\n 4.97\r\n 4.98\r\n 4.99\r\n 5.0","category":"page"},{"location":"examples/simple_hybrid_ME/#RealFMU","page":"Simple ME-NeuralFMU","title":"RealFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In the next lines of code the FMU of the realFMU model from FMIZoo.jl is loaded and the information about the FMU is shown.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"realFMU = fmiLoad(\"SpringFrictionPendulum1D\", \"Dymola\", \"2022x\")\r\nfmiInfo(realFMU)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"#################### Begin information for FMU ####################\r\n\tModel name:\t\t\tSpringFrictionPendulum1D\r\n\tFMI-Version:\t\t\t2.0\r\n\tGUID:\t\t\t\t{2e178ad3-5e9b-48ec-a7b2-baa5669efc0c}\r\n\tGeneration tool:\t\tDymola Version 2022x (64-bit), 2021-10-08\r\n\tGeneration time:\t\t2022-05-19T06:54:12Z\r\n\tVar. naming conv.:\t\tstructured\r\n\tEvent indicators:\t\t24\r\n\tInputs:\t\t\t\t0\r\n\tOutputs:\t\t\t0\r\n\tStates:\t\t\t\t2\r\n\t\t33554432 [\"mass.s\"]\r\n\t\t33554433 [\"mass.v\", \"mass.v_relfric\"]\r\n\tSupports Co-Simulation:\t\ttrue\r\n\t\tModel identifier:\tSpringFrictionPendulum1D\r\n\t\tGet/Set State:\t\ttrue\r\n\t\tSerialize State:\ttrue\r\n\t\tDir. Derivatives:\ttrue\r\n\t\tVar. com. steps:\ttrue\r\n\t\tInput interpol.:\ttrue\r\n\t\tMax order out. der.:\t1\r\n\tSupports Model-Exchange:\ttrue\r\n\t\tModel identifier:\tSpringFrictionPendulum1D\r\n\t\tGet/Set State:\t\ttrue\r\n\t\tSerialize State:\ttrue\r\n\t\tDir. Derivatives:\ttrue\r\n##################### End information for FMU #####################","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In the next steps the parameters are defined. The first parameter is the initial position of the mass, which is initilized with 05𝑚. The second parameter is the initial velocity of the mass, which is initialized with 0fracms. The FMU hase two states: The first state is the position of the mass and the second state is the velocity. In the function fmiSimulate() the realFMU is simulated, still specifying the start and end time, the parameters and which variables are recorded. After the simulation is finished the result of the realFMU can be plotted. This plot also serves as a reference for the other model (simpleFMU).","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"initStates = [\"s0\", \"v0\"]\r\nx₀ = [0.5, 0.0]\r\nparams = Dict(zip(initStates, x₀))\r\nvrs = [\"mass.s\", \"mass.v\", \"mass.a\", \"mass.f\"]\r\n\r\nrealSimData = fmiSimulate(realFMU, (tStart, tStop); parameters=params, recordValues=vrs, saveat=tSave)\r\nfmiPlot(realSimData)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The data from the simulation of the realFMU, are divided into position and velocity data. These data will be needed later. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"velReal = fmi2GetSolutionValue(realSimData, \"mass.v\")\r\nposReal = fmi2GetSolutionValue(realSimData, \"mass.s\")","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"501-element Vector{Float64}:\r\n 0.5\r\n 0.5002235448486548\r\n 0.5008715291319449\r\n 0.5019478597521578\r\n 0.5034570452098334\r\n 0.5053993458877354\r\n 0.5077764240578201\r\n 0.5105886522837868\r\n 0.5138351439717114\r\n 0.5175150321322992\r\n 0.521627087567517\r\n 0.5261682148972211\r\n 0.5311370185654775\r\n ⋮\r\n 1.0657564963384756\r\n 1.066930862706352\r\n 1.0679715872270086\r\n 1.068876303469867\r\n 1.0696434085045978\r\n 1.0702725656148622\r\n 1.0707609890298837\r\n 1.071107075846018\r\n 1.0713093338869186\r\n 1.0713672546639146\r\n 1.0713672546629138\r\n 1.071367254661913","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"After extracting the data, the FMU is cleaned-up.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"fmiUnload(realFMU)","category":"page"},{"location":"examples/simple_hybrid_ME/#SimpleFMU","page":"Simple ME-NeuralFMU","title":"SimpleFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The following lines load, simulate and plot the simpleFMU just like the realFMU. The differences between both systems can be clearly seen from the plots. In the plot for the realFMU it can be seen that the oscillation continues to decrease due to the effect of the friction. If you simulate long enough, the oscillation would come to a standstill in a certain time. The oscillation in the simpleFMU behaves differently, since the friction was not taken into account here. The oscillation in this model would continue to infinity with the same oscillation amplitude. From this observation the desire of an improvement of this model arises.     ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"simpleFMU = fmiLoad(\"SpringPendulum1D\", \"Dymola\", \"2022x\")\r\nfmiInfo(simpleFMU)\r\n\r\nvrs = [\"mass.s\", \"mass.v\", \"mass.a\"]\r\nsimpleSimData = fmiSimulate(simpleFMU, (tStart, tStop); recordValues=vrs, saveat=tSave, reset=false)\r\nfmiPlot(simpleSimData)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"#################### Begin information for FMU ####################\r\n\tModel name:\t\t\tSpringPendulum1D\r\n\tFMI-Version:\t\t\t2.0\r\n\tGUID:\t\t\t\t{fc15d8c4-758b-48e6-b00e-5bf47b8b14e5}\r\n\tGeneration tool:\t\tDymola Version 2022x (64-bit), 2021-10-08\r\n\tGeneration time:\t\t2022-05-19T06:54:23Z\r\n\tVar. naming conv.:\t\tstructured\r\n\tEvent indicators:\t\t0\r\n\tInputs:\t\t\t\t0\r\n\tOutputs:\t\t\t0\r\n\tStates:\t\t\t\t2\r\n\t\t33554432 [\"mass.s\"]\r\n\t\t33554433 [\"mass.v\"]\r\n\tSupports Co-Simulation:\t\ttrue\r\n\t\tModel identifier:\tSpringPendulum1D\r\n\t\tGet/Set State:\t\ttrue\r\n\t\tSerialize State:\ttrue\r\n\t\tDir. Derivatives:\ttrue\r\n\t\tVar. com. steps:\ttrue\r\n\t\tInput interpol.:\ttrue\r\n\t\tMax order out. der.:\t1\r\n\tSupports Model-Exchange:\ttrue\r\n\t\tModel identifier:\tSpringPendulum1D\r\n\t\tGet/Set State:\t\ttrue\r\n\t\tSerialize State:\ttrue\r\n\t\tDir. Derivatives:\ttrue\r\n##################### End information for FMU #####################","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The data from the simulation of the simpleFMU, are divided into position and velocity data. These data will be needed later to plot the results. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"velSimple = fmi2GetSolutionValue(simpleSimData, \"mass.v\")\r\nposSimple = fmi2GetSolutionValue(simpleSimData, \"mass.s\")","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"501-element Vector{Float64}:\r\n 0.5\r\n 0.5003127019074967\r\n 0.5012175433745238\r\n 0.5027172504687035\r\n 0.504812416566759\r\n 0.5075012719497328\r\n 0.5107830165354977\r\n 0.5146534880772458\r\n 0.5191107030735219\r\n 0.5241484264969329\r\n 0.5297629811612266\r\n 0.5359472314461261\r\n 0.5426950964528339\r\n ⋮\r\n 1.6842615646003007\r\n 1.6884869953422783\r\n 1.6921224800662573\r\n 1.69516502108285\r\n 1.6976144547672483\r\n 1.6994659284032172\r\n 1.7007174453690572\r\n 1.7013675684067706\r\n 1.7014154196220592\r\n 1.7008606804843265\r\n 1.69970552855305\r\n 1.6979508813706","category":"page"},{"location":"examples/simple_hybrid_ME/#NeuralFMU","page":"Simple ME-NeuralFMU","title":"NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/#Loss-function","page":"Simple ME-NeuralFMU","title":"Loss function","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In order to train our model, a loss function must be implemented. The solver of the NeuralFMU can calculate the gradient of the loss function. The gradient descent is needed to adjust the weights in the neural network so that the sum of the error is reduced and the model becomes more accurate.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The loss function in this implementation consists of the mean squared error (mse) from the real position of the realFMU simulation (posReal) and the position data of the network (posNet). $ e{mse} = \\frac{1}{n} \\sum\\limits{i=0}^n (posReal[i] - posNet[i])^2 $","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"As it is indicated with the comments, one could also additionally consider the mse from the real velocity (velReal) and the velocity from the network (velNet). The error in this case would be calculated from the sum of both errors.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# loss function for training\r\nfunction lossSum(p)\r\n    global posReal\r\n    solution = neuralFMU(x₀; p=p)\r\n\r\n    posNet = fmi2GetSolutionState(solution, 1; isIndex=true)\r\n    \r\n    FMIFlux.Losses.mse(posReal, posNet) \r\nend","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"lossSum (generic function with 1 method)","category":"page"},{"location":"examples/simple_hybrid_ME/#Callback","page":"Simple ME-NeuralFMU","title":"Callback","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"To output the loss in certain time intervals, a callback is implemented as a function in the following. Here a counter is incremented, every twentieth pass the loss function is called and the average error is printed out.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# callback function for training\r\nglobal counter = 0\r\nfunction callb(p)\r\n    global counter += 1\r\n    if counter % 20 == 1\r\n        avgLoss = lossSum(p[1])\r\n        @info \"Loss [$counter]: $(round(avgLoss, digits=5))   Avg displacement in data: $(round(sqrt(avgLoss), digits=5))\"\r\n    end\r\nend","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"callb (generic function with 1 method)","category":"page"},{"location":"examples/simple_hybrid_ME/#Structure-of-the-NeuralFMU","page":"Simple ME-NeuralFMU","title":"Structure of the NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In the following, the topology of the NeuralFMU is constructed. It consists of an input layer, which then leads into the simpleFMU model. The ME-FMU computes the state derivatives for a given system state. Following the simpleFMU is a dense layer that has exactly as many inputs as the model has states (and therefore state derivatives). The output of this layer consists of 16 output nodes and a tanh activation function. The next layer has 16 input and output nodes with the same activation function. The last layer is again a dense layer with 16 input nodes and the number of states as outputs. Here, it is important that no tanh-activation function follows, because otherwise the pendulums state values would be limited to the interval -11.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# NeuralFMU setup\r\nnumStates = fmiGetNumberOfStates(simpleFMU)\r\n\r\nnet = Chain(inputs -> fmiEvaluateME(simpleFMU, inputs),\r\n            Dense(numStates, 16, tanh),\r\n            Dense(16, 16, tanh),\r\n            Dense(16, numStates))","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Chain(\r\n  var\"#1#2\"(),\r\n  Dense(2 => 16, tanh),                 \u001b[90m# 48 parameters\u001b[39m\r\n  Dense(16 => 16, tanh),                \u001b[90m# 272 parameters\u001b[39m\r\n  Dense(16 => 2),                       \u001b[90m# 34 parameters\u001b[39m\r\n) \u001b[90m                  # Total: 6 arrays, \u001b[39m354 parameters, 3.141 KiB.","category":"page"},{"location":"examples/simple_hybrid_ME/#Definition-of-the-NeuralFMU","page":"Simple ME-NeuralFMU","title":"Definition of the NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The instantiation of the ME-NeuralFMU is done as a one-liner. The FMU (simpleFMU), the structure of the network net, start tStart and end time tStop, the numerical solver Tsit5() and the time steps tSave for saving are specified.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"neuralFMU = ME_NeuralFMU(simpleFMU, net, (tStart, tStop), Tsit5(); saveat=tSave);","category":"page"},{"location":"examples/simple_hybrid_ME/#Plot-before-training","page":"Simple ME-NeuralFMU","title":"Plot before training","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Here the state trajectory of the simpleFMU is recorded. Doesn't really look like a pendulum yet, but the system is random initialized by default. In the plots later on, the effect of learning can be seen.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"solutionBefore = neuralFMU(x₀)\r\nfmiPlot(solutionBefore)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/#Training-of-the-NeuralFMU","page":"Simple ME-NeuralFMU","title":"Training of the NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"For the training of the NeuralFMU the parameters are extracted. The known Adam optimizer for minimizing the gradient descent is used as further passing parameters. In addition, the previously defined loss and callback function, as well as the number of epochs are passed.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# train\r\nparamsNet = FMIFlux.params(neuralFMU)\r\n\r\noptim = Adam()\r\nFMIFlux.train!(lossSum, paramsNet, Iterators.repeated((), 300), optim; cb=()->callb(paramsNet)) ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1]: 22.23882   Avg displacement in data: 4.71581\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [21]: 2.24523   Avg displacement in data: 1.49841\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [41]: 0.09813   Avg displacement in data: 0.31326\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [61]: 0.05951   Avg displacement in data: 0.24394\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [81]: 0.05316   Avg displacement in data: 0.23057\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [101]: 0.05009   Avg displacement in data: 0.22382\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [121]: 0.04776   Avg displacement in data: 0.21855\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [141]: 0.04581   Avg displacement in data: 0.21403\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [161]: 0.04412   Avg displacement in data: 0.21005\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [181]: 0.04264   Avg displacement in data: 0.2065\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [201]: 0.04134   Avg displacement in data: 0.20333\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [221]: 0.04018   Avg displacement in data: 0.20046\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [241]: 0.03915   Avg displacement in data: 0.19787\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [261]: 0.03822   Avg displacement in data: 0.1955\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [281]: 0.03737   Avg displacement in data: 0.19332","category":"page"},{"location":"examples/simple_hybrid_ME/#Comparison-of-the-plots","page":"Simple ME-NeuralFMU","title":"Comparison of the plots","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Here three plots are compared with each other and only the position of the mass is considered. The first plot represents the simpleFMU, the second represents the realFMU (reference) and the third plot represents the result after training the NeuralFMU. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# plot results mass.s\r\nsolutionAfter = neuralFMU(x₀)\r\n\r\nfig = Plots.plot(xlabel=\"t [s]\", ylabel=\"mass position [m]\", linewidth=2,\r\n                 xtickfontsize=12, ytickfontsize=12,\r\n                 xguidefontsize=12, yguidefontsize=12,\r\n                 legendfontsize=8, legend=:topright)\r\n\r\nposNeuralFMU = fmi2GetSolutionState(solutionAfter, 1; isIndex=true)\r\n\r\nPlots.plot!(fig, tSave, posSimple, label=\"SimpleFMU\", linewidth=2)\r\nPlots.plot!(fig, tSave, posReal, label=\"RealFMU\", linewidth=2)\r\nPlots.plot!(fig, tSave, posNeuralFMU, label=\"NeuralFMU (300 epochs)\", linewidth=2)\r\nfig ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/#Continue-training-and-plotting","page":"Simple ME-NeuralFMU","title":"Continue training and plotting","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"As can be seen from the previous figure, the plot of the NeuralFMU has not yet fully converged against the realFMU, so the training of the NeuralFMU is continued. After further training, the plot of NeuralFMU is added to the figure again. The effect of the longer training can be recognized well, since the plot of the NeuralFMU had further converged. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"FMIFlux.train!(lossSum, paramsNet, Iterators.repeated((), 1200), optim; cb=()->callb(paramsNet)) \r\n# plot results mass.s\r\nsolutionAfter = neuralFMU(x₀)\r\nposNeuralFMU = fmi2GetSolutionState(solutionAfter, 1; isIndex=true)\r\nPlots.plot!(fig, tSave, posNeuralFMU, label=\"NeuralFMU (1500 epochs)\", linewidth=2)\r\nfig ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [301]: 0.03668   Avg displacement in data: 0.19153\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [321]: 0.03605   Avg displacement in data: 0.18988\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [341]: 0.0355   Avg displacement in data: 0.18841\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [361]: 0.03501   Avg displacement in data: 0.1871\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [381]: 0.03458   Avg displacement in data: 0.18594\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [401]: 0.03419   Avg displacement in data: 0.1849\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [421]: 0.03383   Avg displacement in data: 0.18393\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [441]: 0.0335   Avg displacement in data: 0.18302\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [461]: 0.03317   Avg displacement in data: 0.18213\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [481]: 0.03285   Avg displacement in data: 0.18123\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [501]: 0.03251   Avg displacement in data: 0.18031\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [521]: 0.03216   Avg displacement in data: 0.17933\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [541]: 0.03179   Avg displacement in data: 0.1783\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [561]: 0.03139   Avg displacement in data: 0.17717\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [581]: 0.03096   Avg displacement in data: 0.17596\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [601]: 0.0305   Avg displacement in data: 0.17463\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [621]: 0.02999   Avg displacement in data: 0.17319\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [641]: 0.02945   Avg displacement in data: 0.17161\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [661]: 0.02886   Avg displacement in data: 0.16989\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [681]: 0.02824   Avg displacement in data: 0.16804\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [701]: 0.02755   Avg displacement in data: 0.16598\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [721]: 0.02681   Avg displacement in data: 0.16374\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [741]: 0.02603   Avg displacement in data: 0.16133\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [761]: 0.0252   Avg displacement in data: 0.15873\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [781]: 0.0243   Avg displacement in data: 0.1559\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [801]: 0.02335   Avg displacement in data: 0.15279\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [821]: 0.02231   Avg displacement in data: 0.14936\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [841]: 0.02117   Avg displacement in data: 0.14551\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [861]: 0.01993   Avg displacement in data: 0.14117\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [881]: 0.01858   Avg displacement in data: 0.13631\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [901]: 0.01717   Avg displacement in data: 0.13103\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [921]: 0.01576   Avg displacement in data: 0.12552\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [941]: 0.0144   Avg displacement in data: 0.12001\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [961]: 0.01311   Avg displacement in data: 0.1145\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [981]: 0.01194   Avg displacement in data: 0.10926\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1001]: 0.01089   Avg displacement in data: 0.10434\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1021]: 0.0099   Avg displacement in data: 0.0995\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1041]: 0.00894   Avg displacement in data: 0.09453\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1061]: 0.00801   Avg displacement in data: 0.08948\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1081]: 0.00716   Avg displacement in data: 0.0846\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1101]: 0.00646   Avg displacement in data: 0.08039\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1121]: 0.00597   Avg displacement in data: 0.07724\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1141]: 0.00564   Avg displacement in data: 0.07511\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1161]: 0.00544   Avg displacement in data: 0.07378\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1181]: 0.0052   Avg displacement in data: 0.07209\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1201]: 0.00508   Avg displacement in data: 0.0713\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1221]: 0.00499   Avg displacement in data: 0.07066\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1241]: 0.0049   Avg displacement in data: 0.07\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1261]: 0.00482   Avg displacement in data: 0.06941\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1281]: 0.0047   Avg displacement in data: 0.06855\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1301]: 0.00467   Avg displacement in data: 0.06831\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1321]: 0.00461   Avg displacement in data: 0.06791\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1341]: 0.00453   Avg displacement in data: 0.06728\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1361]: 0.00447   Avg displacement in data: 0.06684\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1381]: 0.00431   Avg displacement in data: 0.06568\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1401]: 0.00424   Avg displacement in data: 0.06515\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1421]: 0.00419   Avg displacement in data: 0.06475\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1441]: 0.00414   Avg displacement in data: 0.06432\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1461]: 0.00405   Avg displacement in data: 0.06366\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1481]: 0.00404   Avg displacement in data: 0.06353","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Finally, the FMU is cleaned-up.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"fmiUnload(simpleFMU)","category":"page"},{"location":"examples/simple_hybrid_ME/#Summary","page":"Simple ME-NeuralFMU","title":"Summary","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Based on the plots, it can be seen that the NeuralFMU is able to adapt the friction model of the realFMU. After 300 runs, the curves do not overlap very well, but this can be achieved by longer training (1000 runs) or a better initialization.","category":"page"},{"location":"examples/simple_hybrid_ME/#Source","page":"Simple ME-NeuralFMU","title":"Source","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"[1] Tobias Thummerer, Lars Mikelsons and Josef Kircher. 2021. NeuralFMU: towards structural integration of FMUs into neural networks. Martin Sjölund, Lena Buffoni, Adrian Pop and Lennart Ochel (Ed.). Proceedings of 14th Modelica Conference 2021, Linköping, Sweden, September 20-24, 2021. Linköping University Electronic Press, Linköping (Linköping Electronic Conference Proceedings ; 181), 297-306. DOI: 10.3384/ecp21181297","category":"page"},{"location":"faq/#FAQ","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"faq/","page":"FAQ","title":"FAQ","text":"This list some common - often numerical - errors, that can be fixed by better understanding the ODE-Problem inside your FMU.","category":"page"},{"location":"faq/#Double-callback-crossing","page":"FAQ","title":"Double callback crossing","text":"","category":"section"},{"location":"faq/#Description","page":"FAQ","title":"Description","text":"","category":"section"},{"location":"faq/","page":"FAQ","title":"FAQ","text":"Error message, a double zero-crossing happended, often during training a NeuralFMU.","category":"page"},{"location":"faq/#Example","page":"FAQ","title":"Example","text":"","category":"section"},{"location":"faq/","page":"FAQ","title":"FAQ","text":"Double callback crossing floating pointer reducer errored. Report this issue.","category":"page"},{"location":"faq/#Reason","page":"FAQ","title":"Reason","text":"","category":"section"},{"location":"faq/","page":"FAQ","title":"FAQ","text":"This could be, because the event inside of a NeuralFMU can't be located (often when using Zygote). ","category":"page"},{"location":"faq/#Fix","page":"FAQ","title":"Fix","text":"","category":"section"},{"location":"faq/","page":"FAQ","title":"FAQ","text":"Try to increase the root search interpolation points, this is computational expensive for FMUs with many events- and event-indicators. This can be done using fmu.executionConfig.rootSearchInterpolationPoints = 100 (default value is 10).","category":"page"},{"location":"examples/simple_hybrid_CS/#Creation-and-training-of-CS-NeuralFMUs","page":"Simple CS-NeuralFMU","title":"Creation and training of CS-NeuralFMUs","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Tutorial by Johannes Stoljar, Tobias Thummerer","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Last edit: 03.01.2023","category":"page"},{"location":"examples/simple_hybrid_CS/#License","page":"Simple CS-NeuralFMU","title":"License","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# Copyright (c) 2021 Tobias Thummerer, Lars Mikelsons, Johannes Stoljar\r\n# Licensed under the MIT license. \r\n# See LICENSE (https://github.com/thummeto/FMIFlux.jl/blob/main/LICENSE) file in the project root for details.","category":"page"},{"location":"examples/simple_hybrid_CS/#Motivation","page":"Simple CS-NeuralFMU","title":"Motivation","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The Julia Package FMIFlux.jl is motivated by the application of hybrid modeling. This package enables the user to integrate his simulation model between neural networks (NeuralFMU). For this, the simulation model must be exported as FMU (functional mock-up unit), which corresponds to a widely used standard. The big advantage of hybrid modeling with artificial neural networks is, that effects that are difficult to model (because they might be unknown) can be easily learned by the neural networks. For this purpose, the NeuralFMU is trained with measurement data containing the not modeled physical effect. The final product is a simulation model including the originally not modeled effects. Another big advantage of the NeuralFMU is that it works with little data, because the FMU already contains the characteristic functionality of the simulation and only the missing effects are added.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"NeuralFMUs do not need to be as easy as in this example. Basically a NeuralFMU can combine different ANN topologies that manipulate any FMU-input (system state, system inputs, time) and any FMU-output (system state derivative, system outputs, other system variables). However, for this example a NeuralFMU topology as shown in the following picture is used.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: CS-NeuralFMU.svg)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"NeuralFMU (CS) from [1].","category":"page"},{"location":"examples/simple_hybrid_CS/#Introduction-to-the-example","page":"Simple CS-NeuralFMU","title":"Introduction to the example","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In this example, the model of a one-dimensional spring pendulum (with an external acting force) is used to learn the initial states. For this purpose, on the one hand the initial position of the mass of the pendulum is shifted and on the other hand the default position of the mass from the model is used. The model with the shifted initial position serves as reference and is called referenceFMU in the following. The model with the default position is further referenced with defaultFMU. At the beginning, the actual state of both simulations is shown, whereby clear deviations can be seen in the graphs. Afterwards, the defaultFMU is integrated into a co-simulation NeuralFMU (CS-NeuralFMU) architecture. By training the NeuralFMU, an attempt is made to learn the initial displacement of the referenceFMU. It can be clearly seen that the NeuralFMU learns this shift well in just a few training steps. ","category":"page"},{"location":"examples/simple_hybrid_CS/#Target-group","page":"Simple CS-NeuralFMU","title":"Target group","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The example is primarily intended for users who work in the field of first principle and/or hybrid modeling and are further interested in hybrid model building. The example wants to show how simple it is to combine FMUs with machine learning and to illustrate the advantages of this approach.","category":"page"},{"location":"examples/simple_hybrid_CS/#Other-formats","page":"Simple CS-NeuralFMU","title":"Other formats","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Besides, this Jupyter Notebook there is also a Julia file with the same name, which contains only the code cells and for the documentation there is a Markdown file corresponding to the notebook.  ","category":"page"},{"location":"examples/simple_hybrid_CS/#Getting-started","page":"Simple CS-NeuralFMU","title":"Getting started","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/#Installation-prerequisites","page":"Simple CS-NeuralFMU","title":"Installation prerequisites","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":" Description Command\n1. Enter Package Manager via ]\n2. Install FMI via add FMI\n3. Install FMIFlux via add FMIFlux\n4. Install FMIZoo via add FMIZoo\n5. Install DifferentialEquations via add DifferentialEquations\n6. Install Plots via add Plots\n7. Install Random via add Random","category":"page"},{"location":"examples/simple_hybrid_CS/#Code-section","page":"Simple CS-NeuralFMU","title":"Code section","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"To run the example, the previously installed packages must be included. ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# imports\r\nusing FMI\r\nusing FMIFlux\r\nusing FMIZoo\r\nusing DifferentialEquations: Tsit5\r\nimport Plots\r\n\r\n# set seed\r\nimport Random\r\nRandom.seed!(1234);","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"After importing the packages, the path to the Functional Mock-up Units (FMUs) is set. The FMU is a model exported meeting the Functional Mock-up Interface (FMI) Standard. The FMI is a free standard (fmi-standard.org) that defines a container and an interface to exchange dynamic models using a combination of XML files, binaries and C code zipped into a single file. ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The objec-orientated structure of the SpringPendulumExtForce1D can be seen in the following graphic. This model is a simple spring pendulum without friction, but with an external force. ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Next, the start time and end time of the simulation are set. Finally, a step size is specified to store the results of the simulation at these time steps.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"tStart = 0.0\r\ntStep = 0.01\r\ntStop = 5.0\r\ntSave = tStart:tStep:tStop","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"0.0:0.01:5.0","category":"page"},{"location":"examples/simple_hybrid_CS/#ReferenceFMU","page":"Simple CS-NeuralFMU","title":"ReferenceFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In the next lines of code the FMU of the referenceFMU model is loaded from FMIZoo.jl and the information about the FMU is shown.  ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"referenceFMU = fmiLoad(\"SpringPendulumExtForce1D\", \"Dymola\", \"2022x\")\r\nfmiInfo(referenceFMU)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"#################### Begin information for FMU ####################\r\n\tModel name:\t\t\tSpringPendulumExtForce1D\r\n\tFMI-Version:\t\t\t2.0\r\n\tGUID:\t\t\t\t{df5ebe46-3c86-42a5-a68a-7d008395a7a3}\r\n\tGeneration tool:\t\tDymola Version 2022x (64-bit), 2021-10-08\r\n\tGeneration time:\t\t2022-05-19T06:54:33Z\r\n\tVar. naming conv.:\t\tstructured\r\n\tEvent indicators:\t\t0\r\n\tInputs:\t\t\t\t1\r\n\t\t352321536 [\"extForce\"]\r\n\tOutputs:\t\t\t2\r\n\t\t335544320 [\"accSensor.v\", \"der(accSensor.flange.s)\", \"v\", \"der(speedSensor.flange.s)\", \"speedSensor.v\"]\r\n\t\t335544321 [\"der(accSensor.v)\", \"a\", \"accSensor.a\"]\r\n\tStates:\t\t\t\t2\r\n\t\t33554432 [\"mass.s\"]\r\n\t\t33554433 [\"mass.v\"]\r\n\tSupports Co-Simulation:\t\ttrue\r\n\t\tModel identifier:\tSpringPendulumExtForce1D\r\n\t\tGet/Set State:\t\ttrue\r\n\t\tSerialize State:\ttrue\r\n\t\tDir. Derivatives:\ttrue\r\n\t\tVar. com. steps:\ttrue\r\n\t\tInput interpol.:\ttrue\r\n\t\tMax order out. der.:\t1\r\n\tSupports Model-Exchange:\ttrue\r\n\t\tModel identifier:\tSpringPendulumExtForce1D\r\n\t\tGet/Set State:\t\ttrue\r\n\t\tSerialize State:\ttrue\r\n\t\tDir. Derivatives:\ttrue\r\n##################### End information for FMU #####################","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In the next steps the parameters are defined. The first parameter is the initial position of the mass, which is initilized with 13𝑚. The second parameter is the initial velocity of the mass, which is initilized with 0fracms. The FMU hase two states: The first state is the position of the mass and the second state is the velocity. In the function fmiSimulate() the referenceFMU is simulated, still specifying the start and end time, the parameters and which variables are recorded. After the simulation is finished the result of the referenceFMU can be plotted. This plot also serves as a reference for the later CS-NeuralFMU model.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"param = Dict(\"mass_s0\" => 1.3, \"mass.v\" => 0.0)   # increase amplitude, invert phase\r\nvrs = [\"mass.s\", \"mass.v\", \"mass.a\"]\r\nreferenceSimData = fmiSimulate(referenceFMU, (tStart, tStop); parameters=param, recordValues=vrs, saveat=tSave)\r\nfmiPlot(referenceSimData)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The data from the simulation of the referenceFMU, are divided into position, velocity and acceleration data. The data for the acceleration will be needed later. ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"posReference = fmi2GetSolutionValue(referenceSimData, vrs[1])\r\nvelReference = fmi2GetSolutionValue(referenceSimData, vrs[2])\r\naccReference = fmi2GetSolutionValue(referenceSimData, vrs[3])","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"501-element Vector{Float64}:\r\n -1.9999999999999996\r\n -1.9988827275812904\r\n -1.9958127258179004\r\n -1.9907908533763607\r\n -1.9837918439669844\r\n -1.9748258342855118\r\n -1.963890162864621\r\n -1.9510089134488018\r\n -1.9361810148909009\r\n -1.9194099484303728\r\n -1.9007374108186537\r\n -1.8801634598739092\r\n -1.8576990114645708\r\n  ⋮\r\n  1.9971927754348462\r\n  2.0126501310664713\r\n  2.026070116129912\r\n  2.037424725618772\r\n  2.0467236772128947\r\n  2.0541004250985972\r\n  2.0594240680173828\r\n  2.062679095787284\r\n  2.0638499982263325\r\n  2.0629212651525553\r\n  2.059877386383986\r\n  2.0548550901379925","category":"page"},{"location":"examples/simple_hybrid_CS/#DefaultFMU","page":"Simple CS-NeuralFMU","title":"DefaultFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The following is a renaming for the referenceFMU to defaultFMU. The previous initial position of the mass is now set to the default position of the defaultFMU. The initial position of the mass is initilized with 05𝑚 and initial velocity of the mass is initialized with 0fracms.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"defaultFMU = referenceFMU\r\nparam = Dict(\"mass_s0\" => 0.5, \"mass.v\" => 0.0)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Dict{String, Float64} with 2 entries:\r\n  \"mass_s0\" => 0.5\r\n  \"mass.v\"  => 0.0","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The following simulate and plot the defaultFMU just like the referenceFMU. The differences between both systems can be clearly seen from the plots. In the plots for the defaultFMU you can see that other oscillations occur due to the different starting positions. On the one hand the oscillation of the defaultFMU starts in the opposite direction of the referenceFMU and on the other hand the graphs for the velocity and acceleration differ clearly in the amplitude. In the following we try to learn the initial shift of the position so that the graphs for the acceleration of both graphs match.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"defaultSimData = fmiSimulate(defaultFMU, (tStart, tStop); parameters=param, recordValues=vrs, saveat=tSave)\r\nfmiPlot(defaultSimData)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The data from the simualtion of the defaultFMU, are divided into position, velocity and acceleration data. The data for the acceleration will be needed later.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"posDefault = fmi2GetSolutionValue(defaultSimData, vrs[1])\r\nvelDefault = fmi2GetSolutionValue(defaultSimData, vrs[2])\r\naccDefault = fmi2GetSolutionValue(defaultSimData, vrs[3])","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"501-element Vector{Float64}:\r\n  6.0\r\n  5.996872980925033\r\n  5.987824566254761\r\n  5.9728274953129645\r\n  5.95187583433241\r\n  5.9249872805026715\r\n  5.892169834645022\r\n  5.853465119227542\r\n  5.808892969264781\r\n  5.75851573503067\r\n  5.702370188387734\r\n  5.640527685538739\r\n  5.573049035471661\r\n  ⋮\r\n -5.842615646003006\r\n -5.884869953422783\r\n -5.921224800662572\r\n -5.9516502108284985\r\n -5.976144547672481\r\n -5.994659284032171\r\n -6.007174453690571\r\n -6.013675684067705\r\n -6.014154196220591\r\n -6.008606804843264\r\n -5.997055285530499\r\n -5.979508813705998","category":"page"},{"location":"examples/simple_hybrid_CS/#CS-NeuralFMU","page":"Simple CS-NeuralFMU","title":"CS-NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In this section, the defaultFMU is inserted into a CS-NeuralFMU architecture. It has the goal to learn the initial state of the referenceFMU.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"For the external force, a simple function is implemented that always returns a force of 0N at each time point. Also, all other functions and implementations would be possible here. Only for simplification reasons the function was chosen so simply.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"function extForce(t)\r\n    return [0.0]\r\nend ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"extForce (generic function with 1 method)","category":"page"},{"location":"examples/simple_hybrid_CS/#Loss-function","page":"Simple CS-NeuralFMU","title":"Loss function","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In order to train our model, a loss function must be implemented. The solver of the NeuralFMU can calculate the gradient of the loss function. The gradient descent is needed to adjust the weights in the neural network so that the sum of the error is reduced and the model becomes more accurate.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The loss function in this implementation consists of the mean squared error (mse) from the acceleration data of the referenceFMU simulation (accReference) and the acceleration data of the network (accNet). $ e{mse} = \\frac{1}{n} \\sum\\limits{i=0}^n (accReference[i] - accNet[i])^2 $","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# loss function for training\r\nfunction lossSum(p)\r\n    solution = csNeuralFMU(extForce, tStep; p=p)\r\n\r\n    accNet = fmi2GetSolutionValue(solution, 1; isIndex=true)\r\n    \r\n    FMIFlux.Losses.mse(accReference, accNet)\r\nend","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"lossSum (generic function with 1 method)","category":"page"},{"location":"examples/simple_hybrid_CS/#Callback","page":"Simple CS-NeuralFMU","title":"Callback","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"To output the loss in certain time intervals, a callback is implemented as a function in the following. Here a counter is incremented, every twentieth pass the loss function is called and the average error is printed out.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# callback function for training\r\nglobal counter = 0\r\nfunction callb(p)\r\n    global counter += 1\r\n\r\n    if counter % 20 == 1\r\n        avgLoss = lossSum(p[1])\r\n        @info \"Loss [$counter]: $(round(avgLoss, digits=5))\"\r\n    end\r\nend","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"callb (generic function with 1 method)","category":"page"},{"location":"examples/simple_hybrid_CS/#Structure-of-the-CS-NeuralFMU","page":"Simple CS-NeuralFMU","title":"Structure of the CS-NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In the following, the topology of the CS-NeuralFMU is constructed. It consists of an input layer, which then leads into the defaultFMU model. The CS-FMU computes the outputs for the given system state and time step. After the defaultFMU follows a dense layer, which has exactly as many inputs as the model has outputs. The output of this layer consists of 16 output nodes and a tanh activation function. The next layer has 16 input and output nodes with the same activation function. The last layer is again a dense layer with 16 input nodes and the number of model outputs as output nodes. Here, it is important that no tanh-activation function follows, because otherwise the pendulums state values would be limited to the interval -11.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# NeuralFMU setup\r\nnumInputs = length(defaultFMU.modelDescription.inputValueReferences)\r\nnumOutputs = length(defaultFMU.modelDescription.outputValueReferences)\r\n\r\nfunction eval(u)\r\n    y, _ = defaultFMU(;u_refs=defaultFMU.modelDescription.inputValueReferences, u=u, y_refs=defaultFMU.modelDescription.outputValueReferences)\r\n    return y\r\nend\r\n\r\n\r\nnet = Chain(inputs -> eval(inputs),\r\n            Dense(numOutputs, 16, tanh),\r\n            Dense(16, 16, tanh),\r\n            Dense(16, numOutputs))","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Chain(\r\n  var\"#1#2\"(),\r\n  Dense(2 => 16, tanh),                 \u001b[90m# 48 parameters\u001b[39m\r\n  Dense(16 => 16, tanh),                \u001b[90m# 272 parameters\u001b[39m\r\n  Dense(16 => 2),                       \u001b[90m# 34 parameters\u001b[39m\r\n) \u001b[90m                  # Total: 6 arrays, \u001b[39m354 parameters, 3.141 KiB.","category":"page"},{"location":"examples/simple_hybrid_CS/#Definition-of-the-CS-NeuralFMU","page":"Simple CS-NeuralFMU","title":"Definition of the CS-NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The instantiation of the CS-NeuralFMU is done as a one-liner. The FMU defaultFMU, the structure of the network net, start tStart and end time tStop, and the time steps tSave for saving are specified.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"csNeuralFMU = CS_NeuralFMU(defaultFMU, net, (tStart, tStop); saveat=tSave);","category":"page"},{"location":"examples/simple_hybrid_CS/#Plot-before-training","page":"Simple CS-NeuralFMU","title":"Plot before training","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Here the state trajectory of the extForceFMU is recorded. Doesn't really look like a pendulum yet, but the system is random initialized by default. In the plots later on, the effect of learning can be seen.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"solutionBefore = csNeuralFMU(extForce, tStep)\r\naccNeuralFMU = fmi2GetSolutionValue(solutionBefore, 1; isIndex=true)\r\nPlots.plot(tSave, accNeuralFMU, label=\"acc CS-NeuralFMU\", linewidth=2)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_CS/#Training-of-the-CS-NeuralFMU","page":"Simple CS-NeuralFMU","title":"Training of the CS-NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"For the training of the CS-NeuralFMU the parameters are extracted. The known Adam optimizer for minimizing the gradient descent is used as further passing parameters. In addition, the previously defined loss and callback function, as well as the number of epochs are passed.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# train\r\nparamsNet = FMIFlux.params(csNeuralFMU)\r\n\r\noptim = Adam()\r\nFMIFlux.train!(lossSum, paramsNet, Iterators.repeated((), 300), optim; cb=()->callb(paramsNet))","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [1]: 1.54017\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [21]: 0.17445\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [41]: 0.07401\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [61]: 0.04483\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [81]: 0.03093\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [101]: 0.02044\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [121]: 0.01349\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [141]: 0.00901\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [161]: 0.00623\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [181]: 0.00455\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [201]: 0.00354\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [221]: 0.00291\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [241]: 0.00249\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [261]: 0.00217\r\n\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mLoss [281]: 0.00192","category":"page"},{"location":"examples/simple_hybrid_CS/#Comparison-of-the-plots","page":"Simple CS-NeuralFMU","title":"Comparison of the plots","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Here three plots are compared with each other and only the acceleration of the mass is considered. The first plot represents the defaultFMU, the second represents the referenceFMU and the third plot represents the result after training the CS-NeuralFMU. ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# plot results mass.a\r\nsolutionAfter = csNeuralFMU(extForce, tStep)\r\n\r\nfig = Plots.plot(xlabel=\"t [s]\", ylabel=\"mass acceleration [m/s^2]\", linewidth=2,\r\n                 xtickfontsize=12, ytickfontsize=12,\r\n                 xguidefontsize=12, yguidefontsize=12,\r\n                 legendfontsize=8, legend=:topright)\r\n\r\naccNeuralFMU = fmi2GetSolutionValue(solutionAfter, 1; isIndex=true)\r\n\r\nPlots.plot!(fig, tSave, accDefault, label=\"defaultFMU\", linewidth=2)\r\nPlots.plot!(fig, tSave, accReference, label=\"referenceFMU\", linewidth=2)\r\nPlots.plot!(fig, tSave, accNeuralFMU, label=\"CS-NeuralFMU (300 eps.)\", linewidth=2)\r\nfig ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Finally, the FMU is cleaned-up.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"fmiUnload(defaultFMU)","category":"page"},{"location":"examples/simple_hybrid_CS/#Summary","page":"Simple CS-NeuralFMU","title":"Summary","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Based on the plots, it can be clearly seen that the CS-NeuralFMU model is able to learn the shift of the initial position. Even after only 300 runs, the curves overlap very much, so no further training with more runs is needed.","category":"page"},{"location":"examples/simple_hybrid_CS/#Source","page":"Simple CS-NeuralFMU","title":"Source","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"[1] Tobias Thummerer, Lars Mikelsons and Josef Kircher. 2021. NeuralFMU: towards structural integration of FMUs into neural networks. Martin Sjölund, Lena Buffoni, Adrian Pop and Lennart Ochel (Ed.). Proceedings of 14th Modelica Conference 2021, Linköping, Sweden, September 20-24, 2021. Linköping University Electronic Press, Linköping (Linköping Electronic Conference Proceedings ; 181), 297-306. DOI: 10.3384/ecp21181297","category":"page"},{"location":"examples/advanced_hybrid_ME/#Creation-and-training-of-ME-NeuralFMUs","page":"Advanced ME-NeuralFMU","title":"Creation and training of ME-NeuralFMUs","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"Tutorial by Johannes Stoljar, Tobias Thummerer","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"Last edit: 03.01.2023","category":"page"},{"location":"examples/advanced_hybrid_ME/#LICENSE","page":"Advanced ME-NeuralFMU","title":"LICENSE","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"# Copyright (c) 2021 Tobias Thummerer, Lars Mikelsons, Johannes Stoljar\r\n# Licensed under the MIT license. \r\n# See LICENSE (https://github.com/thummeto/FMIFlux.jl/blob/main/LICENSE) file in the project root for details.","category":"page"},{"location":"examples/advanced_hybrid_ME/#Motivation","page":"Advanced ME-NeuralFMU","title":"Motivation","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"The Julia Package FMIFlux.jl is motivated by the application of hybrid modeling. This package enables the user to integrate his simulation model between neural networks (NeuralFMU). For this, the simulation model must be exported as FMU (functional mock-up unit), which corresponds to a widely used standard. The big advantage of hybrid modeling with artificial neural networks is, that effects that are difficult to model (because they might be unknown) can be easily learned by the neural networks. For this purpose, the NeuralFMU is trained with measurement data containing the not modeled physical effect. The final product is a simulation model including the originally not modeled effects. Another big advantage of the NeuralFMU is that it works with little data, because the FMU already contains the characteristic functionality of the simulation and only the missing effects are added.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"NeuralFMUs do not need to be as easy as in this example. Basically a NeuralFMU can combine different ANN topologies that manipulate any FMU-input (system state, system inputs, time) and any FMU-output (system state derivative, system outputs, other system variables). However, for this example a NeuralFMU topology as shown in the following picture is used.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"(Image: NeuralFMU.svg)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"NeuralFMU (ME) from [1].","category":"page"},{"location":"examples/advanced_hybrid_ME/#Introduction-to-the-example","page":"Advanced ME-NeuralFMU","title":"Introduction to the example","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"In this example, simplified modeling of a one-dimensional spring pendulum (without friction) is compared to a model of the same system that includes a nonlinear friction model. The FMU with the simplified model will be named simpleFMU in the following and the model with the friction will be named realFMU. At the beginning, the actual state of both simulations is shown, whereby clear deviations can be seen in the graphs. The realFMU serves as a reference graph. The simpleFMU is then integrated into a NeuralFMU architecture and a training of the entire network is performed. After the training the final state is compared again to the realFMU. It can be clearly seen that by using the NeuralFMU, learning of the friction process has taken place.  ","category":"page"},{"location":"examples/advanced_hybrid_ME/#Target-group","page":"Advanced ME-NeuralFMU","title":"Target group","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"The example is primarily intended for users who work in the field of first principle and/or hybrid modeling and are further interested in hybrid model building. The example wants to show how simple it is to combine FMUs with machine learning and to illustrate the advantages of this approach.","category":"page"},{"location":"examples/advanced_hybrid_ME/#Other-formats","page":"Advanced ME-NeuralFMU","title":"Other formats","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"Besides, this Jupyter Notebook there is also a Julia file with the same name, which contains only the code cells and for the documentation there is a Markdown file corresponding to the notebook.  ","category":"page"},{"location":"examples/advanced_hybrid_ME/#Getting-started","page":"Advanced ME-NeuralFMU","title":"Getting started","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/#Installation-prerequisites","page":"Advanced ME-NeuralFMU","title":"Installation prerequisites","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":" Description Command\n1. Enter Package Manager via ]\n2. Install FMI via add FMI\n3. Install FMIFlux via add FMIFlux\n4. Install FMIZoo via add FMIZoo\n5. Install DifferentialEquations via add DifferentialEquations\n6. Install Plots via add Plots\n7. Install Random via add Random","category":"page"},{"location":"examples/advanced_hybrid_ME/#Code-section","page":"Advanced ME-NeuralFMU","title":"Code section","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"To run the example, the previously installed packages must be included. ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"# imports\r\nusing FMI\r\nusing FMI.FMIImport: fmi2StringToValueReference, fmi2ValueReference, fmi2Real\r\nusing FMIFlux\r\nusing FMIZoo\r\nusing DifferentialEquations: Tsit5\r\nusing Statistics: mean, std\r\nimport Plots\r\n\r\n# set seed\r\nimport Random\r\nRandom.seed!(1234);","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"After importing the packages, the path to the Functional Mock-up Units (FMUs) is set. The FMU is a model exported meeting the Functional Mock-up Interface (FMI) Standard. The FMI is a free standard (fmi-standard.org) that defines a container and an interface to exchange dynamic models using a combination of XML files, binaries and C code zipped into a single file. ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"The object-orientated structure of the SpringPendulum1D (simpleFMU) can be seen in the following graphic and corresponds to a simple modeling.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"In contrast, the model SpringFrictionPendulum1D (realFMU) is somewhat more accurate, because it includes a friction component. ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"Next, the start time and end time of the simulation are set. Finally, a step size is specified to store the results of the simulation at these time steps.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"tStart = 0.0\r\ntStep = 0.1\r\ntStop = 5.0\r\ntSave = collect(tStart:tStep:tStop)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"51-element Vector{Float64}:\r\n 0.0\r\n 0.1\r\n 0.2\r\n 0.3\r\n 0.4\r\n 0.5\r\n 0.6\r\n 0.7\r\n 0.8\r\n 0.9\r\n 1.0\r\n 1.1\r\n 1.2\r\n ⋮\r\n 3.9\r\n 4.0\r\n 4.1\r\n 4.2\r\n 4.3\r\n 4.4\r\n 4.5\r\n 4.6\r\n 4.7\r\n 4.8\r\n 4.9\r\n 5.0","category":"page"},{"location":"examples/advanced_hybrid_ME/#RealFMU","page":"Advanced ME-NeuralFMU","title":"RealFMU","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"In the next lines of code the FMU of the realFMU model from FMIZoo.jl is loaded and the information about the FMU is shown.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"realFMU = fmiLoad(\"SpringFrictionPendulum1D\", \"Dymola\", \"2022x\")\r\nfmiInfo(realFMU)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"#################### Begin information for FMU ####################\r\n\tModel name:\t\t\tSpringFrictionPendulum1D\r\n\tFMI-Version:\t\t\t2.0\r\n\tGUID:\t\t\t\t{2e178ad3-5e9b-48ec-a7b2-baa5669efc0c}\r\n\tGeneration tool:\t\tDymola Version 2022x (64-bit), 2021-10-08\r\n\tGeneration time:\t\t2022-05-19T06:54:12Z\r\n\tVar. naming conv.:\t\tstructured\r\n\tEvent indicators:\t\t24\r\n\tInputs:\t\t\t\t0\r\n\tOutputs:\t\t\t0\r\n\tStates:\t\t\t\t2\r\n\t\t33554432 [\"mass.s\"]\r\n\t\t33554433 [\"mass.v\", \"mass.v_relfric\"]\r\n\tSupports Co-Simulation:\t\ttrue\r\n\t\tModel identifier:\tSpringFrictionPendulum1D\r\n\t\tGet/Set State:\t\ttrue\r\n\t\tSerialize State:\ttrue\r\n\t\tDir. Derivatives:\ttrue\r\n\t\tVar. com. steps:\ttrue\r\n\t\tInput interpol.:\ttrue\r\n\t\tMax order out. der.:\t1\r\n\tSupports Model-Exchange:\ttrue\r\n\t\tModel identifier:\tSpringFrictionPendulum1D\r\n\t\tGet/Set State:\t\ttrue\r\n\t\tSerialize State:\ttrue\r\n\t\tDir. Derivatives:\ttrue\r\n##################### End information for FMU #####################","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"In the function fmiSimulate() the realFMU is simulated, still specifying the start and end time, the parameters and which variables are recorded. After the simulation is finished the result of the realFMU can be plotted. This plot also serves as a reference for the other model (simpleFMU).","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"vrs = [\"mass.s\", \"mass.v\", \"mass.a\", \"mass.f\"]\r\nrealSimData = fmiSimulate(realFMU, (tStart, tStop); recordValues=vrs, saveat=tSave)\r\nfmiPlot(realSimData)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"The data from the simulation of the realFMU, are divided into position and velocity data. These data will be needed later. ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"posReal = fmi2GetSolutionValue(realSimData, \"mass.s\")\r\nvelReal = fmi2GetSolutionValue(realSimData, \"mass.v\")","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"51-element Vector{Float64}:\r\n  0.0\r\n  0.432852398300982\r\n  0.8401743918610578\r\n  1.1702254881462497\r\n  1.3861768532456016\r\n  1.4649609400224617\r\n  1.397962181945595\r\n  1.1917483098990418\r\n  0.8657325133644009\r\n  0.44821918384886916\r\n -0.02200493896693855\r\n -0.380560845401747\r\n -0.7172068753289351\r\n  ⋮\r\n -0.19353187721088116\r\n  0.021605187634145845\r\n  0.12911473439606144\r\n  0.2315130895115627\r\n  0.31667721272388255\r\n  0.37417576531479746\r\n  0.3964197153211615\r\n  0.3795927497483354\r\n  0.3235539803194403\r\n  0.2317738499958648\r\n  0.11061350893737848\r\n -1.0008118292437196e-10","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"The FMU hase two states: The first state is the position of the mass and the second state is the velocity. The initial position of the mass is initilized with 05𝑚. The initial velocity of the mass is initialized with 0fracms. ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"x₀ = [posReal[1], velReal[1]]","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"2-element Vector{Float64}:\r\n 0.5\r\n 0.0","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"After extracting the data, the FMU is cleaned-up.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"fmiUnload(realFMU)","category":"page"},{"location":"examples/advanced_hybrid_ME/#SimpleFMU","page":"Advanced ME-NeuralFMU","title":"SimpleFMU","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"The following lines load, simulate and plot the simpleFMU just like the realFMU. The differences between both systems can be clearly seen from the plots. In the plot for the realFMU it can be seen that the oscillation continues to decrease due to the effect of the friction. If you simulate long enough, the oscillation would come to a standstill in a certain time. The oscillation in the simpleFMU behaves differently, since the friction was not taken into account here. The oscillation in this model would continue to infinity with the same oscillation amplitude. From this observation the desire of an improvement of this model arises.     ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"simpleFMU = fmiLoad(\"SpringPendulum1D\", \"Dymola\", \"2022x\")\r\nfmiInfo(simpleFMU)\r\n\r\nvrs = [\"mass.s\", \"mass.v\", \"mass.a\"]\r\nsimpleSimData = fmiSimulate(simpleFMU, (tStart, tStop); recordValues=vrs, saveat=tSave)\r\nfmiPlot(simpleSimData)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"#################### Begin information for FMU ####################\r\n\tModel name:\t\t\tSpringPendulum1D\r\n\tFMI-Version:\t\t\t2.0\r\n\tGUID:\t\t\t\t{fc15d8c4-758b-48e6-b00e-5bf47b8b14e5}\r\n\tGeneration tool:\t\tDymola Version 2022x (64-bit), 2021-10-08\r\n\tGeneration time:\t\t2022-05-19T06:54:23Z\r\n\tVar. naming conv.:\t\tstructured\r\n\tEvent indicators:\t\t0\r\n\tInputs:\t\t\t\t0\r\n\tOutputs:\t\t\t0\r\n\tStates:\t\t\t\t2\r\n\t\t33554432 [\"mass.s\"]\r\n\t\t33554433 [\"mass.v\"]\r\n\tSupports Co-Simulation:\t\ttrue\r\n\t\tModel identifier:\tSpringPendulum1D\r\n\t\tGet/Set State:\t\ttrue\r\n\t\tSerialize State:\ttrue\r\n\t\tDir. Derivatives:\ttrue\r\n\t\tVar. com. steps:\ttrue\r\n\t\tInput interpol.:\ttrue\r\n\t\tMax order out. der.:\t1\r\n\tSupports Model-Exchange:\ttrue\r\n\t\tModel identifier:\tSpringPendulum1D\r\n\t\tGet/Set State:\t\ttrue\r\n\t\tSerialize State:\ttrue\r\n\t\tDir. Derivatives:\ttrue\r\n##################### End information for FMU #####################","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"The data from the simulation of the simpleFMU, are divided into position and velocity data. These data will be needed later to plot the results. ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"posSimple = fmi2GetSolutionValue(simpleSimData, \"mass.s\")\r\nvelSimple = fmi2GetSolutionValue(simpleSimData, \"mass.v\")","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"51-element Vector{Float64}:\r\n  0.0\r\n  0.5899802196326744\r\n  1.1216144329248279\r\n  1.542195620662035\r\n  1.810172737052044\r\n  1.8985676043018223\r\n  1.7983499725303025\r\n  1.5196216961327944\r\n  1.0900958349841172\r\n  0.5523346620786151\r\n -0.040261546913912205\r\n -0.6289411637396933\r\n -1.1552195220019175\r\n  ⋮\r\n -0.43297835247721894\r\n  0.1644403574466082\r\n  0.7455652283389829\r\n  1.2527659117804728\r\n  1.6356623403044424\r\n  1.8562751551367387\r\n  1.8926761758140136\r\n  1.7412508664862896\r\n  1.417004896988811\r\n  0.9521088322603164\r\n  0.3926807653512623\r\n -0.20575332570677826","category":"page"},{"location":"examples/advanced_hybrid_ME/#NeuralFMU","page":"Advanced ME-NeuralFMU","title":"NeuralFMU","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/#Loss-function-with-growing-horizon","page":"Advanced ME-NeuralFMU","title":"Loss function with growing horizon","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"In order to train our model, a loss function must be implemented. The solver of the NeuralFMU can calculate the gradient of the loss function. The gradient descent is needed to adjust the weights in the neural network so that the sum of the error is reduced and the model becomes more accurate.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"The loss function in this implementation consists of the mean squared error (mse) from the real position of the realFMU simulation (posReal) and the position data of the network (posNet). $ e{mse} = \\frac{1}{n} \\sum\\limits{i=0}^n (posReal[i] - posNet[i])^2 $ A growing horizon is applied, whereby the horizon only goes over the first five values. For this horizon the mse is calculated.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"# loss function for training\r\nglobal horizon = 5\r\nfunction lossSum(p)\r\n    global posReal, neuralFMU, horizon\r\n    solution = neuralFMU(x₀; p=p)\r\n\r\n    posNet = fmi2GetSolutionState(solution, 1; isIndex=true)\r\n    \r\n    horizon = min(length(posNet), horizon)\r\n\r\n    FMIFlux.Losses.mse(posReal[1:horizon], posNet[1:horizon])\r\nend","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"lossSum (generic function with 1 method)","category":"page"},{"location":"examples/advanced_hybrid_ME/#Function-for-plotting","page":"Advanced ME-NeuralFMU","title":"Function for plotting","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"In this section the function for plotting is defined. The function plotResults() creates a new figure object. In dieses figure objekt werden dann die aktuellsten Ergebnisse von realFMU, simpleFMU und neuralFMU gegenübergestellt. ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"To output the loss in certain time intervals, a callback is implemented as a function in the following. Here a counter is incremented, every twentieth pass the loss function is called and the average error is printed out.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"function plotResults()\r\n    global neuralFMU\r\n    solution = neuralFMU(x₀)\r\n\r\n    posNeural = fmi2GetSolutionState(solution, 1; isIndex=true)\r\n    time = fmi2GetSolutionTime(solution)\r\n    \r\n    fig = Plots.plot(xlabel=\"t [s]\", ylabel=\"mass position [m]\", linewidth=2,\r\n                     xtickfontsize=12, ytickfontsize=12,\r\n                     xguidefontsize=12, yguidefontsize=12,\r\n                     legendfontsize=8, legend=:topright)\r\n    \r\n    Plots.plot!(fig, tSave, posSimple, label=\"SimpleFMU\", linewidth=2)\r\n    Plots.plot!(fig, tSave, posReal, label=\"RealFMU\", linewidth=2)\r\n    Plots.plot!(fig, time, posNeural, label=\"NeuralFMU\", linewidth=2)\r\n    fig\r\nend","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"plotResults (generic function with 1 method)","category":"page"},{"location":"examples/advanced_hybrid_ME/#Callback","page":"Advanced ME-NeuralFMU","title":"Callback","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"To output the loss in certain time intervals, a callback is implemented as a function in the following. Here a counter is incremented, every twentieth pass the loss function is called and the average error is printed out.  As soon as a limit value (in this example 0.1) is undershot, the horizon is extended by the next two values.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"# callback function for training\r\nglobal counter = 0\r\nfunction callb(p)\r\n    global counter, horizon \r\n    counter += 1\r\n   \r\n    if counter % 20 == 1\r\n        avgLoss = lossSum(p[1])\r\n        @info \"   Loss [$counter] for horizon $horizon : $(round(avgLoss, digits=5))   \r\n        Avg displacement in data: $(round(sqrt(avgLoss), digits=5))\"\r\n        \r\n        if avgLoss <= 0.01\r\n            horizon += 2\r\n        end\r\n   \r\n        # fig = plotResults()\r\n        # println(\"Figure update.\")\r\n        # display(fig)\r\n    end\r\nend\r\n","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"callb (generic function with 1 method)","category":"page"},{"location":"examples/advanced_hybrid_ME/#Pre-and-Postprocessing","page":"Advanced ME-NeuralFMU","title":"Pre- and Postprocessing","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"In the following functions for pre-processing and post-processing are defined. The function preProc is normalized the input values to mean of zero and a standard deviation of one. ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"global meanVal = 0.0\r\nglobal stdVal = 0.0\r\n\r\nfunction preProc!(data)\r\n    global meanVal, stdVal\r\n\r\n    meanVal = mean(data)\r\n    stdVal = std(data)\r\n    \r\n    (data .- meanVal) ./ stdVal    \r\nend ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"preProc! (generic function with 1 method)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"For post-processing, the previous normalization is undone by applying the calculation steps in reverse order.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"function postProc!(data)\r\n    global meanVal, stdVal\r\n    \r\n    (data .* stdVal) .+ meanVal\r\nend ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"postProc! (generic function with 1 method)","category":"page"},{"location":"examples/advanced_hybrid_ME/#Structure-of-the-NeuralFMU","page":"Advanced ME-NeuralFMU","title":"Structure of the NeuralFMU","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"In the following, the topology of the NeuralFMU is constructed. It consists of an input layer, which then leads into the simpleFMU model. The ME-FMU computes the state derivatives for a given system state. Following the simpleFMU is a dense layer that has exactly as many inputs as the model has states (and therefore state derivatives). The output of this layer consists of 16 output nodes and a tanh activation function. The next layer has 16 input and output nodes with the same activation function. The last layer is again a dense layer with 16 input nodes and the number of states as outputs. Here, it is important that no tanh-activation function follows, because otherwise the pendulums state values would be limited to the interval -11.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"# NeuralFMU setup\r\nnumStates = fmiGetNumberOfStates(simpleFMU)\r\nadditionalVRs = [fmi2StringToValueReference(simpleFMU, \"mass.m\")]\r\nnumAdditionalVRs = length(additionalVRs)\r\n\r\nnet = Chain(\r\n    inputs -> fmiEvaluateME(simpleFMU, inputs, -1.0, zeros(fmi2ValueReference, 0), \r\n                            zeros(fmi2Real, 0), additionalVRs),\r\n    preProc!,\r\n    Dense(numStates+numAdditionalVRs, 16, tanh),\r\n    postProc!,\r\n    preProc!,\r\n    Dense(16, 16, tanh),\r\n    postProc!,\r\n    preProc!,\r\n    Dense(16, numStates),\r\n    postProc!,\r\n)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"Chain(\r\n  var\"#1#2\"(),\r\n  preProc!,\r\n  Dense(3 => 16, tanh),                 \u001b[90m# 64 parameters\u001b[39m\r\n  postProc!,\r\n  preProc!,\r\n  Dense(16 => 16, tanh),                \u001b[90m# 272 parameters\u001b[39m\r\n  postProc!,\r\n  preProc!,\r\n  Dense(16 => 2),                       \u001b[90m# 34 parameters\u001b[39m\r\n  postProc!,\r\n) \u001b[90m                  # Total: 6 arrays, \u001b[39m370 parameters, 3.266 KiB.","category":"page"},{"location":"examples/advanced_hybrid_ME/#Definition-of-the-NeuralFMU","page":"Advanced ME-NeuralFMU","title":"Definition of the NeuralFMU","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"The instantiation of the ME-NeuralFMU is done as a one-liner. The FMU (simpleFMU), the structure of the network net, start tStart and end time tStop, the numerical solver Tsit5() and the time steps tSave for saving are specified.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"neuralFMU = ME_NeuralFMU(simpleFMU, net, (tStart, tStop), Tsit5(); saveat=tSave);","category":"page"},{"location":"examples/advanced_hybrid_ME/#Plot-before-training","page":"Advanced ME-NeuralFMU","title":"Plot before training","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"Here the state trajectory of the simpleFMU is recorded. Doesn't really look like a pendulum yet, but the system is random initialized by default. In the plots later on, the effect of learning can be seen.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"solutionBefore = neuralFMU(x₀)\r\nfmiPlot(solutionBefore)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/advanced_hybrid_ME/#Training-of-the-NeuralFMU","page":"Advanced ME-NeuralFMU","title":"Training of the NeuralFMU","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"For the training of the NeuralFMU the parameters are extracted. The known Adam optimizer for minimizing the gradient descent is used as further passing parameters. In addition, the previously defined loss and callback function, as well as the number of epochs are passed.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"# train\r\nparamsNet = FMIFlux.params(neuralFMU)\r\n\r\noptim = Adam()\r\nFMIFlux.train!(lossSum, paramsNet, Iterators.repeated((), 1000), optim; cb=()->callb(paramsNet)) ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [1] for horizon 5 : 0.04681   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.21636\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [21] for horizon 5 : 0.00079   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02805\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [41] for horizon 7 : 0.003   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.05476\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [61] for horizon 9 : 0.00315   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.05608\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [81] for horizon 11 : 0.00162   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.04022\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [101] for horizon 13 : 0.00442   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.0665\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [121] for horizon 15 : 0.00999   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.09993\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [141] for horizon 17 : 0.02188   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.14792\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [161] for horizon 17 : 0.01912   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.13828\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [181] for horizon 17 : 0.01662   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.12892\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [201] for horizon 17 : 0.01306   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.11426\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [221] for horizon 17 : 0.00698   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.08356\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [241] for horizon 19 : 0.0042   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.06482\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [261] for horizon 21 : 0.00269   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.05188\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [281] for horizon 23 : 0.00141   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03759\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [301] for horizon 25 : 0.00086   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02928\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [321] for horizon 27 : 0.0008   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02828\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [341] for horizon 29 : 0.00066   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02577\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [361] for horizon 31 : 0.00065   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02546\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [381] for horizon 33 : 0.00071   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02662\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [401] for horizon 35 : 0.00067   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02581\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [421] for horizon 37 : 0.00058   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02413\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [441] for horizon 39 : 0.00054   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02323\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [461] for horizon 41 : 0.00089   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02981\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [481] for horizon 43 : 0.0005   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.0223\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [501] for horizon 45 : 0.00047   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02158\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [521] for horizon 47 : 0.00064   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.02532\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [541] for horizon 49 : 0.001   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03164\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [561] for horizon 51 : 0.00144   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03801\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [581] for horizon 51 : 0.00135   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03675\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [601] for horizon 51 : 0.00129   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03595\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [621] for horizon 51 : 0.00125   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.0354\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [641] for horizon 51 : 0.00123   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03506\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [661] for horizon 51 : 0.00118   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03439\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [681] for horizon 51 : 0.0012   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03466\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [701] for horizon 51 : 0.00119   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03446\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [721] for horizon 51 : 0.00118   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03429\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [741] for horizon 51 : 0.00117   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03423\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [761] for horizon 51 : 0.00116   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03404\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [781] for horizon 51 : 0.00114   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03382\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [801] for horizon 51 : 0.00113   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.0336\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [821] for horizon 51 : 0.00111   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03336\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [841] for horizon 51 : 0.0011   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03314\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [861] for horizon 51 : 0.00108   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03288\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [881] for horizon 51 : 0.00106   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03258\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [901] for horizon 51 : 0.00105   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.0324\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [921] for horizon 51 : 0.00103   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03217\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [941] for horizon 51 : 0.00102   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03189\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [961] for horizon 51 : 0.001   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03158\r\n\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m   Loss [981] for horizon 51 : 0.00094   \r\n\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m        Avg displacement in data: 0.03066","category":"page"},{"location":"examples/advanced_hybrid_ME/#Comparison-of-the-plots","page":"Advanced ME-NeuralFMU","title":"Comparison of the plots","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"Here three plots are compared with each other and only the position of the mass is considered. The first plot represents the simpleFMU, the second represents the realFMU (reference) and the third plot represents the result after training the NeuralFMU. ","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"# plot results mass.s\r\nplotResults()","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"Finally, the FMU is cleaned-up.","category":"page"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"fmiUnload(simpleFMU)","category":"page"},{"location":"examples/advanced_hybrid_ME/#Summary","page":"Advanced ME-NeuralFMU","title":"Summary","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"Based on the plots, it can be seen that the NeuralFMU is able to adapt the friction model of the realFMU. After 300 runs, the curves do not overlap very well, but this can be achieved by longer training (1000 runs) or a better initialization.","category":"page"},{"location":"examples/advanced_hybrid_ME/#Source","page":"Advanced ME-NeuralFMU","title":"Source","text":"","category":"section"},{"location":"examples/advanced_hybrid_ME/","page":"Advanced ME-NeuralFMU","title":"Advanced ME-NeuralFMU","text":"[1] Tobias Thummerer, Lars Mikelsons and Josef Kircher. 2021. NeuralFMU: towards structural integration of FMUs into neural networks. Martin Sjölund, Lena Buffoni, Adrian Pop and Lennart Ochel (Ed.). Proceedings of 14th Modelica Conference 2021, Linköping, Sweden, September 20-24, 2021. Linköping University Electronic Press, Linköping (Linköping Electronic Conference Proceedings ; 181), 297-306. DOI: 10.3384/ecp21181297","category":"page"},{"location":"examples/modelica_conference_2021/#ME-NeuralFMU-from-the-Modelica-Conference-2021","page":"Modelica Conference 2021","title":"ME-NeuralFMU from the Modelica Conference 2021","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Tutorial by Johannes Stoljar, Tobias Thummerer","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Last edit: 03.01.2023","category":"page"},{"location":"examples/modelica_conference_2021/#License","page":"Modelica Conference 2021","title":"License","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# Copyright (c) 2021 Tobias Thummerer, Lars Mikelsons, Johannes Stoljar\r\n# Licensed under the MIT license. \r\n# See LICENSE (https://github.com/thummeto/FMIFlux.jl/blob/main/LICENSE) file in the project root for details.","category":"page"},{"location":"examples/modelica_conference_2021/#Motivation","page":"Modelica Conference 2021","title":"Motivation","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The Julia Package FMIFlux.jl is motivated by the application of hybrid modeling. This package enables the user to integrate his simulation model between neural networks (NeuralFMU). For this, the simulation model must be exported as FMU (functional mock-up unit), which corresponds to a widely used standard. The big advantage of hybrid modeling with artificial neural networks is, that the effects that are difficult to model (because they might be unknown) can be easily learned by the neural networks. For this purpose, the NeuralFMU is trained with measurement data containing the not modeled physical effect. The final product is a simulation model including the originally not modeled effects. Another big advantage of the NeuralFMU is that it works with little data, because the FMU already contains the characteristic functionality of the simulation and only the missing effects are added.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"NeuralFMUs do not need to be as easy as in this example. Basically a NeuralFMU can combine different ANN topologies that manipulate any FMU-input (system state, system inputs, time) and any FMU-output (system state derivative, system outputs, other system variables). However, for this example a NeuralFMU topology as shown in the following picture is used.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: NeuralFMU.svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"NeuralFMU (ME) from [1].","category":"page"},{"location":"examples/modelica_conference_2021/#Introduction-to-the-example","page":"Modelica Conference 2021","title":"Introduction to the example","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In this example, simplified modeling of a one-dimensional spring pendulum (without friction) is compared to a model of the same system that includes a nonlinear friction model. The FMU with the simplified model will be named simpleFMU in the following and the model with the friction will be named realFMU. At the beginning, the actual state of both simulations is shown, whereby clear deviations can be seen in the graphs. In addition, the initial states are changed for both models and these graphs are also contrasted, and the differences can again be clearly seen. The realFMU serves as a reference graph. The simpleFMU is then integrated into a NeuralFMU architecture and a training of the entire network is performed. After the training the final state is compared again to the realFMU. It can be clearly seen that by using the NeuralFMU, learning of the friction process has taken place.  ","category":"page"},{"location":"examples/modelica_conference_2021/#Target-group","page":"Modelica Conference 2021","title":"Target group","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The example is primarily intended for users who work in the field of first principle and/or hybrid modeling and are further interested in hybrid model building. The example wants to show how simple it is to combine FMUs with machine learning and to illustrate the advantages of this approach.","category":"page"},{"location":"examples/modelica_conference_2021/#Other-formats","page":"Modelica Conference 2021","title":"Other formats","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Besides, this Jupyter Notebook there is also a Julia file with the same name, which contains only the code cells. For the documentation there is a Markdown file corresponding to the notebook.  ","category":"page"},{"location":"examples/modelica_conference_2021/#Getting-started","page":"Modelica Conference 2021","title":"Getting started","text":"","category":"section"},{"location":"examples/modelica_conference_2021/#Installation-prerequisites","page":"Modelica Conference 2021","title":"Installation prerequisites","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":" Description Command\n1. Enter Package Manager via ]\n2. Install FMI via add FMI\n3. Install FMIFlux via add FMIFlux\n4. Install FMIZoo via add FMIZoo\n5. Install DifferentialEquations via add DifferentialEquations\n6. Install Plots via add Plots\n7. Install Random via add Random","category":"page"},{"location":"examples/modelica_conference_2021/#Code-section","page":"Modelica Conference 2021","title":"Code section","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"To run the example, the previously installed packages must be included. ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# imports\r\nusing FMI\r\nusing FMIFlux\r\nusing FMIZoo\r\nusing DifferentialEquations: Tsit5\r\nimport Plots\r\n\r\n# set seed\r\nimport Random\r\nRandom.seed!(1234);","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"After importing the packages, the path to the Functional Mock-up Units (FMUs) is set. The exported FMU is a model meeting the Functional Mock-up Interface (FMI) Standard. The FMI is a free standard (fmi-standard.org) that defines a container and an interface to exchange dynamic models using a combination of XML files, binaries and C code zipped into a single file. ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The object-orientated structure of the SpringPendulum1D (simpleFMU) can be seen in the following graphic and corresponds to a simple modeling.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In contrast, the model SpringFrictionPendulum1D (realFMU) is somewhat more accurate, because it includes a friction component. ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Next, the start time and end time of the simulation are set. Finally, a step size is specified to store the results of the simulation at these time steps.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"tStart = 0.0\r\ntStep = 0.01\r\ntStop = 4.0\r\ntSave = collect(tStart:tStep:tStop)","category":"page"},{"location":"examples/modelica_conference_2021/#RealFMU","page":"Modelica Conference 2021","title":"RealFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the next lines of code the FMU of the realFMU model from FMIZoo.jl is loaded and the information about the FMU is shown.  ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"realFMU = fmiLoad(\"SpringFrictionPendulum1D\", \"Dymola\", \"2022x\")\r\nfmiInfo(realFMU)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the following two subsections, the realFMU is simulated twice with different initial states to show what effect the choice of initial states has.","category":"page"},{"location":"examples/modelica_conference_2021/#Default-initial-states","page":"Modelica Conference 2021","title":"Default initial states","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the next steps the parameters are defined. The first parameter is the initial position of the mass, which is initialized with 05m, the second parameter is the initial velocity, which is initialized with 0fracms. In the function fmiSimulate() the realFMU is simulated, still specifying the start and end time, the parameters and which variables are recorded. After the simulation is finished the result of the realFMU can be plotted. This plot also serves as a reference for the other model (simpleFMU). The extracted data will still be needed later on.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"initStates = [\"s0\", \"v0\"]\r\nx₀ = [0.5, 0.0]\r\nparams = Dict(zip(initStates, x₀))\r\nvrs = [\"mass.s\", \"mass.v\", \"mass.a\", \"mass.f\"]\r\n\r\nrealSimData = fmiSimulate(realFMU, (tStart, tStop); parameters=params, recordValues=vrs, saveat=tSave)\r\nposReal = fmi2GetSolutionValue(realSimData, \"mass.s\")\r\nvelReal = fmi2GetSolutionValue(realSimData, \"mass.v\")\r\nfmiPlot(realSimData)","category":"page"},{"location":"examples/modelica_conference_2021/#Define-functions","page":"Modelica Conference 2021","title":"Define functions","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The structure of the previous code section is used more often in the further sections, so for clarity the previously explained code section for setting the paramters and simulating are combined into one function simulate().","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function simulate(FMU, initStates, x₀, variables, tStart, tStop, tSave)\r\n    params = Dict(zip(initStates, x₀))\r\n    return fmiSimulate(FMU, (tStart, tStop); parameters=params, recordValues=variables, saveat=tSave)\r\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Also, a function to extract the position and velocity from the simulation data is created.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function extractPosVel(simData)\r\n    if simData.states === nothing\r\n        posData = fmi2GetSolutionValue(simData, \"mass.s\")\r\n        velData = fmi2GetSolutionValue(simData, \"mass.v\")\r\n    else\r\n        posData = fmi2GetSolutionState(simData, 1; isIndex=true)\r\n        velData = fmi2GetSolutionState(simData, 2; isIndex=true)\r\n    end\r\n\r\n    return posData, velData\r\nend","category":"page"},{"location":"examples/modelica_conference_2021/#Modified-initial-states","page":"Modelica Conference 2021","title":"Modified initial states","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In contrast to the previous section, other initial states are selected. The position of the mass is initialized with 10m and the velocity is initialized with -15fracms. With the modified initial states the realFMU is simulated and a graph is generated.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"xMod₀ = [1.0, -1.5]\r\nrealSimDataMod = simulate(realFMU, initStates, xMod₀, vrs, tStart, tStop, tSave)\r\nfmiPlot(realSimDataMod)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"After the plots are created, the FMU is unloaded.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"fmiUnload(realFMU)","category":"page"},{"location":"examples/modelica_conference_2021/#SimpleFMU","page":"Modelica Conference 2021","title":"SimpleFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The following lines load the simpleFMU from FMIZoo.jl. ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"simpleFMU = fmiLoad(\"SpringPendulum1D\", \"Dymola\", \"2022x\")\r\nfmiInfo(simpleFMU)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The differences between both systems can be clearly seen from the plots in the subchapters. In the plot for the realFMU it can be seen that the oscillation continues to decrease due to the effect of the friction. If you simulate long enough, the oscillation would come to a standstill in a certain time. The oscillation in the simpleFMU behaves differently, since the friction was not taken into account here. The oscillation in this model would continue to infinity with the same oscillation amplitude. From this observation the desire of an improvement of this model arises.     ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the following two subsections, the simpleFMU is simulated twice with different initial states to show what effect the choice of initial states has.","category":"page"},{"location":"examples/modelica_conference_2021/#Default-initial-states-2","page":"Modelica Conference 2021","title":"Default initial states","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Similar to the simulation of the realFMU, the simpleFMU is also simulated with the default values for the position and velocity of the mass and then plotted. There is one difference, however, as another state representing a fixed displacement is set. In addition, the last variable is also removed from the variables to be plotted.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"initStates = [\"mass_s0\", \"mass_v0\", \"fixed.s0\"]\r\ndisplacement = 0.1\r\nxSimple₀ = vcat(x₀, displacement)\r\nvrs = vrs[1:end-1]\r\n\r\nsimpleSimData = simulate(simpleFMU, initStates, xSimple₀, vrs, tStart, tStop, tSave)\r\nfmiPlot(simpleSimData)","category":"page"},{"location":"examples/modelica_conference_2021/#Modified-initial-states-2","page":"Modelica Conference 2021","title":"Modified initial states","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The same values for the initial states are used for this simulation as for the simulation from the realFMU with the modified initial states.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"xSimpleMod₀ = vcat(xMod₀, displacement)\r\n\r\nsimpleSimDataMod = simulate(simpleFMU, initStates, xSimpleMod₀, vrs, tStart, tStop, tSave)\r\nfmiPlot(simpleSimDataMod)","category":"page"},{"location":"examples/modelica_conference_2021/#NeuralFMU","page":"Modelica Conference 2021","title":"NeuralFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/#Loss-function","page":"Modelica Conference 2021","title":"Loss function","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In order to train our model, a loss function must be implemented. The solver of the NeuralFMU can calculate the gradient of the loss function. The gradient descent is needed to adjust the weights in the neural network so that the sum of the error is reduced and the model becomes more accurate.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The error function in this implementation consists of the mean of the mean squared errors. The first part of the addition is the deviation of the position and the second part is the deviation of the velocity. The mean squared error (mse) for the position consists from the real position of the realFMU simulation (posReal) and the position data of the network (posNet). The mean squared error for the velocity consists of the real velocity of the realFMU simulation (velReal) and the velocity data of the network (velNet). $ e{loss} = \\frac{1}{2} \\Bigl[ \\frac{1}{n} \\sum\\limits{i=0}^n (posReal[i] - posNet[i])^2 + \\frac{1}{n} \\sum\\limits_{i=0}^n (velReal[i] - velNet[i])^2 \\Bigr]$","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# loss function for training\r\nfunction lossSum(p)\r\n    global x₀\r\n    solution = neuralFMU(x₀; p=p)\r\n\r\n    posNet, velNet = extractPosVel(solution)\r\n\r\n    (FMIFlux.Losses.mse(posReal, posNet) + FMIFlux.Losses.mse(velReal, velNet)) / 2.0\r\nend","category":"page"},{"location":"examples/modelica_conference_2021/#Callback","page":"Modelica Conference 2021","title":"Callback","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"To output the loss in certain time intervals, a callback is implemented as a function in the following. Here a counter is incremented, every fiftieth pass the loss function is called and the average error is printed out. Also, the parameters for the velocity in the first layer are kept to a fixed value.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# callback function for training\r\nglobal counter = 0\r\nfunction callb(p)\r\n    global counter\r\n    counter += 1\r\n\r\n    # freeze first layer parameters (2,4,6) for velocity -> (static) direct feed trough for velocity\r\n    # parameters for position (1,3,5) are learned\r\n    p[1][2] = 0.0\r\n    p[1][4] = 1.0\r\n    p[1][6] = 0.0\r\n\r\n    if counter % 50 == 1\r\n        avgLoss = lossSum(p[1])\r\n        @info \"  Loss [$counter]: $(round(avgLoss, digits=5))\r\n        Avg displacement in data: $(round(sqrt(avgLoss), digits=5))\r\n        Weight/Scale: $(paramsNet[1][1])   Bias/Offset: $(paramsNet[1][5])\"\r\n    end\r\nend","category":"page"},{"location":"examples/modelica_conference_2021/#Functions-for-plotting","page":"Modelica Conference 2021","title":"Functions for plotting","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In this section some important functions for plotting are defined. The function generate_figure() creates a new figure object and sets some attributes.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function generate_figure(title, xLabel, yLabel, xlim=:auto)\r\n    Plots.plot(\r\n        title=title, xlabel=xLabel, ylabel=yLabel, linewidth=2,\r\n        xtickfontsize=12, ytickfontsize=12, xguidefontsize=12, yguidefontsize=12,\r\n        legendfontsize=12, legend=:topright, xlim=xlim)\r\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the following function, the data of the realFMU, simpleFMU and neuralFMU are summarized and displayed in a graph.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function plot_results(title, xLabel, yLabel, interval, realData, simpleData, neuralData)\r\n    linestyles = [:dot, :solid]\r\n    \r\n    fig = generate_figure(title, xLabel, yLabel)\r\n    Plots.plot!(fig, interval, simpleData, label=\"SimpleFMU\", linewidth=2)\r\n    Plots.plot!(fig, interval, realData, label=\"Reference\", linewidth=2)\r\n    for i in 1:length(neuralData)\r\n        Plots.plot!(fig, neuralData[i][1], neuralData[i][2], label=\"NeuralFMU ($(i*2500))\", \r\n                    linewidth=2, linestyle=linestyles[i], linecolor=:green)\r\n    end\r\n    Plots.display(fig)\r\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"This is the superordinate function, which at the beginning extracts the position and velocity from the simulation data (realSimData, realSimDataMod, simpleSimData,..., solutionAfterMod). Four graphs are then generated, each comparing the corresponding data from the realFMU, simpleFMU, and neuralFMU. The comparison is made with the simulation data from the simulation with the default and modified initial states. According to the data, the designation of the title and the naming of the axes is adapted.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function plot_all_results(realSimData, realSimDataMod, simpleSimData, \r\n        simpleSimDataMod, solutionAfter, solutionAfterMod)    \r\n    # collect all data\r\n    posReal, velReal = extractPosVel(realSimData)\r\n    posRealMod, velRealMod = extractPosVel(realSimDataMod)\r\n    posSimple, velSimple = extractPosVel(simpleSimData)\r\n    posSimpleMod, velSimpleMod = extractPosVel(simpleSimDataMod)\r\n    \r\n    run = length(solutionAfter)\r\n    \r\n    posNeural, velNeural = [], []\r\n    posNeuralMod, velNeuralMod = [], []\r\n    for i in 1:run\r\n        dataNeural = extractPosVel(solutionAfter[i])\r\n        time = fmi2GetSolutionTime(solutionAfter[i])\r\n\r\n        push!(posNeural, (time, dataNeural[1]))\r\n        push!(velNeural, (time, dataNeural[2]))\r\n        \r\n        dataNeuralMod = extractPosVel(solutionAfterMod[i])\r\n        time = fmi2GetSolutionTime(solutionAfterMod[i])\r\n        push!(posNeuralMod, (time, dataNeuralMod[1]))\r\n        push!(velNeuralMod, (time, dataNeuralMod[2]))\r\n    end\r\n         \r\n    # plot results s (default initial states)\r\n    xLabel=\"t [s]\"\r\n    yLabel=\"mass position [m]\"\r\n    title = \"Default: Mass position after Run: $(run)\"\r\n    plot_results(title, xLabel, yLabel, tSave, posReal, posSimple, posNeural)\r\n\r\n    # plot results s (modified initial states)\r\n    title = \"Modified: Mass position after Run: $(run)\"\r\n    plot_results(title, xLabel, yLabel, tSave, posRealMod, posSimpleMod, posNeuralMod)\r\n\r\n    # plot results v (default initial states)\r\n    yLabel=\"mass velocity [m/s]\"\r\n    title = \"Default: Mass velocity after Run: $(run)\"\r\n    plot_results(title, xLabel, yLabel, tSave, velReal, velSimple, velNeural)\r\n\r\n    # plot results v (modified initial states)    \r\n    title = \"Modified: Mass velocity after Run: $(run)\"\r\n    plot_results(title, xLabel, yLabel, tSave, velRealMod, velSimpleMod, velNeuralMod)\r\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The function plot_friction_model() compares the friction model of the realFMU, simpleFMU and neuralFMU. For this, the velocity and force from the simulation data of the realFMU is needed. The force data is calculated with the extracted last layer of the neuralFMU to the real velocity in line 9 by iterating over the vector velReal. In the next rows, the velocity and force data (if available) for each of the three FMUs are combined into a matrix. The first row of the matrix corresponds to the later x-axis and here the velocity is plotted. The second row corresponds to the y-axis and here the force is plotted. This matrix is sorted and plotted by the first entries (velocity) with the function sortperm(). The graph with at least three graphs is plotted in line 33. As output this function has the forces of the neuralFMU.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function plot_friction_model(realSimData, netBottom, forces)    \r\n    linestyles = [:dot, :solid]\r\n    \r\n    velReal = fmi2GetSolutionValue(realSimData, \"mass.v\")\r\n    forceReal = fmi2GetSolutionValue(realSimData, \"mass.f\")\r\n\r\n    push!(forces, zeros(length(velReal)))\r\n    for i in 1:length(velReal)\r\n        forces[end][i] = -netBottom([velReal[i], 0.0])[2]\r\n    end\r\n\r\n    run = length(forces) \r\n    \r\n    fig = generate_figure(\"Friction model $(run)\", \"v [m/s]\", \"friction force [N]\", (-1.25, 1.25))\r\n\r\n    fricSimple = hcat(velReal, zeros(length(velReal)))\r\n    fricSimple[sortperm(fricSimple[:, 1]), :]\r\n    Plots.plot!(fig, fricSimple[:,1], fricSimple[:,2], label=\"SimpleFMU\", linewidth=2)\r\n\r\n    fricReal = hcat(velReal, forceReal)\r\n    fricReal[sortperm(fricReal[:, 1]), :]\r\n    Plots.plot!(fig, fricReal[:,1], fricReal[:,2], label=\"reference\", linewidth=2)\r\n\r\n    for i in 1:run\r\n        fricNeural = hcat(velReal, forces[i])\r\n        fricNeural[sortperm(fricNeural[:, 1]), :]\r\n        Plots.plot!(fig, fricNeural[:,1], fricNeural[:,2], label=\"NeuralFMU ($(i*2500))\", \r\n                    linewidth=2, linestyle=linestyles[i], linecolor=:green)\r\n        @info \"Friction model $i mse: $(FMIFlux.Losses.mse(fricNeural[:,2], fricReal[:,2]))\"\r\n    end\r\n    flush(stderr)\r\n\r\n    Plots.display(fig)\r\n    \r\n    return forces   \r\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The following function is used to display the different displacement modells of the realFMU, simpleFMU and neuralFMU. The displacement of the realFMU and simpleFMU is very trivial and is only a constant. The position data of the realFMU is needed to calculate the displacement. The displacement for the neuralFMU is calculated using the first extracted layer of the neural network, subtracting the real position and the displacement of the simpleFMU. Also in this function, the graphs of the three FMUs are compared in a plot.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function plot_displacement_model(realSimData, netTop, displacements, tSave, displacement)\r\n    linestyles = [:dot, :solid]\r\n    \r\n    posReal = fmi2GetSolutionValue(realSimData, \"mass.s\")\r\n    \r\n    push!(displacements, zeros(length(posReal)))\r\n    for i in 1:length(posReal)\r\n        displacements[end][i] = netTop([posReal[i], 0.0])[1] - posReal[i] - displacement\r\n    end\r\n\r\n    run = length(displacements)\r\n    fig = generate_figure(\"Displacement model $(run)\", \"t [s]\", \"displacement [m]\")\r\n    Plots.plot!(fig, [tSave[1], tSave[end]], [displacement, displacement], label=\"simpleFMU\", linewidth=2)\r\n    Plots.plot!(fig, [tSave[1], tSave[end]], [0.0, 0.0], label=\"reference\", linewidth=2)\r\n    for i in 1:run\r\n        Plots.plot!(fig, tSave, displacements[i], label=\"NeuralFMU ($(i*2500))\", \r\n                    linewidth=2, linestyle=linestyles[i], linecolor=:green)\r\n    end\r\n\r\n    Plots.display(fig)\r\n    \r\n    return displacements\r\nend","category":"page"},{"location":"examples/modelica_conference_2021/#Structure-of-the-NeuralFMU","page":"Modelica Conference 2021","title":"Structure of the NeuralFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the following, the topology of the NeuralFMU is constructed. It consists of a dense layer that has exactly as many inputs and outputs as the model has states numStates (and therefore state derivatives). It also sets the initial weights and offsets for the first dense layer, as well as the activation function, which consists of the identity. An input layer follows, which then leads into the simpleFMU model. The ME-FMU computes the state derivatives for a given system state. Following the simpleFMU is a dense layer that has numStates states. The output of this layer consists of 8 output nodes and a identity activation function. The next layer has 8 input and output nodes with a tanh activation function. The last layer is again a dense layer with 8 input nodes and the number of states as outputs. Here, it is important that no tanh-activation function follows, because otherwise the pendulums state values would be limited to the interval -11.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# NeuralFMU setup\r\nnumStates = fmiGetNumberOfStates(simpleFMU)\r\n\r\n# diagonal matrix \r\ninitW = zeros(numStates, numStates)\r\nfor i in 1:numStates\r\n    initW[i,i] = 1\r\nend\r\n\r\nnet = Chain(# Dense(initW, zeros(numStates),  identity),\r\n            Dense(numStates, numStates,  identity),\r\n            inputs -> fmiEvaluateME(simpleFMU, inputs),\r\n            Dense(numStates, 8, identity),\r\n            Dense(8, 8, tanh),\r\n            Dense(8, numStates))","category":"page"},{"location":"examples/modelica_conference_2021/#Definition-of-the-NeuralFMU","page":"Modelica Conference 2021","title":"Definition of the NeuralFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The instantiation of the ME-NeuralFMU is done as a one-liner. The FMU (simpleFMU), the structure of the network net, start tStart and end time tStop, the numerical solver Tsit5() and the time steps tSave for saving are specified.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"neuralFMU = ME_NeuralFMU(simpleFMU, net, (tStart, tStop), Tsit5(); saveat=tSave);","category":"page"},{"location":"examples/modelica_conference_2021/#Plot-before-training","page":"Modelica Conference 2021","title":"Plot before training","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Here the state trajectory of the simpleFMU is recorded. Doesn't really look like a pendulum yet, but the system is random initialized by default. In the plots later on, the effect of learning can be seen.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"solutionBefore = neuralFMU(x₀)\r\nfmiPlot(solutionBefore)","category":"page"},{"location":"examples/modelica_conference_2021/#Training-of-the-NeuralFMU","page":"Modelica Conference 2021","title":"Training of the NeuralFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"For the training of the NeuralFMU the parameters are extracted. All parameters of the first layer are set to the absolute value.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# train\r\nparamsNet = FMIFlux.params(neuralFMU)\r\n\r\nfor i in 1:length(paramsNet[1])\r\n    if paramsNet[1][i] < 0.0 \r\n        paramsNet[1][i] = -paramsNet[1][i]\r\n    end\r\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The well-known Adam optimizer for minimizing the gradient descent is used as further passing parameters. Additionally, the previously defined loss and callback function as well as a one for the number of epochs are passed. Only one epoch is trained so that the NeuralFMU is precompiled.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"optim = Adam()\r\nFMIFlux.train!(lossSum, paramsNet, Iterators.repeated((), 1), optim; cb=()->callb(paramsNet)) ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Some vectors for collecting data are initialized and the number of runs, epochs and iterations are set.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"solutionAfter = []\r\nsolutionAfterMod = []\r\nforces = []\r\ndisplacements = []\r\n\r\nnumRuns = 2\r\nnumEpochs= 5\r\nnumIterations = 500;","category":"page"},{"location":"examples/modelica_conference_2021/#Training-loop","page":"Modelica Conference 2021","title":"Training loop","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The code section shown here represents the training loop. The loop is structured so that it has numRuns runs, where each run has numEpochs epochs, and the training is performed at each epoch with numIterations iterations. In lines 9 and 10, the data for the neuralFMU for the default and modified initial states are appended to the corresponding vectors. The plots for the opposition of position and velocity is done in line 13 by calling the function plot_all_results. In the following lines the last layers are extracted from the neuralFMU and formed into an independent network netBottom. The parameters for the netBottom network come from the original architecture and are shared. In line 20, the new network is used to represent the friction model in a graph. An analogous construction of the next part of the training loop, where here the first layer is taken from the neuralFMU and converted to its own network netTop. This network is used to record the displacement model. The different graphs are generated for each run and can thus be compared. ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"for run in 1:numRuns\r\n    @time for epoch in 1:numEpochs\r\n        @info \"Run: $(run)/$(numRuns)  Epoch: $(epoch)/$(numEpochs)\"\r\n        FMIFlux.train!(lossSum, paramsNet, Iterators.repeated((), numIterations), optim; cb=()->callb(paramsNet))\r\n    end\r\n    flush(stderr)\r\n    flush(stdout)\r\n    \r\n    push!(solutionAfter, neuralFMU(x₀))\r\n    push!(solutionAfterMod, neuralFMU(xMod₀))\r\n\r\n    # generate all plots for the position and velocity\r\n    plot_all_results(realSimData, realSimDataMod, simpleSimData, simpleSimDataMod, solutionAfter, solutionAfterMod)\r\n    \r\n    # friction model extraction\r\n    layersBottom = neuralFMU.neuralODE.model.layers[3:5]\r\n    netBottom = Chain(layersBottom...)\r\n    transferFlatParams!(netBottom, paramsNet, 7)\r\n    \r\n    forces = plot_friction_model(realSimData, netBottom, forces) \r\n    \r\n    # displacement model extraction\r\n    layersTop = neuralFMU.neuralODE.model.layers[1:1]\r\n    netTop = Chain(layersTop...)\r\n    transferFlatParams!(netTop, paramsNet, 1)\r\n\r\n    displacements = plot_displacement_model(realSimData, netTop, displacements, tSave, displacement)\r\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Finally, the FMU is cleaned-up.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"fmiUnload(simpleFMU)","category":"page"},{"location":"examples/modelica_conference_2021/#Summary","page":"Modelica Conference 2021","title":"Summary","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Based on the plots, it can be seen that the curves of the realFMU and the neuralFMU are very close. The neuralFMU is able to learn the friction and displacement model.","category":"page"},{"location":"examples/modelica_conference_2021/#Source","page":"Modelica Conference 2021","title":"Source","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"[1] Tobias Thummerer, Lars Mikelsons and Josef Kircher. 2021. NeuralFMU: towards structural integration of FMUs into neural networks. Martin Sjölund, Lena Buffoni, Adrian Pop and Lennart Ochel (Ed.). Proceedings of 14th Modelica Conference 2021, Linköping, Sweden, September 20-24, 2021. Linköping University Electronic Press, Linköping (Linköping Electronic Conference Proceedings ; 181), 297-306. DOI: 10.3384/ecp21181297","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: FMIFlux.jl Logo)","category":"page"},{"location":"#FMIFlux.jl","page":"Introduction","title":"FMIFlux.jl","text":"","category":"section"},{"location":"#What-is-FMIFlux.jl?","page":"Introduction","title":"What is FMIFlux.jl?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"FMIFlux.jl is a free-to-use software library for the Julia programming language, which offers the ability to set up NeuralFMUs just like NeuralODEs: You can place FMUs (fmi-standard.org) simply inside any feed-forward ANN topology and still keep the resulting hybrid model trainable with a standard (or custom) FluxML training process.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: Dev Docs)  (Image: Run Tests) (Image: Run Examples) (Image: Build Docs) (Image: Coverage) (Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages)","category":"page"},{"location":"#How-can-I-use-FMIFlux.jl?","page":"Introduction","title":"How can I use FMIFlux.jl?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"1. Open a Julia-REPL, switch to package mode using ], activate your preferred environment.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"2. Install  FMIFlux.jl:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(@v1.x) pkg> add FMIFlux","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"3. If you want to check that everything works correctly, you can run the tests bundled with FMIFlux.jl:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(@v1.x) pkg> test FMIFlux","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"4. Have a look inside the examples folder in the examples branch or the examples section of the documentation. All examples are available as Julia-Script (.jl), Jupyter-Notebook (.ipynb) and Markdown (.md).","category":"page"},{"location":"#What-is-currently-supported-in-FMIFlux.jl?","page":"Introduction","title":"What is currently supported in FMIFlux.jl?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"building and training ME-NeuralFMUs (event-handling is supported) with the default Flux-Front-End\nbuilding and training CS-NeuralFMUs with the default Flux-Front-End\n...","category":"page"},{"location":"#What-is-under-development-in-FMIFlux.jl?","page":"Introduction","title":"What is under development in FMIFlux.jl?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"performance optimizations\ndifferent modes for sensitivity estimation\nimproved documentation\nmore examples\n...","category":"page"},{"location":"#What-Platforms-are-supported?","page":"Introduction","title":"What Platforms are supported?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"FMIFlux.jl is tested (and testing) under Julia versions 1.6 LTS and latest on Windows (latest) and Ubuntu (latest). MacOS should work, but untested. However, please use Julia versions >= 1.7 if possible, because FMIFlux.jl runs a lot faster with these Julia versions. FMIFlux.jl currnetly only works with FMI2-FMUs.","category":"page"},{"location":"#What-FMI.jl-Library-should-I-use?","page":"Introduction","title":"What FMI.jl-Library should I use?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: FMI.jl Family) To keep dependencies nice and clean, the original package FMI.jl had been split into new packages:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"FMI.jl: High level loading, manipulating, saving or building entire FMUs from scratch\nFMIImport.jl: Importing FMUs into Julia\nFMIExport.jl: Exporting stand-alone FMUs from Julia Code\nFMICore.jl: C-code wrapper for the FMI-standard\nFMIBuild.jl: Compiler/Compilation dependencies for FMIExport.jl\nFMIFlux.jl: Machine Learning with FMUs (differentiation over FMUs)\nFMIZoo.jl: A collection of testing and example FMUs","category":"page"},{"location":"#How-to-cite?","page":"Introduction","title":"How to cite?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Tobias Thummerer, Johannes Stoljar and Lars Mikelsons. 2022. NeuralFMU: presenting a workflow for integrating hybrid NeuralODEs into real-world applications. Electronics 11, 19, 3202. DOI: 10.3390/electronics11193202","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Tobias Thummerer, Lars Mikelsons and Josef Kircher. 2021. NeuralFMU: towards structural integration of FMUs into neural networks. Martin Sjölund, Lena Buffoni, Adrian Pop and Lennart Ochel (Ed.). Proceedings of 14th Modelica Conference 2021, Linköping, Sweden, September 20-24, 2021. Linköping University Electronic Press, Linköping (Linköping Electronic Conference Proceedings ; 181), 297-306. DOI: 10.3384/ecp21181297","category":"page"},{"location":"#Related-publications?","page":"Introduction","title":"Related publications?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Tobias Thummerer, Johannes Tintenherr, Lars Mikelsons 2021. Hybrid modeling of the human cardiovascular system using NeuralFMUs Journal of Physics: Conference Series 2090, 1, 012155. DOI: 10.1088/1742-6596/2090/1/012155","category":"page"}]
}
