var documenterSearchIndex = {"docs":
[{"location":"library/overview/#library","page":"Library Functions","title":"Library Functions","text":"","category":"section"},{"location":"library/overview/","page":"Library Functions","title":"Library Functions","text":"","category":"page"},{"location":"library/overview/#FMIFlux-functions","page":"Library Functions","title":"FMIFlux functions","text":"","category":"section"},{"location":"library/overview/","page":"Library Functions","title":"Library Functions","text":"CS_NeuralFMU\nME_NeuralFMU\nNeuralFMU","category":"page"},{"location":"library/overview/#FMIFlux.CS_NeuralFMU","page":"Library Functions","title":"FMIFlux.CS_NeuralFMU","text":"Structure definition for a NeuralFMU, that runs in mode Co-Simulation (CS).\n\n\n\n\n\n","category":"type"},{"location":"library/overview/#FMIFlux.ME_NeuralFMU","page":"Library Functions","title":"FMIFlux.ME_NeuralFMU","text":"Structure definition for a NeuralFMU, that runs in mode Model Exchange (ME).\n\n\n\n\n\n","category":"type"},{"location":"library/overview/#FMIFlux.NeuralFMU","page":"Library Functions","title":"FMIFlux.NeuralFMU","text":"The mutable struct representing an abstract (simulation mode unknown) NeuralFMU.\n\n\n\n\n\n","category":"type"},{"location":"library/overview/#FMI-2-version-dependent-functions","page":"Library Functions","title":"FMI 2 version dependent functions","text":"","category":"section"},{"location":"library/overview/","page":"Library Functions","title":"Library Functions","text":"fmi2DoStepCS\nfmi2DoStepME\nfmi2InputDoStepCSOutput","category":"page"},{"location":"library/overview/#FMIFlux.fmi2DoStepCS","page":"Library Functions","title":"FMIFlux.fmi2DoStepCS","text":"Performs a fmiDoStep for CS-FMUs (note, that fmiDoStep is for CS-FMUs only).\n\nOptional, FMU-values can be set via keyword arguments setValueReferences and setValues. Optional, FMU-values can be retrieved by keyword argument getValueReferences.\n\nFunction returns the FMU-values for the optional keyword argument getValueReferences. The CS-FMU performs one macro step with step size dt. Dependent on the integrated numerical solver, the FMU may perform multiple (internal) micro steps if needed to meet solver requirements (stability/accuracy). These micro steps are hidden by FMI2.\n\n\n\n\n\n","category":"function"},{"location":"library/overview/#FMIFlux.fmi2InputDoStepCSOutput","page":"Library Functions","title":"FMIFlux.fmi2InputDoStepCSOutput","text":"fmi2InputDoStepCSOutput(comp::FMU2Component, \n                        dt::Real, \n                        u::Array{<:Real})\n\nSets all FMU inputs to u, performs a ´´´fmi2DoStep´´´ and returns all FMU outputs.\n\n\n\n\n\n","category":"function"},{"location":"library/overview/#FMI-version-independent-functions","page":"Library Functions","title":"FMI version independent functions","text":"","category":"section"},{"location":"library/overview/","page":"Library Functions","title":"Library Functions","text":"fmiDoStepCS\nfmiDoStepME\nfmiInputDoStepCSOutput","category":"page"},{"location":"library/overview/#FMIFlux.fmiDoStepCS","page":"Library Functions","title":"FMIFlux.fmiDoStepCS","text":"Wrapper. Call fmi2DoStepCS for more information.\n\n\n\n\n\n","category":"function"},{"location":"library/overview/#FMIFlux.fmiInputDoStepCSOutput","page":"Library Functions","title":"FMIFlux.fmiInputDoStepCSOutput","text":"Wrapper. Call fmi2InputDoStepCSOutput for more information.\n\n\n\n\n\n","category":"function"},{"location":"library/overview/#Additional-functions","page":"Library Functions","title":"Additional functions","text":"","category":"section"},{"location":"library/overview/","page":"Library Functions","title":"Library Functions","text":"mse_interpolate\ntransferParams!","category":"page"},{"location":"library/overview/#FMIFlux.mse_interpolate","page":"Library Functions","title":"FMIFlux.mse_interpolate","text":"Compares non-equidistant (or equdistant) datapoints by linear interpolating and comparing at given interpolation points t_comp.  (Zygote-friendly: Zygote can differentiate through via AD.)\n\n\n\n\n\n","category":"function"},{"location":"library/overview/#FMIFlux.transferParams!","page":"Library Functions","title":"FMIFlux.transferParams!","text":"Writes/Copies training parameters from p_net to net with data offset c.\n\n\n\n\n\n","category":"function"},{"location":"examples/simple_hybrid_CS/#Creation-and-training-of-CS-NeuralFMUs","page":"Simple CS-NeuralFMU","title":"Creation and training of CS-NeuralFMUs","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Tutorial by Johannes Stoljar, Tobias Thummerer","category":"page"},{"location":"examples/simple_hybrid_CS/#License","page":"Simple CS-NeuralFMU","title":"License","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Copyright (c) 2021 Tobias Thummerer, Lars Mikelsons, Johannes Stoljar","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Licensed under the MIT license. See LICENSE file in the project root for details.","category":"page"},{"location":"examples/simple_hybrid_CS/#Motivation","page":"Simple CS-NeuralFMU","title":"Motivation","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"This Julia Package is motivated by the application of hybrid modeling. This package enables the user to integrate his simulation model between neural networks (NeuralFMU). For this, the simulation model must be exported as FMU (functional mock-up unit), which corresponds to a widely used standard. The big advantage of hybrid modeling with artificial neural networks is, that effects that are difficult to model (because they might be unknown) can be easily learned by the neural networks. For this purpose, the NeuralFMU is trained with measurement data containing the unmodeled physical effect. The final product is a simulation model including the orignially unmodeled effects. Another big advantage of the NeuralFMU is that it works with little data, because the FMU already contains the characterisitic functionality of the simulation and only the missing effects are added.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"NeuralFMUs need not to be as easy as in this example. Basically a NeuralFMU can combine different ANN topologies that manipulate any FMU-input (system state, system inputs, time) and any FMU-output (system state derivative, system outputs, other system variables). However, for this example a NeuralFMU topology as shown in the following picture is used.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: CS-NeuralFMU.svg)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"NeuralFMU (CS) from [1].","category":"page"},{"location":"examples/simple_hybrid_CS/#Introduction-to-the-example","page":"Simple CS-NeuralFMU","title":"Introduction to the example","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In this example, the model of a one-dimensional spring pendulum (with an external acting force) is used to learn the initial states. For this purpose, on the one hand the initial position of the mass of the pendulum is shifted and on the other hand the default position of the mass from the model is used. The model with the shifted initial position serves as reference and is called referenceFMU in the following. The model with the default position is further referenced with defaultFMU. At the beginning, the actual state of both simulations is shown, whereby clear deviations can be seen in the graphs. Afterwards, the defaultFMU is integrated into a co-simulation NeuralFMU (CS-NeuralFMU) architecture. By training the NeuralFMU, an attempt is made to learn the initial displacement of the referenceFMU. It can be clearly seen that the NeuralFMU learns this shift well in just a few training steps. ","category":"page"},{"location":"examples/simple_hybrid_CS/#Target-group","page":"Simple CS-NeuralFMU","title":"Target group","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The example is primarily intended for users who work in the field of first principle and/or hybrid modeling and are further interested in hybrid model building. The example wants to show how simple it is to combine FMUs with machine learning and to illustrate the advantages of this approach.","category":"page"},{"location":"examples/simple_hybrid_CS/#Other-formats","page":"Simple CS-NeuralFMU","title":"Other formats","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Besides this Jupyter Notebook there is also a Julia file with the same name, which contains only the code cells and for the documentation there is a Markdown file corresponding to the notebook.  ","category":"page"},{"location":"examples/simple_hybrid_CS/#Getting-started","page":"Simple CS-NeuralFMU","title":"Getting started","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/#Installation-prerequisites","page":"Simple CS-NeuralFMU","title":"Installation prerequisites","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":" Description Command Alternative\n1. Enter Package Manager via ] \n2. Install FMI via add FMI add \" https://github.com/ThummeTo/FMI.jl \"\n3. Install FMIFlux via add FMIFlux add \" https://github.com/ThummeTo/FMIFlux.jl \"\n4. Install Flux via add Flux \n5. Install DifferentialEquations via add DifferentialEquations \n6. Install Plots via add Plots ","category":"page"},{"location":"examples/simple_hybrid_CS/#Code-section","page":"Simple CS-NeuralFMU","title":"Code section","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"To run the example, the previously installed packages must be included. ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# imports\nusing FMI\nusing FMIFlux\nusing Flux\nusing DifferentialEquations: Tsit5\nimport Plots","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"After importing the packages, the path to the Functional Mock-up Units (FMUs) is set. The FMU is a model exported meeting the Functional Mock-up Interface (FMI) Standard. The FMI is a free standard (fmi-standard.org) that defines a container and an interface to exchange dynamic models using a combination of XML files, binaries and C code zipped into a single file. ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The objec-orientated structure of the SpringPendulumExtForce1D can be seen in the following graphic. This model is a simple spring pendulum without friction, but with an external force. ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Here the path for the SpringPendulumExtForce1D is set: ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"pathFMU = joinpath(dirname(@__FILE__), \"../model/SpringPendulumExtForce1D.fmu\")\nprintln(\"FMU path: \", pathFMU)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"FMU path: ../model/SpringPendulumExtForce1D.fmu","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Next, the start time and end time of the simulation are set. Finally, a step size is specified to store the results of the simulation at these time steps.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"tStart = 0.0\ntStep = 0.01\ntStop = 5.0\ntSave = tStart:tStep:tStop","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"0.0:0.01:5.0","category":"page"},{"location":"examples/simple_hybrid_CS/#ReferenceFMU","page":"Simple CS-NeuralFMU","title":"ReferenceFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In the next lines of code the FMU of the referenceFMU model is loaded and instantiated.  ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"referenceFMU = fmiLoad(pathFMU)\nfmiInstantiate!(referenceFMU; loggingOn=false)\nfmiInfo(referenceFMU)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"┌ Info: fmi2Unzip(...): Successfully unzipped 147 files at `C:\\Users\\JOHANN~1\\AppData\\Local\\Temp\\fmijl_ZVpiA1\\SpringPendulumExtForce1D`.\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:273\n┌ Info: fmi2Load(...): FMU supports both CS and ME, using CS as default if nothing specified.\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:376\n┌ Info: fmi2Load(...): FMU resources location is `file:///C:/Users/JOHANN~1/AppData/Local/Temp/fmijl_ZVpiA1/SpringPendulumExtForce1D/resources`\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:384\n\n\n#################### Begin information for FMU ####################\n\tModel name:\t\t\tSpringPendulumExtForce1D\n\tFMI-Version:\t\t\t2.0\n\tGUID:\t\t\t\t{b376bbba-5027-4429-a701-20b703fda94e}\n\tGeneration tool:\t\tDymola Version 2020x (64-bit), 2019-10-10\n\tGeneration time:\t\t2021-06-18T11:01:53Z\n\tVar. naming conv.:\t\tstructured\n\tEvent indicators:\t\t0\n\tInputs:\t\t\t\t1\n\t\t352321536 [\"extForce\"]\n\tOutputs:\t\t\t2\n\t\t335544320 [\"der(accSensor.v)\", \"a\", \"accSensor.a\"]\n\t\t335544321 [\"accSensor.v\", \"der(accSensor.flange.s)\", \"v\", \"der(speedSensor.flange.s)\", \"speedSensor.v\"]\n\tStates:\t\t\t\t2\n\t\t33554432 [\"mass.s\"]\n\t\t33554433 [\"mass.v\"]\n\tSupports Co-Simulation:\t\ttrue\n\t\tModel identifier:\tSpringPendulumExtForce1D\n\t\tGet/Set State:\t\ttrue\n\t\tSerialize State:\ttrue\n\t\tDir. Derivatives:\ttrue\n\t\tVar. com. steps:\ttrue\n\t\tInput interpol.:\ttrue\n\t\tMax order out. der.:\t1\n\tSupports Model-Exchange:\ttrue\n\t\tModel identifier:\tSpringPendulumExtForce1D\n\t\tGet/Set State:\t\ttrue\n\t\tSerialize State:\ttrue\n\t\tDir. Derivatives:\ttrue\n##################### End information for FMU #####################","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Both the start and end time are set via the fmiSetupExperiment() function. In addition, the initial position of the mass is set to a value of 13m  The experiment is initialized to get the information of the continuous states. You can get all continuous states of a FMU by the function fmiGetContinuousStates() and this is also done for the referenceFMU. It has two states: The first state is the previously initialized position of the mass, the second state is the velocity, which is initialized with 0fracms.   ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"fmiSetupExperiment(referenceFMU, tStart, tStop)\nfmiSetReal(referenceFMU, \"mass_s0\", 1.3)   # increase amplitude, invert phase\nfmiEnterInitializationMode(referenceFMU)\nfmiExitInitializationMode(referenceFMU)\n\nx₀ = fmiGetContinuousStates(referenceFMU)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"2-element Vector{Float64}:\n 1.3\n 0.0","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In the following code block the referenceFMU is simulated, still specifying which variables are included. After the simulation is finished the result of the referenceFMU can be plotted. This plot also serves as a reference for the later CS-NeuralFMU model.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"vrs = [\"mass.s\", \"mass.v\", \"mass.a\"]\n_, referenceSimData = fmiSimulate(referenceFMU, tStart, tStop; recordValues=vrs, setup=false, reset=false, saveat=tSave)\nfmiPlot(referenceFMU, vrs, referenceSimData)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The data from the simualtion of the referenceFMU, are divided into position, velocity and acceleration data. The data for the acceleration will be needed later. ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"posReference = collect(data[1] for data in referenceSimData.saveval)\nvelReference = collect(data[2] for data in referenceSimData.saveval)\naccReference = collect(data[3] for data in referenceSimData.saveval)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"501-element Vector{Float64}:\n -1.9999999999999996\n -1.9989808107156004\n -1.995976332371232\n -1.9909821938997307\n -1.9839989801021418\n -1.9750314004124547\n -1.9640884504035183\n -1.951180580066516\n -1.9363227134824257\n -1.9195319560818125\n -1.9008203166719828\n -1.8802131771552166\n -1.8577245801802755\n  ⋮\n  1.9439538472626405\n  1.9581269688364755\n  1.970346615172437\n  1.9805952930006132\n  1.9888623187994514\n  1.9951388459819808\n  1.9994178648958127\n  2.0016968375647415\n  2.0019759530917005\n  2.0002523498984894\n  1.9965275218318568\n  1.9908049090723823","category":"page"},{"location":"examples/simple_hybrid_CS/#DefaultFMU","page":"Simple CS-NeuralFMU","title":"DefaultFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The following is a reset for the referenceFMU and a renaming to defaultFMU. After the reset, the previous initial position of the mass is not set, so the default position of the defaultFMU is used. The first state indicates the position of the mass, which is initilized with 05𝑚.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"fmiReset(referenceFMU)\ndefaultFMU = referenceFMU\n\nfmiSetupExperiment(defaultFMU, tStart, tStop)\nfmiEnterInitializationMode(defaultFMU)\nfmiExitInitializationMode(defaultFMU)\n\nx₀ = fmiGetContinuousStates(defaultFMU)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"2-element Vector{Float64}:\n 0.5\n 0.0","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The following simulate and plot the defaultFMU just like the referenceFMU. The differences between both systems can be clearly seen from the plots. In the plots for the defaultFMU you can see that other oscillations occur due to the different starting positions. On the one hand the oscillation of the defaultFMU starts in the opposite direction of the referenceFMU and on the other hand the graphs for the velocity and acceleration differ clearly in the amplitude. In the following we try to learn the initial shift of the position so that the graphs for the acceleration of both graphs match.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"_, defaultSimData = fmiSimulate(defaultFMU, tStart, tStop; recordValues=vrs, setup=false, reset=false, saveat=tSave)\nfmiPlot(defaultFMU, vrs, defaultSimData)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The data from the simualtion of the defaultFMU, are divided into position, velocity and acceleration data. The data for the acceleration will be needed later.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"posDefault = collect(data[1] for data in defaultSimData.saveval)\nvelDefault = collect(data[2] for data in defaultSimData.saveval)\naccDefault = collect(data[3] for data in defaultSimData.saveval)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"501-element Vector{Float64}:\n  6.0\n  5.996982513180007\n  5.987986261034271\n  5.9730046030442665\n  5.95205107717745\n  5.925151171646224\n  5.892330901036602\n  5.853619029884401\n  5.809060133574773\n  5.758686483925127\n  5.702562314755022\n  5.640726974472335\n  5.5732600661315335\n  ⋮\n -5.817454106640481\n -5.860262621029506\n -5.897211631373532\n -5.928264987575014\n -5.953392775960465\n -5.972564609104964\n -5.985762763815119\n -5.99297561097946\n -5.994196580640214\n -5.989425410415006\n -5.9786675103892755\n -5.961926527257058","category":"page"},{"location":"examples/simple_hybrid_CS/#CS-NeuralFMU","page":"Simple CS-NeuralFMU","title":"CS-NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In this section, the defaultFMU is inserted into a CS-NeuralFMU architecture. It has the goal to learn the initial state of the referenceFMU.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"For the external force, a simple function is implemented that always returns a force of 0N at each time point. Also, all other functions and implementations would be possible here. Only for simplification reasons the function was chosen so simply.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"function extForce(t)\n    return [0.0]\nend ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"extForce (generic function with 1 method)","category":"page"},{"location":"examples/simple_hybrid_CS/#Loss-function","page":"Simple CS-NeuralFMU","title":"Loss function","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In order to train our model, a loss function must be implemented. The solver of the NeuralFMU can calculate the gradient of the loss function. The gradient descent is needed to adjust the weights in the neural network so that the sum of the error is reduced and the model becomes more accurate.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The loss function in this implmentation consists of the mean squared error (mse) from the acceleration data of the referenceFMU simulation (accReference) and the acceleration data of the network (accNet). $ mse = \\frac{1}{n} \\sum\\limits_{i=0}^n (accReference[i] - accNet[i])^2 $","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# loss function for training\nfunction lossSum()\n    solution = csNeuralFMU(extForce, tStep)\n\n    accNet = collect(data[1] for data in solution)\n    \n    Flux.Losses.mse(accReference, accNet)\nend","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"lossSum (generic function with 1 method)","category":"page"},{"location":"examples/simple_hybrid_CS/#Callback","page":"Simple CS-NeuralFMU","title":"Callback","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"To output the loss in certain time intervals, a callback is implemented as a function in the following. Here a counter is incremented, every twentieth pass the loss function is called and the average error is printed out.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# callback function for training\nglobal counter = 0\nfunction callb()\n    global counter += 1\n\n    if counter % 20 == 1\n        avgLoss = lossSum()\n        @info \"Loss [$counter]: $(round(avgLoss, digits=5))\"\n    end\nend","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"callb (generic function with 1 method)","category":"page"},{"location":"examples/simple_hybrid_CS/#Structure-of-the-CS-NeuralFMU","page":"Simple CS-NeuralFMU","title":"Structure of the CS-NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"In the following, the topology of the CS-NeuralFMU is constructed. It consists of an input layer, which then leads into the defaultFMU model. The CS-FMU computes the outputs for the given system state and time step. After the defaultFMU follows a dense layer, which has exactly as many inputs as the model has outputs. The output of this layer consists of 16 output nodes and a tanh activation function. The next layer has 16 input and output nodes with the same activation function. The last layer is again a dense layer with 16 input nodes and the number of model outputs as output nodes. Here, it is important that no tanh-activation function follows, because otherwise the pendulums state values would be limited to the interval -11.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# NeuralFMU setup\nnumInputs = length(defaultFMU.modelDescription.inputValueReferences)\nnumOutputs = length(defaultFMU.modelDescription.outputValueReferences)\n\nnet = Chain(inputs -> fmiInputDoStepCSOutput(defaultFMU, tStep, inputs),\n            Dense(numOutputs, 16, tanh),\n            Dense(16, 16, tanh),\n            Dense(16, numOutputs))","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Chain(\n  var\"#15#16\"(),\n  Dense(2, 16, tanh),                   \u001b[90m# 48 parameters\u001b[39m\n  Dense(16, 16, tanh),                  \u001b[90m# 272 parameters\u001b[39m\n  Dense(16, 2),                         \u001b[90m# 34 parameters\u001b[39m\n)\u001b[90m                   # Total: 6 arrays, \u001b[39m354 parameters, 1.758 KiB.","category":"page"},{"location":"examples/simple_hybrid_CS/#Definition-of-the-CS-NeuralFMU","page":"Simple CS-NeuralFMU","title":"Definition of the CS-NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"The instantiation of the CS-NeuralFMU is done as a one-liner. The FMU defaultFMU, the structure of the network net, start tStart and end time tStop, and the time steps tSave for saving are specified.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"csNeuralFMU = CS_NeuralFMU(defaultFMU, net, (tStart, tStop); saveat=tSave);","category":"page"},{"location":"examples/simple_hybrid_CS/#Plot-before-training","page":"Simple CS-NeuralFMU","title":"Plot before training","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Here the state trajactory of the extForceFMU is recorded. Doesn't really look like a pendulum yet, but the system is random initialized by default. In the later plots, the effect of learning can be seen.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"solutionBefore = csNeuralFMU(extForce, tStep)\nPlots.plot(tSave, collect(data[1] for data in solutionBefore), label=\"acc CS-NeuralFMU\", linewidth=2)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_CS/#Training-of-the-CS-NeuralFMU","page":"Simple CS-NeuralFMU","title":"Training of the CS-NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"For the training of the CS-NeuralFMU the parameters are extracted. The known ADAM optimizer for minimizing the gradient descent is used as further passing parameters. In addition, the previously defined loss and callback function, as well as the number of epochs are passed.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# train\nparamsNet = Flux.params(csNeuralFMU)\n\noptim = ADAM()\nFlux.train!(lossSum, paramsNet, Iterators.repeated((), 300), optim; cb=callb)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"┌ Info: Loss [1]: 1.36812\n└ @ Main In[13]:8\n┌ Info: Loss [21]: 0.23761\n└ @ Main In[13]:8\n┌ Info: Loss [41]: 0.04257\n└ @ Main In[13]:8\n┌ Info: Loss [61]: 0.0265\n└ @ Main In[13]:8\n┌ Info: Loss [81]: 0.01623\n└ @ Main In[13]:8\n┌ Info: Loss [101]: 0.01005\n└ @ Main In[13]:8\n┌ Info: Loss [121]: 0.00617\n└ @ Main In[13]:8\n┌ Info: Loss [141]: 0.0038\n└ @ Main In[13]:8\n┌ Info: Loss [161]: 0.0024\n└ @ Main In[13]:8\n┌ Info: Loss [181]: 0.0016\n└ @ Main In[13]:8\n┌ Info: Loss [201]: 0.00113\n└ @ Main In[13]:8\n┌ Info: Loss [221]: 0.00086\n└ @ Main In[13]:8\n┌ Info: Loss [241]: 0.00069\n└ @ Main In[13]:8\n┌ Info: Loss [261]: 0.00058\n└ @ Main In[13]:8\n┌ Info: Loss [281]: 0.0005\n└ @ Main In[13]:8","category":"page"},{"location":"examples/simple_hybrid_CS/#Comparison-of-the-plots","page":"Simple CS-NeuralFMU","title":"Comparison of the plots","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Here three plots are compared with each other and only the acceleration of the mass is considered. The first plot represents the defaultFMU, the second represents the referenceFMU and the third plot represents the result after training the CS-NeuralFMU. ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"# plot results mass.a\nsolutionAfter = csNeuralFMU(extForce, tStep)\n\nfig = Plots.plot(xlabel=\"t [s]\", ylabel=\"mass acceleration [m/s^2]\", linewidth=2,\n                 xtickfontsize=12, ytickfontsize=12,\n                 xguidefontsize=12, yguidefontsize=12,\n                 legendfontsize=8, legend=:topright)\n\naccNeuralFMU = collect(data[1] for data in solutionAfter)\n\nPlots.plot!(fig, tSave, accDefault, label=\"defaultFMU\", linewidth=2)\nPlots.plot!(fig, tSave, accReference, label=\"referenceFMU\", linewidth=2)\nPlots.plot!(fig, tSave, accNeuralFMU, label=\"CS-NeuralFMU (300 eps.)\", linewidth=2)\nfig ","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Finally, the FMU is cleaned-up.","category":"page"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"fmiUnload(defaultFMU)","category":"page"},{"location":"examples/simple_hybrid_CS/#Summary","page":"Simple CS-NeuralFMU","title":"Summary","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"Based on the plots, it can be clearly seen that the CS-NeuralFMU model is able to learn the shift of the initial position. Even after only 300 runs, the curves overlap very much, so no further training with more runs is needed.","category":"page"},{"location":"examples/simple_hybrid_CS/#Source","page":"Simple CS-NeuralFMU","title":"Source","text":"","category":"section"},{"location":"examples/simple_hybrid_CS/","page":"Simple CS-NeuralFMU","title":"Simple CS-NeuralFMU","text":"[1] Tobias Thummerer, Lars Mikelsons and Josef Kircher. 2021. NeuralFMU: towards structural integration of FMUs into neural networks. Martin Sjölund, Lena Buffoni, Adrian Pop and Lennart Ochel (Ed.). Proceedings of 14th Modelica Conference 2021, Linköping, Sweden, September 20-24, 2021. Linköping University Electronic Press, Linköping (Linköping Electronic Conference Proceedings ; 181), 297-306. DOI: 10.3384/ecp21181297","category":"page"},{"location":"contents/","page":"Contents","title":"Contents","text":"Depth = 2","category":"page"},{"location":"related/#Related-Publications","page":"Related Publication","title":"Related Publications","text":"","category":"section"},{"location":"related/","page":"Related Publication","title":"Related Publication","text":"Thummerer T, Kircher J and Mikelsons L: Neural FMU: Towards structual integration of FMUs into neural networks (Preprint, accepted 14th International Modelica Conference) pdf|DOI","category":"page"},{"location":"related/","page":"Related Publication","title":"Related Publication","text":"Thummerer T, Tintenherr J, Mikelsons L: Hybrid modeling of the human cardiovascular system using NeuralFMUs (Preprint, accepted 10th International Conference on Mathematical Modeling in Physical Sciences) pdf|DOI","category":"page"},{"location":"examples/overview/#Examples-Overview","page":"Examples Overview","title":"Examples - Overview","text":"","category":"section"},{"location":"examples/overview/","page":"Examples Overview","title":"Examples Overview","text":"This section discusses the included examples of the FMIFlux.jl library. So you can execute them on your machine and get detailed information about all of the steps. If you require further information about the function calls, see library functions section. For more information related to the setup and simulation of an FMU see FMI.jl library.","category":"page"},{"location":"examples/overview/","page":"Examples Overview","title":"Examples Overview","text":"The examples are:","category":"page"},{"location":"examples/overview/","page":"Examples Overview","title":"Examples Overview","text":"Simple hybrid CS: Showing how to train a Neural CS FMU.\nSimple hybrid ME: Showing how to train a Neural ME FMU.\nModelica Conference 2021: Showing how to train a Neural ME FMU.","category":"page"},{"location":"examples/modelica_conference_2021/#ME-NeuralFMU-from-the-Modelica-Conference","page":"Modelica Conference 2021","title":"ME-NeuralFMU from the Modelica Conference","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Tutorial by Johannes Stoljar, Tobias Thummerer","category":"page"},{"location":"examples/modelica_conference_2021/#License","page":"Modelica Conference 2021","title":"License","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Copyright (c) 2021 Tobias Thummerer, Lars Mikelsons, Johannes Stoljar","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Licensed under the MIT license. See LICENSE file in the project root for details.","category":"page"},{"location":"examples/modelica_conference_2021/#Motivation","page":"Modelica Conference 2021","title":"Motivation","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"This Julia Package is motivated by the application of hybrid modeling. This package enables the user to integrate his simulation model between neural networks (NeuralFMU). For this, the simulation model must be exported as FMU (functional mock-up unit), which corresponds to a widely used standard. The big advantage of hybrid modeling with artificial neural networks is, that the effects that are difficult to model (because they might be unknown) can be easily learned by the neural networks. For this purpose, the NeuralFMU is trained with measurement data containing the unmodeled physical effect. The final product is a simulation model including the orignially unmodeled effects. Another big advantage of the NeuralFMU is that it works with little data, because the FMU already contains the characterisitic functionality of the simulation and only the missing effects are added.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"NeuralFMUs do not need to be as easy as in this example. Basically a NeuralFMU can combine different ANN topologies that manipulate any FMU-input (system state, system inputs, time) and any FMU-output (system state derivative, system outputs, other system variables). However, for this example a NeuralFMU topology as shown in the following picture is used.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: NeuralFMU.svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"NeuralFMU (ME) from [1].","category":"page"},{"location":"examples/modelica_conference_2021/#Introduction-to-the-example","page":"Modelica Conference 2021","title":"Introduction to the example","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In this example, simplified modeling of a one-dimensional spring pendulum (without friction) is compared to a model of the same system that includes a nonlinear friction model. The FMU with the simplified model will be named simpleFMU in the following and the model with the friction will be named realFMU. At the beginning, the actual state of both simulations is shown, whereby clear deviations can be seen in the graphs. In addition, the initial states are changed for both models and these graphs are also contrasted, and the differences can again be clearly seen. The realFMU serves as a reference graph. The simpleFMU is then integrated into a NeuralFMU architecture and a training of the entire network is performed. After the training the final state is compared again to the realFMU. It can be clearly seen that by using the NeuralFMU, learning of the friction process has taken place.  ","category":"page"},{"location":"examples/modelica_conference_2021/#Target-group","page":"Modelica Conference 2021","title":"Target group","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The example is primarily intended for users who work in the field of first principle and/or hybrid modeling and are further interested in hybrid model building. The example wants to show how simple it is to combine FMUs with machine learning and to illustrate the advantages of this approach.","category":"page"},{"location":"examples/modelica_conference_2021/#Other-formats","page":"Modelica Conference 2021","title":"Other formats","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Besides this Jupyter Notebook there is also a Julia file with the same name, which contains only the code cells. For the documentation there is a Markdown file corresponding to the notebook.  ","category":"page"},{"location":"examples/modelica_conference_2021/#Getting-started","page":"Modelica Conference 2021","title":"Getting started","text":"","category":"section"},{"location":"examples/modelica_conference_2021/#Installation-prerequisites","page":"Modelica Conference 2021","title":"Installation prerequisites","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":" Description Command Alternative\n1. Enter Package Manager via ] \n2. Install FMI via add FMI add \" https://github.com/ThummeTo/FMI.jl \"\n3. Install FMIFlux via add FMIFlux add \" https://github.com/ThummeTo/FMIFlux.jl \"\n4. Install Flux via add Flux \n5. Install DifferentialEquations via add DifferentialEquations \n6. Install Plots via add Plots ","category":"page"},{"location":"examples/modelica_conference_2021/#Code-section","page":"Modelica Conference 2021","title":"Code section","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"To run the example, the previously installed packages must be included. ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# imports\nusing FMI\nusing FMIFlux\nusing Flux\nusing DifferentialEquations: Tsit5\nimport Plots","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"After importing the packages, the path to the Functional Mock-up Units (FMUs) is set. The exported FMU is a model meeting the Functional Mock-up Interface (FMI) Standard. The FMI is a free standard (fmi-standard.org) that defines a container and an interface to exchange dynamic models using a combination of XML files, binaries and C code zipped into a single file. ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The object-orientated structure of the SpringPendulum1D (simpleFMU) can be seen in the following graphic and corresponds to a simple modeling.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In contrast, the model SpringFrictionPendulum1D (realFMU) is somewhat more accurate, because it includes a friction component. ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Here the path for the SpringPendulum1D and the SpringFrictionPendulum1D model is set: ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"simpleFMUPath = joinpath(dirname(@__FILE__), \"../model/SpringPendulum1D.fmu\")\nrealFMUPath = joinpath(dirname(@__FILE__), \"../model/SpringFrictionPendulum1D.fmu\")\nprintln(\"SimpleFMU path: \", simpleFMUPath)\nprintln(\"RealFMU path: \", realFMUPath)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"SimpleFMU path: ../model/SpringPendulum1D.fmu\nRealFMU path: ../model/SpringFrictionPendulum1D.fmu","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Next, the start time and end time of the simulation are set. Finally, a step size is specified to store the results of the simulation at these time steps.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"tStart = 0.0\ntStep = 0.01\ntStop = 4.0\ntSave = collect(tStart:tStep:tStop)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"401-element Vector{Float64}:\n 0.0\n 0.01\n 0.02\n 0.03\n 0.04\n 0.05\n 0.06\n 0.07\n 0.08\n 0.09\n 0.1\n 0.11\n 0.12\n ⋮\n 3.89\n 3.9\n 3.91\n 3.92\n 3.93\n 3.94\n 3.95\n 3.96\n 3.97\n 3.98\n 3.99\n 4.0","category":"page"},{"location":"examples/modelica_conference_2021/#RealFMU","page":"Modelica Conference 2021","title":"RealFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the next lines of code the FMU of the realFMU model is loaded and instantiated.  ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"realFMU = fmiLoad(realFMUPath)\nfmiInstantiate!(realFMU; loggingOn=false)\nfmiInfo(realFMU)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"┌ Info: fmi2Unzip(...): Successfully unzipped 28 files at `C:\\Users\\JOHANN~1\\AppData\\Local\\Temp\\fmijl_DBHK6a\\SpringFrictionPendulum1D`.\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:273\n┌ Info: fmi2Load(...): FMU supports both CS and ME, using CS as default if nothing specified.\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:376\n┌ Info: fmi2Load(...): FMU resources location is `file:///C:/Users/JOHANN~1/AppData/Local/Temp/fmijl_DBHK6a/SpringFrictionPendulum1D/resources`\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:384\n\n\n#################### Begin information for FMU ####################\n\tModel name:\t\t\tSpringFrictionPendulum1D\n\tFMI-Version:\t\t\t2.0\n\tGUID:\t\t\t\t{b02421b8-652a-4d48-9ffc-c2b223aa1b94}\n\tGeneration tool:\t\tDymola Version 2020x (64-bit), 2019-10-10\n\tGeneration time:\t\t2021-11-23T13:36:30Z\n\tVar. naming conv.:\t\tstructured\n\tEvent indicators:\t\t24\n\tInputs:\t\t\t\t0\n\tOutputs:\t\t\t0\n\tStates:\t\t\t\t2\n\t\t33554432 [\"mass.s\"]\n\t\t33554433 [\"mass.v\", \"mass.v_relfric\"]\n\tSupports Co-Simulation:\t\ttrue\n\t\tModel identifier:\tSpringFrictionPendulum1D\n\t\tGet/Set State:\t\ttrue\n\t\tSerialize State:\ttrue\n\t\tDir. Derivatives:\ttrue\n\t\tVar. com. steps:\ttrue\n\t\tInput interpol.:\ttrue\n\t\tMax order out. der.:\t1\n\tSupports Model-Exchange:\ttrue\n\t\tModel identifier:\tSpringFrictionPendulum1D\n\t\tGet/Set State:\t\ttrue\n\t\tSerialize State:\ttrue\n\t\tDir. Derivatives:\ttrue\n##################### End information for FMU #####################","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the following two subsections, the realFMU is simulated twice with different initial states to show what effect the choice of initial states has.","category":"page"},{"location":"examples/modelica_conference_2021/#Default-initial-states","page":"Modelica Conference 2021","title":"Default initial states","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The FMU is reset to the defined state by the function fmiReset(). Then the start and end time are set via the fmiSetupExperiment() function. In the next steps the initial states are set. The first state is the position of the mass, which is initilized with 05m, the second state is the velocity, which is initialized with 0fracms.   ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"fmiReset(realFMU)\nfmiSetupExperiment(realFMU, tStart, tStop)\nstates = [\"s0\", \"v0\"]\nx₀ = [0.5, 0.0]\n\nfmiSetReal(realFMU, states, x₀)\nfmiEnterInitializationMode(realFMU)\nfmiExitInitializationMode(realFMU);","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the following code block the realFMU is simulated, still specifying which variables are included. After the simulation is finished the result of the realFMU can be plotted. This plot also serves as a reference for the other model (simpleFMU). The extracted data will still be needed later on.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"vrs = [\"mass.s\", \"mass.v\", \"mass.a\", \"mass.f\"]\nsuccess, realSimData = fmiSimulate(realFMU, tStart, tStop; recordValues=vrs, saveat=tSave, setup=false, reset=false)\nposReal = collect(data[1] for data in realSimData.saveval)\nvelReal = collect(data[2] for data in realSimData.saveval)\nfmiPlot(realFMU, vrs, realSimData)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/#Define-functions","page":"Modelica Conference 2021","title":"Define functions","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The structure of the previous code section is used more often in the further sections, so for clarity the previously explained code sections for resetting, initializing and simulating are combined into one function simulate().","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function simulate(FMU, states, x₀, variables, tStart, tStop, tSave)\n    fmiReset(FMU)\n    fmiSetupExperiment(FMU, tStart, tStop)\n\n    fmiSetReal(FMU, states, x₀)\n    fmiEnterInitializationMode(FMU)\n    fmiExitInitializationMode(FMU)\n\n\n    success, simData = fmiSimulate(FMU, tStart, tStop; recordValues=variables, saveat=tSave, setup=false, reset=false)\n    return simData\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"simulate (generic function with 1 method)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Also, a function to extract the position and velocity from the simulation data is created.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function extractPosVel(simData)\n    posData = collect(data[1] for data in simData)\n    velData = collect(data[2] for data in simData)\n    return posData, velData\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"extractPosVel (generic function with 1 method)","category":"page"},{"location":"examples/modelica_conference_2021/#Modified-initial-states","page":"Modelica Conference 2021","title":"Modified initial states","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In contrast to the previous section, other initial states are selected. The position of the mass is initilized with 10m and the velocity is initialized with -15fracms. With the modified initial states the realFMU is simulated and a graph is generated.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"xMod₀ = [1.0, -1.5]\nrealSimDataMod = simulate(realFMU, states, xMod₀, vrs, tStart, tStop, tSave)\nfmiPlot(realFMU, vrs, realSimDataMod)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"After the plots are created, the FMU is unloaded.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"fmiUnload(realFMU)","category":"page"},{"location":"examples/modelica_conference_2021/#SimpleFMU","page":"Modelica Conference 2021","title":"SimpleFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The following lines load and instantiate the simpleFMU. ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"simpleFMU = fmiLoad(simpleFMUPath)\nfmiInstantiate!(simpleFMU; loggingOn=false)\nfmiInfo(simpleFMU)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"┌ Info: fmi2Unzip(...): Successfully unzipped 28 files at `C:\\Users\\JOHANN~1\\AppData\\Local\\Temp\\fmijl_VQfCmt\\SpringPendulum1D`.\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:273\n\n\n#################### Begin information for FMU ####################\n\tModel name:\t\t\tSpringPendulum1D\n\tFMI-Version:\t\t\t2.0\n\tGUID:\t\t\t\t{5030e5a4-87c0-42cf-8779-74ebea1906aa}\n\tGeneration tool:\t\tDymola Version 2020x (64-bit), 2019-10-10\n\tGeneration time:\t\t2021-07-21T05:28:53Z\n\tVar. naming conv.:\t\tstructured\n\tEvent indicators:\t\t0\n\tInputs:\t\t\t\t0\n\tOutputs:\t\t\t0\n\tStates:\t\t\t\t2\n\t\t33554432 [\"mass.s\"]\n\t\t33554433 [\"mass.v\"]\n\tSupports Co-Simulation:\t\ttrue\n\t\tModel identifier:\tSpringPendulum1D\n\t\tGet/Set State:\t\ttrue\n\t\tSerialize State:\ttrue\n\t\tDir. Derivatives:\ttrue\n\t\tVar. com. steps:\ttrue\n\t\tInput interpol.:\ttrue\n\t\tMax order out. der.:\t1\n\tSupports Model-Exchange:\ttrue\n\t\tModel identifier:\tSpringPendulum1D\n\t\tGet/Set State:\t\ttrue\n\t\tSerialize State:\ttrue\n\t\tDir. Derivatives:\ttrue\n##################### End information for FMU #####################\n\n\n┌ Info: fmi2Load(...): FMU supports both CS and ME, using CS as default if nothing specified.\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:376\n┌ Info: fmi2Load(...): FMU resources location is `file:///C:/Users/JOHANN~1/AppData/Local/Temp/fmijl_VQfCmt/SpringPendulum1D/resources`\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:384","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The differences between both systems can be clearly seen from the plots in the subchapters. In the plot for the realFMU it can be seen that the oscillation continues to decrease due to the effect of the friction. If you would simulate long enough, the oscillation would come to a standstill in a certain time. The oscillation in the simpleFMU behaves differently, since the friction was not taken into account here. The oscillation in this model would continue to infinity with the same oscillation amplitude. From this observation the desire of an improvement of this model arises.     ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the following two subsections, the simpleFMU is simulated twice with different initial states to show what effect the choice of initial states has.","category":"page"},{"location":"examples/modelica_conference_2021/#Default-initial-states-2","page":"Modelica Conference 2021","title":"Default initial states","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Similar to the simulation of the realFMU, the simpleFMU is also simulated with the default values for the position and velocity of the mass and then plotted. There is one difference, however, as another state representing a fixed displacement is set. In addition, the last variable is also removed from the varibals to be plotted.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"states = [\"mass_s0\", \"mass_v0\", \"fixed.s0\"]\ndisplacement = 0.1\nxSimple₀ = vcat(x₀, displacement)\nvrs = vrs[1:end-1]\n\nsimpleSimData = simulate(simpleFMU, states, xSimple₀, vrs, tStart, tStop, tSave)\nfmiPlot(simpleFMU, vrs, simpleSimData)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/#Modified-initial-states-2","page":"Modelica Conference 2021","title":"Modified initial states","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The same values for the initial states are used for this simulation as for the simulation from the realFMU with the modified initial states.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"xSimpleMod₀ = vcat(xMod₀, displacement)\n\nsimpleSimDataMod = simulate(simpleFMU, states, xSimpleMod₀, vrs, tStart, tStop, tSave)\nfmiPlot(simpleFMU, vrs, simpleSimDataMod)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/#NeuralFMU","page":"Modelica Conference 2021","title":"NeuralFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/#Loss-function","page":"Modelica Conference 2021","title":"Loss function","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In order to train our model, a loss function must be implemented. The solver of the NeuralFMU can calculate the gradient of the loss function. The gradient descent is needed to adjust the weights in the neural network so that the sum of the error is reduced and the model becomes more accurate.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The error function in this implementation consists of the mean of the mean squared erros. The first part of the addition is the deviation of the position and the second part is the deviation of the velocity. The mean squared error (mse) for the position consists from the real position of the realFMU simulation (posReal) and the position data of the network (posNet). The mean squared error for the velocity consists of the real velocity of the realFMU simualtion (velReal) and the velocity data of the network (velNet). $ loss = \\frac{1}{2} \\Bigl[ \\frac{1}{n} \\sum\\limits{i=0}^n (posReal[i] - posNet[i])^2 + \\frac{1}{n} \\sum\\limits{i=0}^n (velReal[i] - velNet[i])^2 \\Bigr]$","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# loss function for training\nfunction lossSum()\n    global x₀\n    solution = neuralFMU(x₀)\n\n    posNet, velNet = extractPosVel(solution.u)\n\n    (Flux.Losses.mse(posReal, posNet) + Flux.Losses.mse(velReal, velNet)) / 2.0\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"lossSum (generic function with 1 method)","category":"page"},{"location":"examples/modelica_conference_2021/#Callback","page":"Modelica Conference 2021","title":"Callback","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"To output the loss in certain time intervals, a callback is implemented as a function in the following. Here a counter is incremented, every fiftieth pass the loss function is called and the average error is printed out. Also the parmaters for the velocity in the first layer are kept to a fixed value.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# callback function for training\nglobal counter = 0\nfunction callb()\n    global counter, paramsNet\n    counter += 1\n\n    # freeze first layer parameters (2,4,6) for velocity -> (static) direct feed trough for velocity\n    # parameters for position (1,3,5) are learned\n    paramsNet[1][2] = 0.0\n    paramsNet[1][4] = 1.0\n    paramsNet[1][6] = 0.0\n\n    if counter % 50 == 1\n        avgLoss = lossSum()\n        @info \"  Loss [$counter]: $(round(avgLoss, digits=5))\n        Avg displacement in data: $(round(sqrt(avgLoss), digits=5))\n        Weight/Scale: $(paramsNet[1][1])   Bias/Offset: $(paramsNet[1][5])\"\n    end\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"callb (generic function with 1 method)","category":"page"},{"location":"examples/modelica_conference_2021/#Functions-for-plotting","page":"Modelica Conference 2021","title":"Functions for plotting","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In this section some important functions for plotting are defined. The function generate_figure() creates a new figure object and sets some attributes.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function generate_figure(title, xLabel, yLabel, xlim=\"auto\")\n    Plots.plot(\n        title=title, xlabel=xLabel, ylabel=yLabel, linewidth=2,\n        xtickfontsize=12, ytickfontsize=12, xguidefontsize=12, yguidefontsize=12,\n        legendfontsize=12, legend=:topright, xlim=xlim)\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"generate_figure (generic function with 2 methods)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the following function, the data of the realFMU, simpleFMU and neuralFMU are summarized and displayed in a graph.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function plot_results(title, xLabel, yLabel, interval, realData, simpleData, neuralData)\n    linestyles = [:dot, :solid]\n    \n    fig = generate_figure(title, xLabel, yLabel)\n    Plots.plot!(fig, interval, simpleData, label=\"SimpleFMU\", linewidth=2)\n    Plots.plot!(fig, interval, realData, label=\"reference\", linewidth=2)\n    for i in 1:length(neuralData)\n        Plots.plot!(fig, neuralData[i][1], neuralData[i][2], label=\"NeuralFMU ($(i*2500))\", \n                    linewidth=2, linestyle=linestyles[i], linecolor=:green)\n    end\n    Plots.display(fig)\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"plot_results (generic function with 1 method)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"This is the superordinate function, which at the beginning extracts the position and velocity from the simulation data (realSimData, realSimDataMod, simpleSimData,..., solutionAfterMod). Four graphs are then generated, each comparing the corresponding data from the realFMU, simpleFMU, and neuralFMU. The comparison is made with the simulation data from the simulation with the default and modified initial states. According to the data, the designation of the title and the naming of the axes is adapted.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function plot_all_results(realSimData, realSimDataMod, simpleSimData, \n        simpleSimDataMod, solutionAfter, solutionAfterMod)    \n    # collect all data\n    posReal, velReal = extractPosVel(realSimData.saveval)\n    posRealMod, velRealMod = extractPosVel(realSimDataMod.saveval)\n    posSimple, velSimple = extractPosVel(simpleSimData.saveval)\n    posSimpleMod, velSimpleMod = extractPosVel(simpleSimDataMod.saveval)\n    \n    run = length(solutionAfter)\n    \n    posNeural, velNeural = [], []\n    posNeuralMod, velNeuralMod = [], []\n    for i in 1:run\n        dataNeural = extractPosVel(solutionAfter[i].u)\n        push!(posNeural, (solutionAfter[i].t, dataNeural[1]))\n        push!(velNeural, (solutionAfter[i].t, dataNeural[2]))\n        \n        dataNeuralMod = extractPosVel(solutionAfterMod[i].u)\n        push!(posNeuralMod, (solutionAfterMod[i].t, dataNeuralMod[1]))\n        push!(velNeuralMod, (solutionAfterMod[i].t, dataNeuralMod[2]))\n    end\n         \n    # plot results s (default initial states)\n    xLabel=\"t [s]\"\n    yLabel=\"mass position [m]\"\n    title = \"Default: Mass position after Run: $(run)\"\n    plot_results(title, xLabel, yLabel, tSave, posReal, posSimple, posNeural)\n\n    # plot results s (modified initial states)\n    title = \"Modified: Mass position after Run: $(run)\"\n    plot_results(title, xLabel, yLabel, tSave, posRealMod, posSimpleMod, posNeuralMod)\n\n    # plot results v (default initial states)\n    yLabel=\"mass velocity [m/s]\"\n    title = \"Default: Mass velocity after Run: $(run)\"\n    plot_results(title, xLabel, yLabel, tSave, velReal, velSimple, velNeural)\n\n    # plot results v (modified initial states)    \n    title = \"Modified: Mass velocity after Run: $(run)\"\n    plot_results(title, xLabel, yLabel, tSave, velRealMod, velSimpleMod, velNeuralMod)\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"plot_all_results (generic function with 1 method)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The function plot_friction_model() compares the friction model of the realFMU, simpleFMU and neuralFMU. For this, the velocity and force from the simulation data of the realFMU is needed. The force data is calculated with the extracted last layer of the neuralFMU to the real velocity in line 9 by iterating over the vector velReal. In the next rows, the velocity and force data (if available) for each of the three FMUs are combined into a matrix. The first row of the matrix corresponds to the later x-axis and here the velocity is plotted. The second row corresponds to the y-axis and here the force is plotted. This matrix is sorted and plotted by the first entries (velocity) with the function sortperm(). The graph with at least three graphs is plotted in line 33. As output this function has the forces of the neuralFMU.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function plot_friction_model(realSimData, netBottom, forces)    \n    linestyles = [:dot, :solid]\n    \n    velReal = collect(data[2] for data in realSimData.saveval)\n    forceReal = collect(data[4] for data in realSimData.saveval)\n\n    push!(forces, zeros(length(velReal)))\n    for i in 1:length(velReal)\n        forces[end][i] = -netBottom([velReal[i], 0.0])[2]\n    end\n\n    run = length(forces) \n    \n    fig = generate_figure(\"Friction model $(run)\", \"v [m/s]\", \"friction force [N]\", (-1.25, 1.25))\n\n    fricSimple = hcat(velReal, zeros(length(velReal)))\n    fricSimple[sortperm(fricSimple[:, 1]), :]\n    Plots.plot!(fig, fricSimple[:,1], fricSimple[:,2], label=\"SimpleFMU\", linewidth=2)\n\n    fricReal = hcat(velReal, forceReal)\n    fricReal[sortperm(fricReal[:, 1]), :]\n    Plots.plot!(fig, fricReal[:,1], fricReal[:,2], label=\"reference\", linewidth=2)\n\n    for i in 1:run\n        fricNeural = hcat(velReal, forces[i])\n        fricNeural[sortperm(fricNeural[:, 1]), :]\n        Plots.plot!(fig, fricNeural[:,1], fricNeural[:,2], label=\"NeuralFMU ($(i*2500))\", \n                    linewidth=2, linestyle=linestyles[i], linecolor=:green)\n        @info \"Friction model $i mse: $(Flux.Losses.mse(fricNeural[:,2], fricReal[:,2]))\"\n    end\n    flush(stderr)\n\n    Plots.display(fig)\n    \n    return forces   \nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"plot_friction_model (generic function with 1 method)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The following function is used to display the different displacement modells of the realFMU, simpleFMU and neuralFMU. The displacement of the realFMU and simpleFMU is very trivial and is only a constant. The position data of the realFMU is needed to calculate the displacement. The displacement for the neuralFMU is calculated using the first extracted layer of the neural network, subtracting the real position and the displacement of the simpleFMU. Also in this function, the graphs of the three FMUs are compared in a plot.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"function plot_displacement_model(realSimData, netTop, displacements, tSave, displacement)\n    linestyles = [:dot, :solid]\n    \n    posReal = collect(data[1] for data in realSimData.saveval)\n    \n    push!(displacements, zeros(length(posReal)))\n    for i in 1:length(posReal)\n        displacements[end][i] = netTop([posReal[i], 0.0])[1] - posReal[i] - displacement\n    end\n\n    run = length(displacements)\n    fig = generate_figure(\"Displacement model $(run)\", \"t [s]\", \"displacement [m]\")\n    Plots.plot!(fig, [tSave[1], tSave[end]], [displacement, displacement], label=\"simpleFMU\", linewidth=2)\n    Plots.plot!(fig, [tSave[1], tSave[end]], [0.0, 0.0], label=\"reference\", linewidth=2)\n    for i in 1:run\n        Plots.plot!(fig, tSave, displacements[i], label=\"NeuralFMU ($(i*2500))\", \n                    linewidth=2, linestyle=linestyles[i], linecolor=:green)\n    end\n\n    Plots.display(fig)\n    \n    return displacements\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"plot_displacement_model (generic function with 1 method)","category":"page"},{"location":"examples/modelica_conference_2021/#Structure-of-the-NeuralFMU","page":"Modelica Conference 2021","title":"Structure of the NeuralFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"In the following, the topology of the NeuralFMU is constructed. It consists of a dense layer that has exactly as many inputs and outputs as the model has states numStates (and therefore state derivatives). It also sets the initial weights and offsets for the first dense layer, as well as the activation function, which consists of the identity. An input layer follows, which then leads into the simpleFMU model. The ME-FMU computes the state derivatives for a given system state. Following the simpleFMU is a dense layer that has numStates states. The output of this layer consists of 8 output nodes and a identity activation function. The next layer has 8 input and output nodes with a tanh activation function. The last layer is again a dense layer with 8 input nodes and the number of states as outputs. Here, it is important that no tanh-activation function follows, because otherwise the pendulums state values would be limited to the interval -11.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# NeuralFMU setup\nnumStates = fmiGetNumberOfStates(simpleFMU)\n\nnet = Chain(Dense(numStates, numStates, identity; \n                  initW = (out, in) -> [[1.0, 0.0] [0.0, 1.0]], \n                  initb = out -> zeros(out)),\n            inputs -> fmi2EvaluateME(simpleFMU, inputs),\n            Dense(numStates, 8, identity),\n            Dense(8, 8, tanh),\n            Dense(8, numStates))","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Chain(\n  Dense(2, 2),                          \u001b[90m# 6 parameters\u001b[39m\n  var\"#17#20\"(),\n  Dense(2, 8),                          \u001b[90m# 24 parameters\u001b[39m\n  Dense(8, 8, tanh),                    \u001b[90m# 72 parameters\u001b[39m\n  Dense(8, 2),                          \u001b[90m# 18 parameters\u001b[39m\n)\u001b[90m                   # Total: 8 arrays, \u001b[39m120 parameters, 1016 bytes.","category":"page"},{"location":"examples/modelica_conference_2021/#Definition-of-the-NeuralFMU","page":"Modelica Conference 2021","title":"Definition of the NeuralFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The instantiation of the ME-NeuralFMU is done as a one-liner. The FMU (simpleFMU), the structure of the network net, start tStart and end time tStop, the numerical solver Tsit5() and the time steps tSave for saving are specified.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"neuralFMU = ME_NeuralFMU(simpleFMU, net, (tStart, tStop), Tsit5(); saveat=tSave);","category":"page"},{"location":"examples/modelica_conference_2021/#Plot-before-training","page":"Modelica Conference 2021","title":"Plot before training","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Here the state trajactory of the simpleFMU is recorded. Doesn't really look like a pendulum yet, but the system is random initialized by default. In the later plots, the effect of learning can be seen.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"solutionBefore = neuralFMU(x₀)\nfmiPlot(simpleFMU, solutionBefore)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/#Training-of-the-NeuralFMU","page":"Modelica Conference 2021","title":"Training of the NeuralFMU","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"For the training of the NeuralFMU the parameters are extracted. All parameters of the first layer are set to the absolute value.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"# train\nparamsNet = Flux.params(neuralFMU)\n\nfor i in 1:length(paramsNet[1])\n    if paramsNet[1][i] < 0.0 \n        paramsNet[1][i] = -paramsNet[1][i]\n    end\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The well-known ADAM optimizer for minimizing the gradient descent is used as further passing parameters. Additionally, the previously defined loss and callback function as well as a one for the number of epochs are passed. Only one epoch is trained so that the NeuralFMU is precompiled.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"optim = ADAM()\nFlux.train!(lossSum, paramsNet, Iterators.repeated((), 1), optim; cb=callb) ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"┌ Info:   Loss [1]: 0.42176\n│         Avg displacement in data: 0.64943\n│         Weight/Scale: 1.0009999999844448   Bias/Offset: 0.0009999999815963334\n└ @ Main In[15]:15","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Some vectors for collecting data are initialized and the number of runs, epochs and iterations are set.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"solutionAfter = []\nsolutionAfterMod = []\nforces = []\ndisplacements = []\n\nnumRuns = 2\nnumEpochs= 5\nnumIterations = 500;","category":"page"},{"location":"examples/modelica_conference_2021/#Training-loop","page":"Modelica Conference 2021","title":"Training loop","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"The code section shown here represents the training loop. The loop is structured so that it has numRuns runs, where each run has numEpochs epochs, and the training is performed at each epoch with numIterations iterations. In lines 9 and 10, the data for the neuralFMU for the default and modified initial states are appended to the corresponding vectors. The plots for the opposition of position and velocity is done in line 13 by calling the function plot_all_results. In the following lines the last layers are extracted from the neuralFMU and formed into an independent network netBottom. The parmaters for the netBottom network come from the original architecture and are shared. In line 20, the new network is used to represent the friction model in a graph. An analogous construction of the next part of the training loop, where here the first layer is taken from the neuralFMU and converted to its own network netTop. This network is used to record the displacement model. The different graphs are generated for each run and can thus be compared. ","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"for run in 1:numRuns    \n    @time for epoch in 1:numEpochs\n        @info \"Run: $(run)/$(numRuns)  Epoch: $(epoch)/$(numEpochs)\"\n        Flux.train!(lossSum, paramsNet, Iterators.repeated((), numIterations), optim; cb=callb)\n    end\n    flush(stderr)\n    flush(stdout)\n    \n    push!(solutionAfter, neuralFMU(x₀))\n    push!(solutionAfterMod, neuralFMU(xMod₀))\n\n    # generate all plots for the position and velocity\n    plot_all_results(realSimData, realSimDataMod, simpleSimData, simpleSimDataMod, solutionAfter, solutionAfterMod)\n    \n    # friction model extraction\n    layersBottom = neuralFMU.neuralODE.model.layers[3:5]\n    netBottom = Chain(layersBottom...)\n    transferParams!(netBottom, paramsNet, 7)\n    \n    forces = plot_friction_model(realSimData, netBottom, forces) \n    \n    # displacement model extraction\n    layersTop = neuralFMU.neuralODE.model.layers[1:1]\n    netTop = Chain(layersTop...)\n    transferParams!(netTop, paramsNet, 1)\n\n    displacements = plot_displacement_model(realSimData, netTop, displacements, tSave, displacement)\nend","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"┌ Info: Run: 1/2  Epoch: 1/5\n└ @ Main In[27]:3\n┌ Info:   Loss [51]: 0.28565\n│         Avg displacement in data: 0.53446\n│         Weight/Scale: 1.0286111783896719   Bias/Offset: 0.02956336213052823\n└ @ Main In[15]:15\n┌ Info:   Loss [101]: 0.27637\n│         Avg displacement in data: 0.52571\n│         Weight/Scale: 1.0227380319341028   Bias/Offset: 0.024779979783415926\n└ @ Main In[15]:15\n┌ Info:   Loss [151]: 0.23802\n│         Avg displacement in data: 0.48787\n│         Weight/Scale: 1.0109595043163218   Bias/Offset: 0.017657584767572194\n└ @ Main In[15]:15\n┌ Info:   Loss [201]: 0.11284\n│         Avg displacement in data: 0.33591\n│         Weight/Scale: 1.0187168091838543   Bias/Offset: 0.04967643587576684\n└ @ Main In[15]:15\n┌ Info:   Loss [251]: 0.04202\n│         Avg displacement in data: 0.20498\n│         Weight/Scale: 1.0513631843151288   Bias/Offset: 0.08709946191489178\n└ @ Main In[15]:15\n┌ Info:   Loss [301]: 0.03441\n│         Avg displacement in data: 0.18549\n│         Weight/Scale: 1.0521782955624404   Bias/Offset: 0.0833209946731039\n└ @ Main In[15]:15\n┌ Info:   Loss [351]: 0.02889\n│         Avg displacement in data: 0.16997\n│         Weight/Scale: 1.0507789805347363   Bias/Offset: 0.07821369759961062\n└ @ Main In[15]:15\n┌ Info:   Loss [401]: 0.0247\n│         Avg displacement in data: 0.15715\n│         Weight/Scale: 1.0489247270188242   Bias/Offset: 0.07403606623859026\n└ @ Main In[15]:15\n┌ Info:   Loss [451]: 0.0214\n│         Avg displacement in data: 0.14629\n│         Weight/Scale: 1.0466033317763261   Bias/Offset: 0.07065149828990938\n└ @ Main In[15]:15\n┌ Info:   Loss [501]: 0.01887\n│         Avg displacement in data: 0.13738\n│         Weight/Scale: 1.0438603361948664   Bias/Offset: 0.06787694206377189\n└ @ Main In[15]:15\n┌ Info: Run: 1/2  Epoch: 2/5\n└ @ Main In[27]:3\n┌ Info:   Loss [551]: 0.01706\n│         Avg displacement in data: 0.13061\n│         Weight/Scale: 1.040893662600301   Bias/Offset: 0.06575502539158813\n└ @ Main In[15]:15\n┌ Info:   Loss [601]: 0.01562\n│         Avg displacement in data: 0.12498\n│         Weight/Scale: 1.0380266250358758   Bias/Offset: 0.06428430875434155\n└ @ Main In[15]:15\n┌ Info:   Loss [651]: 0.01439\n│         Avg displacement in data: 0.11995\n│         Weight/Scale: 1.0351399591310342   Bias/Offset: 0.06312510899266015\n└ @ Main In[15]:15\n┌ Info:   Loss [701]: 0.01332\n│         Avg displacement in data: 0.11542\n│         Weight/Scale: 1.032320441439091   Bias/Offset: 0.062197166818365436\n└ @ Main In[15]:15\n┌ Info:   Loss [751]: 0.01234\n│         Avg displacement in data: 0.1111\n│         Weight/Scale: 1.029645335632183   Bias/Offset: 0.06143114604889372\n└ @ Main In[15]:15\n┌ Info:   Loss [801]: 0.01144\n│         Avg displacement in data: 0.10697\n│         Weight/Scale: 1.027236022857356   Bias/Offset: 0.06085332930418252\n└ @ Main In[15]:15\n┌ Info:   Loss [851]: 0.01064\n│         Avg displacement in data: 0.10313\n│         Weight/Scale: 1.0251223753196803   Bias/Offset: 0.06044785999331985\n└ @ Main In[15]:15\n┌ Info:   Loss [901]: 0.0099\n│         Avg displacement in data: 0.09951\n│         Weight/Scale: 1.023315362695265   Bias/Offset: 0.06024866642970074\n└ @ Main In[15]:15\n┌ Info:   Loss [951]: 0.00922\n│         Avg displacement in data: 0.09602\n│         Weight/Scale: 1.0217680458848062   Bias/Offset: 0.06023526171860316\n└ @ Main In[15]:15\n┌ Info:   Loss [1001]: 0.00858\n│         Avg displacement in data: 0.09265\n│         Weight/Scale: 1.020440934786138   Bias/Offset: 0.06039598811238556\n└ @ Main In[15]:15\n┌ Info: Run: 1/2  Epoch: 3/5\n└ @ Main In[27]:3\n┌ Info:   Loss [1051]: 0.00796\n│         Avg displacement in data: 0.0892\n│         Weight/Scale: 1.019234221745652   Bias/Offset: 0.06063012165772161\n└ @ Main In[15]:15\n┌ Info:   Loss [1101]: 0.00733\n│         Avg displacement in data: 0.08563\n│         Weight/Scale: 1.0180596810795992   Bias/Offset: 0.06083635634563215\n└ @ Main In[15]:15\n┌ Info:   Loss [1151]: 0.00672\n│         Avg displacement in data: 0.08195\n│         Weight/Scale: 1.0169472276860123   Bias/Offset: 0.061046194352924965\n└ @ Main In[15]:15\n┌ Info:   Loss [1201]: 0.00612\n│         Avg displacement in data: 0.0782\n│         Weight/Scale: 1.0159093038209581   Bias/Offset: 0.061218072186984086\n└ @ Main In[15]:15\n┌ Info:   Loss [1251]: 0.00556\n│         Avg displacement in data: 0.07456\n│         Weight/Scale: 1.014996461256444   Bias/Offset: 0.061342041802755105\n└ @ Main In[15]:15\n┌ Info:   Loss [1301]: 0.00504\n│         Avg displacement in data: 0.07099\n│         Weight/Scale: 1.014237800262788   Bias/Offset: 0.0613774404803237\n└ @ Main In[15]:15\n┌ Info:   Loss [1351]: 0.00462\n│         Avg displacement in data: 0.06797\n│         Weight/Scale: 1.0137294101233838   Bias/Offset: 0.06146807431241549\n└ @ Main In[15]:15\n┌ Info:   Loss [1401]: 0.00428\n│         Avg displacement in data: 0.06539\n│         Weight/Scale: 1.0134108488171056   Bias/Offset: 0.06159381154669232\n└ @ Main In[15]:15\n┌ Info:   Loss [1451]: 0.00399\n│         Avg displacement in data: 0.06318\n│         Weight/Scale: 1.0132445181483052   Bias/Offset: 0.06178230970201576\n└ @ Main In[15]:15\n┌ Info:   Loss [1501]: 0.00375\n│         Avg displacement in data: 0.06122\n│         Weight/Scale: 1.0131776018405887   Bias/Offset: 0.062025043165095266\n└ @ Main In[15]:15\n┌ Info: Run: 1/2  Epoch: 4/5\n└ @ Main In[27]:3\n┌ Info:   Loss [1551]: 0.00354\n│         Avg displacement in data: 0.05947\n│         Weight/Scale: 1.0131710033335706   Bias/Offset: 0.062306134823292655\n└ @ Main In[15]:15\n┌ Info:   Loss [1601]: 0.00335\n│         Avg displacement in data: 0.05786\n│         Weight/Scale: 1.0132024755679327   Bias/Offset: 0.06261319064256955\n└ @ Main In[15]:15\n┌ Info:   Loss [1651]: 0.00318\n│         Avg displacement in data: 0.05638\n│         Weight/Scale: 1.013260685107001   Bias/Offset: 0.06293832944092709\n└ @ Main In[15]:15\n┌ Info:   Loss [1701]: 0.00303\n│         Avg displacement in data: 0.05502\n│         Weight/Scale: 1.0133396705608375   Bias/Offset: 0.06327652852868175\n└ @ Main In[15]:15\n┌ Info:   Loss [1751]: 0.00289\n│         Avg displacement in data: 0.05375\n│         Weight/Scale: 1.0134358522567677   Bias/Offset: 0.0636243484149065\n└ @ Main In[15]:15\n┌ Info:   Loss [1801]: 0.00276\n│         Avg displacement in data: 0.05258\n│         Weight/Scale: 1.0135467571177528   Bias/Offset: 0.06397929890444008\n└ @ Main In[15]:15\n┌ Info:   Loss [1851]: 0.00265\n│         Avg displacement in data: 0.05149\n│         Weight/Scale: 1.013670516900537   Bias/Offset: 0.06433952955751007\n└ @ Main In[15]:15\n┌ Info:   Loss [1901]: 0.00255\n│         Avg displacement in data: 0.05046\n│         Weight/Scale: 1.0138056596433167   Bias/Offset: 0.0647036516197806\n└ @ Main In[15]:15\n┌ Info:   Loss [1951]: 0.00245\n│         Avg displacement in data: 0.04951\n│         Weight/Scale: 1.0139510057836334   Bias/Offset: 0.06507062834118049\n└ @ Main In[15]:15\n┌ Info:   Loss [2001]: 0.00236\n│         Avg displacement in data: 0.04861\n│         Weight/Scale: 1.0141055915661854   Bias/Offset: 0.06543967970994506\n└ @ Main In[15]:15\n┌ Info: Run: 1/2  Epoch: 5/5\n└ @ Main In[27]:3\n┌ Info:   Loss [2051]: 0.00228\n│         Avg displacement in data: 0.04776\n│         Weight/Scale: 1.0142686074193126   Bias/Offset: 0.06581023672978648\n└ @ Main In[15]:15\n┌ Info:   Loss [2101]: 0.00221\n│         Avg displacement in data: 0.04698\n│         Weight/Scale: 1.014447820077644   Bias/Offset: 0.06619614472538896\n└ @ Main In[15]:15\n┌ Info:   Loss [2151]: 0.00214\n│         Avg displacement in data: 0.04622\n│         Weight/Scale: 1.0146195124031292   Bias/Offset: 0.06656358400019907\n└ @ Main In[15]:15\n┌ Info:   Loss [2201]: 0.00207\n│         Avg displacement in data: 0.0455\n│         Weight/Scale: 1.0148042412420994   Bias/Offset: 0.06693688337331748\n└ @ Main In[15]:15\n┌ Info:   Loss [2251]: 0.00201\n│         Avg displacement in data: 0.04482\n│         Weight/Scale: 1.0149946664984544   Bias/Offset: 0.06730950022717663\n└ @ Main In[15]:15\n┌ Info:   Loss [2301]: 0.00195\n│         Avg displacement in data: 0.04417\n│         Weight/Scale: 1.0151905860411778   Bias/Offset: 0.06768173855181797\n└ @ Main In[15]:15\n┌ Info:   Loss [2351]: 0.0019\n│         Avg displacement in data: 0.04355\n│         Weight/Scale: 1.0153912809305963   Bias/Offset: 0.06805325518281151\n└ @ Main In[15]:15\n┌ Info:   Loss [2401]: 0.00185\n│         Avg displacement in data: 0.04297\n│         Weight/Scale: 1.0155961224151784   Bias/Offset: 0.06842376493363168\n└ @ Main In[15]:15\n┌ Info:   Loss [2451]: 0.0018\n│         Avg displacement in data: 0.04241\n│         Weight/Scale: 1.0158045051687399   Bias/Offset: 0.06879297485736173\n└ @ Main In[15]:15\n┌ Info:   Loss [2501]: 0.00175\n│         Avg displacement in data: 0.04188\n│         Weight/Scale: 1.0160158385445512   Bias/Offset: 0.0691605750753205\n└ @ Main In[15]:15\n\n\n3164.063564 seconds (7.76 G allocations: 476.333 GiB, 4.15% gc time)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"┌ Info: Friction model 1 mse: 0.7541038092408717\n└ @ Main In[19]:29","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"┌ Info: Run: 2/2  Epoch: 1/5\n└ @ Main In[27]:3\n┌ Info:   Loss [2551]: 0.00171\n│         Avg displacement in data: 0.04137\n│         Weight/Scale: 1.016229548801126   Bias/Offset: 0.06952624500869685\n└ @ Main In[15]:15\n┌ Info:   Loss [2601]: 0.00167\n│         Avg displacement in data: 0.04088\n│         Weight/Scale: 1.0164450811761696   Bias/Offset: 0.06988965411129339\n└ @ Main In[15]:15\n┌ Info:   Loss [2651]: 0.00163\n│         Avg displacement in data: 0.04042\n│         Weight/Scale: 1.0166619044860516   Bias/Offset: 0.07025046670690757\n└ @ Main In[15]:15\n┌ Info:   Loss [2701]: 0.0016\n│         Avg displacement in data: 0.03998\n│         Weight/Scale: 1.0168795146203686   Bias/Offset: 0.07060834433025422\n└ @ Main In[15]:15\n┌ Info:   Loss [2751]: 0.00156\n│         Avg displacement in data: 0.03955\n│         Weight/Scale: 1.0170974387940153   Bias/Offset: 0.07096295141121652\n└ @ Main In[15]:15\n┌ Info:   Loss [2801]: 0.00153\n│         Avg displacement in data: 0.03915\n│         Weight/Scale: 1.0173152335687383   Bias/Offset: 0.07131395463827797\n└ @ Main In[15]:15\n┌ Info:   Loss [2851]: 0.00151\n│         Avg displacement in data: 0.03881\n│         Weight/Scale: 1.0175129332462627   Bias/Offset: 0.07166247578110631\n└ @ Main In[15]:15\n┌ Info:   Loss [2901]: 0.00147\n│         Avg displacement in data: 0.0384\n│         Weight/Scale: 1.0176737837896552   Bias/Offset: 0.07198377152194411\n└ @ Main In[15]:15\n┌ Info:   Loss [2951]: 0.00145\n│         Avg displacement in data: 0.03805\n│         Weight/Scale: 1.0178463348529252   Bias/Offset: 0.0722931481446665\n└ @ Main In[15]:15\n┌ Info:   Loss [3001]: 0.00142\n│         Avg displacement in data: 0.03771\n│         Weight/Scale: 1.0180181098515222   Bias/Offset: 0.07259755977316348\n└ @ Main In[15]:15\n┌ Info: Run: 2/2  Epoch: 2/5\n└ @ Main In[27]:3\n┌ Info:   Loss [3051]: 0.0014\n│         Avg displacement in data: 0.03738\n│         Weight/Scale: 1.0181885812904008   Bias/Offset: 0.07289715852819231\n└ @ Main In[15]:15\n┌ Info:   Loss [3101]: 0.00137\n│         Avg displacement in data: 0.03707\n│         Weight/Scale: 1.018357766645548   Bias/Offset: 0.07319177404784898\n└ @ Main In[15]:15\n┌ Info:   Loss [3151]: 0.00135\n│         Avg displacement in data: 0.03677\n│         Weight/Scale: 1.0185255265576187   Bias/Offset: 0.07348121013093241\n└ @ Main In[15]:15\n┌ Info:   Loss [3201]: 0.00134\n│         Avg displacement in data: 0.03664\n│         Weight/Scale: 1.0186384410864069   Bias/Offset: 0.07375546910282832\n└ @ Main In[15]:15\n┌ Info:   Loss [3251]: 0.00131\n│         Avg displacement in data: 0.0362\n│         Weight/Scale: 1.0187702803549026   Bias/Offset: 0.0740192177800107\n└ @ Main In[15]:15\n┌ Info:   Loss [3301]: 0.00129\n│         Avg displacement in data: 0.03593\n│         Weight/Scale: 1.0188989023698134   Bias/Offset: 0.07426758729816377\n└ @ Main In[15]:15\n┌ Info:   Loss [3351]: 0.00127\n│         Avg displacement in data: 0.03567\n│         Weight/Scale: 1.0190236614619597   Bias/Offset: 0.07450872117620551\n└ @ Main In[15]:15\n┌ Info:   Loss [3401]: 0.00125\n│         Avg displacement in data: 0.03542\n│         Weight/Scale: 1.0191465831768112   Bias/Offset: 0.07474436265438232\n└ @ Main In[15]:15\n┌ Info:   Loss [3451]: 0.00124\n│         Avg displacement in data: 0.03517\n│         Weight/Scale: 1.0192677355297068   Bias/Offset: 0.0749743867108784\n└ @ Main In[15]:15\n┌ Info:   Loss [3501]: 0.00122\n│         Avg displacement in data: 0.03495\n│         Weight/Scale: 1.0193812473692951   Bias/Offset: 0.07519395643987335\n└ @ Main In[15]:15\n┌ Info: Run: 2/2  Epoch: 3/5\n└ @ Main In[27]:3\n┌ Info:   Loss [3551]: 0.00121\n│         Avg displacement in data: 0.03472\n│         Weight/Scale: 1.019422564973201   Bias/Offset: 0.07539694229469925\n└ @ Main In[15]:15\n┌ Info:   Loss [3601]: 0.00119\n│         Avg displacement in data: 0.03447\n│         Weight/Scale: 1.0195160436668147   Bias/Offset: 0.07559780044664313\n└ @ Main In[15]:15\n┌ Info:   Loss [3651]: 0.00117\n│         Avg displacement in data: 0.03426\n│         Weight/Scale: 1.0195969683940844   Bias/Offset: 0.07577955787811985\n└ @ Main In[15]:15\n┌ Info:   Loss [3701]: 0.00116\n│         Avg displacement in data: 0.03404\n│         Weight/Scale: 1.0196760008625247   Bias/Offset: 0.07595622675555126\n└ @ Main In[15]:15\n┌ Info:   Loss [3751]: 0.00114\n│         Avg displacement in data: 0.03383\n│         Weight/Scale: 1.0197531837969303   Bias/Offset: 0.07612745575775592\n└ @ Main In[15]:15\n┌ Info:   Loss [3801]: 0.00113\n│         Avg displacement in data: 0.03362\n│         Weight/Scale: 1.019828629356617   Bias/Offset: 0.07629319358816561\n└ @ Main In[15]:15\n┌ Info:   Loss [3851]: 0.00116\n│         Avg displacement in data: 0.03401\n│         Weight/Scale: 1.019816626062623   Bias/Offset: 0.07640549578331124\n└ @ Main In[15]:15\n┌ Info:   Loss [3901]: 0.0011\n│         Avg displacement in data: 0.03323\n│         Weight/Scale: 1.0198752279345842   Bias/Offset: 0.07658674166120087\n└ @ Main In[15]:15\n┌ Info:   Loss [3951]: 0.00109\n│         Avg displacement in data: 0.03304\n│         Weight/Scale: 1.0199207839653588   Bias/Offset: 0.0767172503081904\n└ @ Main In[15]:15\n┌ Info:   Loss [4001]: 0.00108\n│         Avg displacement in data: 0.03285\n│         Weight/Scale: 1.0199597058433785   Bias/Offset: 0.07683851508060009\n└ @ Main In[15]:15\n┌ Info: Run: 2/2  Epoch: 4/5\n└ @ Main In[27]:3\n┌ Info:   Loss [4051]: 0.00107\n│         Avg displacement in data: 0.03266\n│         Weight/Scale: 1.0199968911982995   Bias/Offset: 0.07695517518994958\n└ @ Main In[15]:15\n┌ Info:   Loss [4101]: 0.00105\n│         Avg displacement in data: 0.03248\n│         Weight/Scale: 1.020032467149272   Bias/Offset: 0.07706709946127448\n└ @ Main In[15]:15\n┌ Info:   Loss [4151]: 0.00104\n│         Avg displacement in data: 0.0323\n│         Weight/Scale: 1.0200654616983746   Bias/Offset: 0.07717347536342965\n└ @ Main In[15]:15\n┌ Info:   Loss [4201]: 0.00103\n│         Avg displacement in data: 0.03214\n│         Weight/Scale: 1.020010641353738   Bias/Offset: 0.0772656517477099\n└ @ Main In[15]:15\n┌ Info:   Loss [4251]: 0.00102\n│         Avg displacement in data: 0.03195\n│         Weight/Scale: 1.0200099098377153   Bias/Offset: 0.07734909864140466\n└ @ Main In[15]:15\n┌ Info:   Loss [4301]: 0.00101\n│         Avg displacement in data: 0.03178\n│         Weight/Scale: 1.0200111463586086   Bias/Offset: 0.07741977155652452\n└ @ Main In[15]:15\n┌ Info:   Loss [4351]: 0.001\n│         Avg displacement in data: 0.03161\n│         Weight/Scale: 1.0200114085786474   Bias/Offset: 0.07748793490454804\n└ @ Main In[15]:15\n┌ Info:   Loss [4401]: 0.00099\n│         Avg displacement in data: 0.03144\n│         Weight/Scale: 1.0200100185809562   Bias/Offset: 0.0775524808439198\n└ @ Main In[15]:15\n┌ Info:   Loss [4451]: 0.00098\n│         Avg displacement in data: 0.03127\n│         Weight/Scale: 1.0200070281665077   Bias/Offset: 0.07761338094958463\n└ @ Main In[15]:15\n┌ Info:   Loss [4501]: 0.00102\n│         Avg displacement in data: 0.03201\n│         Weight/Scale: 1.0199471604345323   Bias/Offset: 0.07762620102212063\n└ @ Main In[15]:15\n┌ Info: Run: 2/2  Epoch: 5/5\n└ @ Main In[27]:3\n┌ Info:   Loss [4551]: 0.00096\n│         Avg displacement in data: 0.03095\n│         Weight/Scale: 1.0198657900275487   Bias/Offset: 0.07769937232889036\n└ @ Main In[15]:15\n┌ Info:   Loss [4601]: 0.00095\n│         Avg displacement in data: 0.03079\n│         Weight/Scale: 1.019842886879103   Bias/Offset: 0.07774192990756451\n└ @ Main In[15]:15\n┌ Info:   Loss [4651]: 0.00094\n│         Avg displacement in data: 0.03063\n│         Weight/Scale: 1.019808929671946   Bias/Offset: 0.0777690163720817\n└ @ Main In[15]:15\n┌ Info:   Loss [4701]: 0.00093\n│         Avg displacement in data: 0.03047\n│         Weight/Scale: 1.0197720180347962   Bias/Offset: 0.0777931244870986\n└ @ Main In[15]:15\n┌ Info:   Loss [4751]: 0.00092\n│         Avg displacement in data: 0.03031\n│         Weight/Scale: 1.0197330579203314   Bias/Offset: 0.07781469421503724\n└ @ Main In[15]:15\n┌ Info:   Loss [4801]: 0.00091\n│         Avg displacement in data: 0.03015\n│         Weight/Scale: 1.0196920696171694   Bias/Offset: 0.07783379376725892\n└ @ Main In[15]:15\n┌ Info:   Loss [4851]: 0.00094\n│         Avg displacement in data: 0.03062\n│         Weight/Scale: 1.0196964319427353   Bias/Offset: 0.07789033139683764\n└ @ Main In[15]:15\n┌ Info:   Loss [4901]: 0.00089\n│         Avg displacement in data: 0.02985\n│         Weight/Scale: 1.0194643521872544   Bias/Offset: 0.07784603468403976\n└ @ Main In[15]:15\n┌ Info:   Loss [4951]: 0.00088\n│         Avg displacement in data: 0.02969\n│         Weight/Scale: 1.0193938823333755   Bias/Offset: 0.07784323586407052\n└ @ Main In[15]:15\n┌ Info:   Loss [5001]: 0.00087\n│         Avg displacement in data: 0.02953\n│         Weight/Scale: 1.0193205792837756   Bias/Offset: 0.07783135347576647\n└ @ Main In[15]:15\n\n\n3141.788865 seconds (7.75 G allocations: 476.097 GiB, 4.18% gc time)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"┌ Info: Friction model 1 mse: 0.7541038092408717\n└ @ Main In[19]:29\n┌ Info: Friction model 2 mse: 0.7459716547805014\n└ @ Main In[19]:29","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"(Image: svg)","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Finally, the FMU is cleaned-up.","category":"page"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"fmiUnload(simpleFMU)","category":"page"},{"location":"examples/modelica_conference_2021/#Summar","page":"Modelica Conference 2021","title":"Summar","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"Based on the plots, it can be seen that the curves of the realFMU and the neuralFMU are very close. The neuralFMU is able to learn the friction and displacement model.","category":"page"},{"location":"examples/modelica_conference_2021/#Source","page":"Modelica Conference 2021","title":"Source","text":"","category":"section"},{"location":"examples/modelica_conference_2021/","page":"Modelica Conference 2021","title":"Modelica Conference 2021","text":"[1] Tobias Thummerer, Lars Mikelsons and Josef Kircher. 2021. NeuralFMU: towards structural integration of FMUs into neural networks. Martin Sjölund, Lena Buffoni, Adrian Pop and Lennart Ochel (Ed.). Proceedings of 14th Modelica Conference 2021, Linköping, Sweden, September 20-24, 2021. Linköping University Electronic Press, Linköping (Linköping Electronic Conference Proceedings ; 181), 297-306. DOI: 10.3384/ecp21181297","category":"page"},{"location":"examples/simple_hybrid_ME/#Creation-and-training-of-ME-NeuralFMUs","page":"Simple ME-NeuralFMU","title":"Creation and training of ME-NeuralFMUs","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Tutorial by Johannes Stoljar, Tobias Thummerer","category":"page"},{"location":"examples/simple_hybrid_ME/#License","page":"Simple ME-NeuralFMU","title":"License","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Copyright (c) 2021 Tobias Thummerer, Lars Mikelsons, Johannes Stoljar","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Licensed under the MIT license. See LICENSE file in the project root for details.","category":"page"},{"location":"examples/simple_hybrid_ME/#Motivation","page":"Simple ME-NeuralFMU","title":"Motivation","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"This Julia Package is motivated by the application of hybrid modeling. This package enables the user to integrate his simulation model between neural networks (NeuralFMU). For this, the simulation model must be exported as FMU (functional mock-up unit), which corresponds to a widely used standard. The big advantage of hybrid modeling with artificial neural networks is, that effects that are difficult to model (because they might be unknown) can be easily learned by the neural networks. For this purpose, the NeuralFMU is trained with measurement data containing the unmodeled physical effect. The final product is a simulation model including the orignially unmodeled effects. Another big advantage of the NeuralFMU is that it works with little data, because the FMU already contains the characterisitic functionality of the simulation and only the missing effects are added.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"NeuralFMUs need not to be as easy as in this example. Basically a NeuralFMU can combine different ANN topologies that manipulate any FMU-input (system state, system inputs, time) and any FMU-output (system state derivative, system outputs, other system variables). However, for this example a NeuralFMU topology as shown in the following picture is used.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: NeuralFMU.svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"NeuralFMU (ME) from [1].","category":"page"},{"location":"examples/simple_hybrid_ME/#Introduction-to-the-example","page":"Simple ME-NeuralFMU","title":"Introduction to the example","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In this example, simplified modeling of a one-dimensional spring pendulum (without friction) is compared to a model of the same system that includes a nonlinear friction model. The FMU with the simplified model will be named simpleFMU in the following and the model with the friction will be named realFMU. At the beginning, the actual state of both simulations is shown, whereby clear deviations can be seen in the graphs. The realFMU serves as a reference graph. The simpleFMU is then integrated into a NeuralFMU architecture and a training of the entire network is performed. After the training the final state is compared again to the realFMU. It can be clearly seen that by using the NeuralFMU, learning of the friction process has taken place.  ","category":"page"},{"location":"examples/simple_hybrid_ME/#Target-group","page":"Simple ME-NeuralFMU","title":"Target group","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The example is primarily intended for users who work in the field of first principle and/or hybrid modeling and are further interested in hybrid model building. The example wants to show how simple it is to combine FMUs with machine learning and to illustrate the advantages of this approach.","category":"page"},{"location":"examples/simple_hybrid_ME/#Other-formats","page":"Simple ME-NeuralFMU","title":"Other formats","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Besides this Jupyter Notebook there is also a Julia file with the same name, which contains only the code cells and for the documentation there is a Markdown file corresponding to the notebook.  ","category":"page"},{"location":"examples/simple_hybrid_ME/#Getting-started","page":"Simple ME-NeuralFMU","title":"Getting started","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/#Installation-prerequisites","page":"Simple ME-NeuralFMU","title":"Installation prerequisites","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":" Description Command Alternative\n1. Enter Package Manager via ] \n2. Install FMI via add FMI add \" https://github.com/ThummeTo/FMI.jl \"\n3. Install FMIFlux via add FMIFlux add \" https://github.com/ThummeTo/FMIFlux.jl \"\n4. Install Flux via add Flux \n5. Install DifferentialEquations via add DifferentialEquations \n6. Install Plots via add Plots ","category":"page"},{"location":"examples/simple_hybrid_ME/#Code-section","page":"Simple ME-NeuralFMU","title":"Code section","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"To run the example, the previously installed packages must be included. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# imports\nusing FMI\nusing FMIFlux\nusing Flux\nusing DifferentialEquations: Tsit5\nimport Plots","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"After importing the packages, the path to the Functional Mock-up Units (FMUs) is set. The FMU is a model exported meeting the Functional Mock-up Interface (FMI) Standard. The FMI is a free standard (fmi-standard.org) that defines a container and an interface to exchange dynamic models using a combination of XML files, binaries and C code zipped into a single file. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The objec-orientated structure of the SpringPendulum1D (simpleFMU) can be seen in the following graphic and corresponds to a simple modeling.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In contrast, the model SpringFrictionPendulum1D (realFMU) is somewhat more accurate, because it includes a friction component. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Here the path for the SpringPendulum1D and the SpringFrictionPendulum1D model is set: ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"simpleFMUPath = joinpath(dirname(@__FILE__), \"../model/SpringPendulum1D.fmu\")\nrealFMUPath = joinpath(dirname(@__FILE__), \"../model/SpringFrictionPendulum1D.fmu\")\nprintln(\"SimpleFMU path: \", simpleFMUPath)\nprintln(\"RealFMU path: \", realFMUPath)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"SimpleFMU path: ../model/SpringPendulum1D.fmu\nRealFMU path: ../model/SpringFrictionPendulum1D.fmu","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Next, the start time and end time of the simulation are set. Finally, a step size is specified to store the results of the simulation at these time steps.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"tStart = 0.0\ntStep = 0.01\ntStop = 5.0\ntSave = collect(tStart:tStep:tStop)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"501-element Vector{Float64}:\n 0.0\n 0.01\n 0.02\n 0.03\n 0.04\n 0.05\n 0.06\n 0.07\n 0.08\n 0.09\n 0.1\n 0.11\n 0.12\n ⋮\n 4.89\n 4.9\n 4.91\n 4.92\n 4.93\n 4.94\n 4.95\n 4.96\n 4.97\n 4.98\n 4.99\n 5.0","category":"page"},{"location":"examples/simple_hybrid_ME/#RealFMU","page":"Simple ME-NeuralFMU","title":"RealFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In the next lines of code the FMU of the realFMU model is loaded and instantiated.  ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"realFMU = fmiLoad(realFMUPath)\nfmiInstantiate!(realFMU; loggingOn=false)\nfmiInfo(realFMU)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"┌ Info: fmi2Unzip(...): Successfully unzipped 28 files at `C:\\Users\\JOHANN~1\\AppData\\Local\\Temp\\fmijl_xMsNGp\\SpringFrictionPendulum1D`.\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:273\n┌ Info: fmi2Load(...): FMU supports both CS and ME, using CS as default if nothing specified.\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:376\n\n\n#################### Begin information for FMU ####################\n\tModel name:\t\t\tSpringFrictionPendulum1D\n\tFMI-Version:\t\t\t2.0\n\tGUID:\t\t\t\t{b02421b8-652a-4d48-9ffc-c2b223aa1b94}\n\tGeneration tool:\t\tDymola Version 2020x (64-bit), 2019-10-10\n\tGeneration time:\t\t2021-11-23T13:36:30Z\n\tVar. naming conv.:\t\tstructured\n\tEvent indicators:\t\t24\n\tInputs:\t\t\t\t0\n\tOutputs:\t\t\t0\n\tStates:\t\t\t\t2\n\n\n┌ Info: fmi2Load(...): FMU resources location is `file:///C:/Users/JOHANN~1/AppData/Local/Temp/fmijl_xMsNGp/SpringFrictionPendulum1D/resources`\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:384\n\n\n\t\t33554432 [\"mass.s\"]\n\t\t33554433 [\"mass.v\", \"mass.v_relfric\"]\n\tSupports Co-Simulation:\t\ttrue\n\t\tModel identifier:\tSpringFrictionPendulum1D\n\t\tGet/Set State:\t\ttrue\n\t\tSerialize State:\ttrue\n\t\tDir. Derivatives:\ttrue\n\t\tVar. com. steps:\ttrue\n\t\tInput interpol.:\ttrue\n\t\tMax order out. der.:\t1\n\tSupports Model-Exchange:\ttrue\n\t\tModel identifier:\tSpringFrictionPendulum1D\n\t\tGet/Set State:\t\ttrue\n\t\tSerialize State:\ttrue\n\t\tDir. Derivatives:\ttrue\n##################### End information for FMU #####################","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Both the start and end time are set via the fmiSetupExperiment() function. The experiment is initialized to get the information of the continuous states. You can get all continuous states of a FMU by the function fmiGetContinuousStates() and this is also done for the realFMU. It has two states: The first state is the position of the mass, which is initilized with 05m, the second state is the velocity, which is initialized with 0fracms.   ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"fmiSetupExperiment(realFMU, tStart, tStop)\n\nfmiEnterInitializationMode(realFMU)\nfmiExitInitializationMode(realFMU)\n\nx₀ = fmiGetContinuousStates(realFMU)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"2-element Vector{Float64}:\n 0.5\n 0.0","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In the following code block the realFMU is simulated, still specifying which variables are included. After the simulation is finished the result of the realFMU can be plotted. This plot also serves as a reference for the other model (simpleFMU).","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"vrs = [\"mass.s\", \"mass.v\", \"mass.a\", \"mass.f\"]\n_, realSimData = fmiSimulate(realFMU, tStart, tStop; recordValues=vrs, saveat=tSave, setup=false, reset=false)\nfmiPlot(realFMU, vrs, realSimData)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"After the plots are created, the FMU is unloaded.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"fmiUnload(realFMU)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The data from the simualtion of the realFMU, are divided into position and velocity data. These data will be needed later. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"velReal = collect(data[2] for data in realSimData.saveval)\nposReal = collect(data[1] for data in realSimData.saveval)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"501-element Vector{Float64}:\n 0.5\n 0.5002147350958549\n 0.5008572068150906\n 0.5019307343013393\n 0.5034377791759037\n 0.5053797361931734\n 0.507757421354132\n 0.5105710760625108\n 0.5138202661486302\n 0.5175035429290001\n 0.5216187663581402\n 0.5261643784845171\n 0.5311361825547609\n ⋮\n 1.0618338496933084\n 1.0629441718413108\n 1.0639256972572486\n 1.0647763670106132\n 1.06549412855799\n 1.0660771282511987\n 1.066523587371857\n 1.066831801521633\n 1.0670001400125004\n 1.0670339828286313\n 1.0670339828276312\n 1.067033982826631","category":"page"},{"location":"examples/simple_hybrid_ME/#SimpleFMU","page":"Simple ME-NeuralFMU","title":"SimpleFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The following lines load, instantiate, simulate and plot the simpleFMU just like the realFMU. The differences between both systems can be clearly seen from the plots. In the plot for the realFMU it can be seen that the oscillation continues to decrease due to the effect of the friction. If you would simulate long enough, the oscillation would come to a standstill in a certain time. The oscillation in the simpleFMU behaves differently, since the friction was not taken into account here. The oscillation in this model would continue to infinity with the same oscillation amplitude. From this observation the desire of an improvement of this model arises.     ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"simpleFMU = fmiLoad(simpleFMUPath)\n\nfmiInstantiate!(simpleFMU; loggingOn=false)\nfmiInfo(simpleFMU)\n\nvrs = [\"mass.s\", \"mass.v\", \"mass.a\"]\n_, simpleSimData = fmiSimulate(simpleFMU, tStart, tStop; recordValues=vrs, saveat=tSave, reset=false)\nfmiPlot(simpleFMU, vrs, simpleSimData)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"┌ Info: fmi2Unzip(...): Successfully unzipped 28 files at `C:\\Users\\JOHANN~1\\AppData\\Local\\Temp\\fmijl_waNZuu\\SpringPendulum1D`.\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:273\n\n\n#################### Begin information for FMU ####################\n\tModel name:\t\t\tSpringPendulum1D\n\tFMI-Version:\t\t\t2.0\n\tGUID:\t\t\t\t{5030e5a4-87c0-42cf-8779-74ebea1906aa}\n\tGeneration tool:\t\tDymola Version 2020x (64-bit), 2019-10-10\n\tGeneration time:\t\t2021-07-21T05:28:53Z\n\tVar. naming conv.:\t\tstructured\n\tEvent indicators:\t\t0\n\tInputs:\t\t\t\t0\n\tOutputs:\t\t\t0\n\tStates:\t\t\t\t2\n\t\t33554432 [\"mass.s\"]\n\t\t33554433 [\"mass.v\"]\n\tSupports Co-Simulation:\t\ttrue\n\t\tModel identifier:\tSpringPendulum1D\n\t\tGet/Set State:\t\ttrue\n\t\tSerialize State:\ttrue\n\t\tDir. Derivatives:\ttrue\n\t\tVar. com. steps:\ttrue\n\t\tInput interpol.:\ttrue\n\t\tMax order out. der.:\t1\n\tSupports Model-Exchange:\ttrue\n\t\tModel identifier:\tSpringPendulum1D\n\t\tGet/Set State:\t\ttrue\n\t\tSerialize State:\ttrue\n\t\tDir. Derivatives:\ttrue\n##################### End information for FMU #####################\n\n┌ Info: fmi2Load(...): FMU supports both CS and ME, using CS as default if nothing specified.\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:376\n┌ Info: fmi2Load(...): FMU resources location is `file:///C:/Users/JOHANN~1/AppData/Local/Temp/fmijl_waNZuu/SpringPendulum1D/resources`\n└ @ FMI C:\\Users\\Johannes Stoljar\\.julia\\packages\\FMI\\l4qPg\\src\\FMI2.jl:384","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The data from the simualtion of the simpleFMU, are divided into position and velocity data. These data will be needed later to plot the results. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"velSimple = collect(data[2] for data in simpleSimData.saveval)\nposSimple = collect(data[1] for data in simpleSimData.saveval)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"501-element Vector{Float64}:\n 0.5\n 0.5003017486819993\n 0.5012013738965729\n 0.5026995396955733\n 0.5047948922822549\n 0.5074848828353776\n 0.5107669098963398\n 0.5146380970115599\n 0.5190939866425227\n 0.5241313516074874\n 0.5297437685244978\n 0.5359273025527665\n 0.5426739933868466\n ⋮\n 1.6817454106640481\n 1.6860262621029507\n 1.6897211631373532\n 1.6928264987575015\n 1.6953392775960465\n 1.6972564609104965\n 1.698576276381512\n 1.6992975610979462\n 1.6994196580640215\n 1.6989425410415007\n 1.6978667510389276\n 1.6961926527257059","category":"page"},{"location":"examples/simple_hybrid_ME/#NeuralFMU","page":"Simple ME-NeuralFMU","title":"NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/#Loss-function","page":"Simple ME-NeuralFMU","title":"Loss function","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In order to train our model, a loss function must be implemented. The solver of the NeuralFMU can calculate the gradient of the loss function. The gradient descent is needed to adjust the weights in the neural network so that the sum of the error is reduced and the model becomes more accurate.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The loss function in this implmentation consists of the mean squared error (mse) from the real position of the realFMU simulation (posReal) and the position data of the network (posNet). $ mse = \\frac{1}{n} \\sum\\limits_{i=0}^n (posReal[i] - posNet[i])^2 $","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"As it is indicated with the comments, one could also additionally consider the mse from the real velocity (velReal) and the velocity from the network (velNet). The error in this case would be calculated from the sum of both errors.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# loss function for training\nfunction lossSum()\n    solution = neuralFMU(x₀, tStart)\n\n    posNet = collect(data[1] for data in solution.u)\n    #velNet = collect(data[2] for data in solution.u)\n\n    Flux.Losses.mse(posReal, posNet) #+ Flux.Losses.mse(velReal, velNet)\nend","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"lossSum (generic function with 1 method)","category":"page"},{"location":"examples/simple_hybrid_ME/#Callback","page":"Simple ME-NeuralFMU","title":"Callback","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"To output the loss in certain time intervals, a callback is implemented as a function in the following. Here a counter is incremented, every twentieth pass the loss function is called and the average error is printed out.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# callback function for training\nglobal counter = 0\nfunction callb()\n    global counter += 1\n\n    if counter % 20 == 1\n        avgLoss = lossSum()\n        @info \"Loss [$counter]: $(round(avgLoss, digits=5))   Avg displacement in data: $(round(sqrt(avgLoss), digits=5))\"\n    end\nend","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"callb (generic function with 1 method)","category":"page"},{"location":"examples/simple_hybrid_ME/#Structure-of-the-NeuralFMU","page":"Simple ME-NeuralFMU","title":"Structure of the NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"In the following, the topology of the NeuralFMU is constructed. It consists of an input layer, which then leads into the simpleFMU model. The ME-FMU computes the state derivatives for a given system state. Following the simpleFMU is a dense layer that has exactly as many inputs as the model has states (and therefore state derivatives). The output of this layer consists of 16 output nodes and a tanh activation function. The next layer has 16 input and output nodes with the same activation function. The last layer is again a dense layer with 16 input nodes and the number of states as outputs. Here, it is important that no tanh-activation function follows, because otherwise the pendulums state values would be limited to the interval -11.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# NeuralFMU setup\nnumStates = fmiGetNumberOfStates(simpleFMU)\n\nnet = Chain(inputs -> fmiEvaluateME(simpleFMU, inputs),\n            Dense(numStates, 16, tanh),\n            Dense(16, 16, tanh),\n            Dense(16, numStates))","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Chain(\n  var\"#11#12\"(),\n  Dense(2, 16, tanh),                   \u001b[90m# 48 parameters\u001b[39m\n  Dense(16, 16, tanh),                  \u001b[90m# 272 parameters\u001b[39m\n  Dense(16, 2),                         \u001b[90m# 34 parameters\u001b[39m\n)\u001b[90m                   # Total: 6 arrays, \u001b[39m354 parameters, 1.758 KiB.","category":"page"},{"location":"examples/simple_hybrid_ME/#Definition-of-the-NeuralFMU","page":"Simple ME-NeuralFMU","title":"Definition of the NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"The instantiation of the ME-NeuralFMU is done as a one-liner. The FMU (simpleFMU), the structure of the network net, start tStart and end time tStop, the numerical solver Tsit5() and the time steps tSave for saving are specified.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"neuralFMU = ME_NeuralFMU(simpleFMU, net, (tStart, tStop), Tsit5(); saveat=tSave);","category":"page"},{"location":"examples/simple_hybrid_ME/#Plot-before-training","page":"Simple ME-NeuralFMU","title":"Plot before training","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Here the state trajactory of the simpleFMU is recorded. Doesn't really look like a pendulum yet, but the system is random initialized by default. In the later plots, the effect of learning can be seen.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"solutionBefore = neuralFMU(x₀, tStart)\nfmiPlot(simpleFMU, solutionBefore)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/#Training-of-the-NeuralFMU","page":"Simple ME-NeuralFMU","title":"Training of the NeuralFMU","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"For the training of the NeuralFMU the parameters are extracted. The known ADAM optimizer for minimizing the gradient descent is used as further passing parameters. In addition, the previously defined loss and callback function, as well as the number of epochs are passed.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# train\nparamsNet = Flux.params(neuralFMU)\n\noptim = ADAM()\nFlux.train!(lossSum, paramsNet, Iterators.repeated((), 300), optim; cb=callb) ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"┌ Info: Loss [1]: 0.07528   Avg displacement in data: 0.27437\n└ @ Main In[12]:8\n┌ Info: Loss [21]: 0.04558   Avg displacement in data: 0.2135\n└ @ Main In[12]:8\n┌ Info: Loss [41]: 0.04354   Avg displacement in data: 0.20867\n└ @ Main In[12]:8\n┌ Info: Loss [61]: 0.04236   Avg displacement in data: 0.20582\n└ @ Main In[12]:8\n┌ Info: Loss [81]: 0.04193   Avg displacement in data: 0.20478\n└ @ Main In[12]:8\n┌ Info: Loss [101]: 0.04153   Avg displacement in data: 0.20379\n└ @ Main In[12]:8\n┌ Info: Loss [121]: 0.04094   Avg displacement in data: 0.20233\n└ @ Main In[12]:8\n┌ Info: Loss [141]: 0.04003   Avg displacement in data: 0.20008\n└ @ Main In[12]:8\n┌ Info: Loss [161]: 0.03917   Avg displacement in data: 0.19793\n└ @ Main In[12]:8\n┌ Info: Loss [181]: 0.03808   Avg displacement in data: 0.19515\n└ @ Main In[12]:8\n┌ Info: Loss [201]: 0.03616   Avg displacement in data: 0.19016\n└ @ Main In[12]:8\n┌ Info: Loss [221]: 0.0313   Avg displacement in data: 0.17692\n└ @ Main In[12]:8\n┌ Info: Loss [241]: 0.01322   Avg displacement in data: 0.11497\n└ @ Main In[12]:8\n┌ Info: Loss [261]: 0.006   Avg displacement in data: 0.07743\n└ @ Main In[12]:8\n┌ Info: Loss [281]: 0.00461   Avg displacement in data: 0.06793\n└ @ Main In[12]:8","category":"page"},{"location":"examples/simple_hybrid_ME/#Comparison-of-the-plots","page":"Simple ME-NeuralFMU","title":"Comparison of the plots","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Here three plots are compared with each other and only the position of the mass is considered. The first plot represents the simpleFMU, the second represents the realFMU (reference) and the third plot represents the result after training the NeuralFMU. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"# plot results mass.s\nsolutionAfter = neuralFMU(x₀, tStart)\n\nfig = Plots.plot(xlabel=\"t [s]\", ylabel=\"mass position [m]\", linewidth=2,\n                 xtickfontsize=12, ytickfontsize=12,\n                 xguidefontsize=12, yguidefontsize=12,\n                 legendfontsize=8, legend=:topright)\n\nposNeuralFMU = collect(data[1] for data in solutionAfter.u)\n\nPlots.plot!(fig, tSave, posSimple, label=\"SimpleFMU\", linewidth=2)\nPlots.plot!(fig, tSave, posReal, label=\"RealFMU\", linewidth=2)\nPlots.plot!(fig, tSave, posNeuralFMU, label=\"NeuralFMU (300 epochs)\", linewidth=2)\nfig ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/#Continue-training-and-plotting","page":"Simple ME-NeuralFMU","title":"Continue training and plotting","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"As can be seen from the previous figure, the plot of the NeuralFMU has not yet fully converged against the realFMU, so the training of the NeuralFMU is continued. After further training, the plot of NeuralFMU is added to the figure again. The effect of the longer training is well recognizable, since the plot of the NeuralFMU had further converged. ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Flux.train!(lossSum, paramsNet, Iterators.repeated((), 700), optim; cb=callb) \n# plot results mass.s\nsolutionAfter = neuralFMU(x₀, tStart)\nposNeuralFMU = collect(data[1] for data in solutionAfter.u)\nPlots.plot!(fig, tSave, posNeuralFMU, label=\"NeuralFMU (1000 epochs)\", linewidth=2)\nfig ","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"┌ Info: Loss [301]: 0.00412   Avg displacement in data: 0.06419\n└ @ Main In[12]:8\n┌ Info: Loss [321]: 0.00379   Avg displacement in data: 0.0616\n└ @ Main In[12]:8\n┌ Info: Loss [341]: 0.0035   Avg displacement in data: 0.05918\n└ @ Main In[12]:8\n┌ Info: Loss [361]: 0.00322   Avg displacement in data: 0.05673\n└ @ Main In[12]:8\n┌ Info: Loss [381]: 0.00295   Avg displacement in data: 0.05432\n└ @ Main In[12]:8\n┌ Info: Loss [401]: 0.0027   Avg displacement in data: 0.05199\n└ @ Main In[12]:8\n┌ Info: Loss [421]: 0.00246   Avg displacement in data: 0.04961\n└ @ Main In[12]:8\n┌ Info: Loss [441]: 0.00223   Avg displacement in data: 0.04724\n└ @ Main In[12]:8\n┌ Info: Loss [461]: 0.00201   Avg displacement in data: 0.0448\n└ @ Main In[12]:8\n┌ Info: Loss [481]: 0.00179   Avg displacement in data: 0.04236\n└ @ Main In[12]:8\n┌ Info: Loss [501]: 0.00159   Avg displacement in data: 0.03988\n└ @ Main In[12]:8\n┌ Info: Loss [521]: 0.0014   Avg displacement in data: 0.03736\n└ @ Main In[12]:8\n┌ Info: Loss [541]: 0.00121   Avg displacement in data: 0.03481\n└ @ Main In[12]:8\n┌ Info: Loss [561]: 0.00104   Avg displacement in data: 0.03221\n└ @ Main In[12]:8\n┌ Info: Loss [581]: 0.00087   Avg displacement in data: 0.02957\n└ @ Main In[12]:8\n┌ Info: Loss [601]: 0.00072   Avg displacement in data: 0.02692\n└ @ Main In[12]:8\n┌ Info: Loss [621]: 0.00059   Avg displacement in data: 0.02432\n└ @ Main In[12]:8\n┌ Info: Loss [641]: 0.00048   Avg displacement in data: 0.02189\n└ @ Main In[12]:8\n┌ Info: Loss [661]: 0.00039   Avg displacement in data: 0.01966\n└ @ Main In[12]:8\n┌ Info: Loss [681]: 0.00031   Avg displacement in data: 0.01768\n└ @ Main In[12]:8\n┌ Info: Loss [701]: 0.00026   Avg displacement in data: 0.01598\n└ @ Main In[12]:8\n┌ Info: Loss [721]: 0.00021   Avg displacement in data: 0.01447\n└ @ Main In[12]:8\n┌ Info: Loss [741]: 0.00017   Avg displacement in data: 0.01323\n└ @ Main In[12]:8\n┌ Info: Loss [761]: 0.00015   Avg displacement in data: 0.0123\n└ @ Main In[12]:8\n┌ Info: Loss [781]: 0.00013   Avg displacement in data: 0.01157\n└ @ Main In[12]:8\n┌ Info: Loss [801]: 0.00012   Avg displacement in data: 0.01099\n└ @ Main In[12]:8\n┌ Info: Loss [821]: 0.00011   Avg displacement in data: 0.01054\n└ @ Main In[12]:8\n┌ Info: Loss [841]: 0.0001   Avg displacement in data: 0.01017\n└ @ Main In[12]:8\n┌ Info: Loss [861]: 0.0001   Avg displacement in data: 0.00983\n└ @ Main In[12]:8\n┌ Info: Loss [881]: 9.0e-5   Avg displacement in data: 0.00958\n└ @ Main In[12]:8\n┌ Info: Loss [901]: 9.0e-5   Avg displacement in data: 0.00936\n└ @ Main In[12]:8\n┌ Info: Loss [921]: 8.0e-5   Avg displacement in data: 0.00917\n└ @ Main In[12]:8\n┌ Info: Loss [941]: 8.0e-5   Avg displacement in data: 0.00901\n└ @ Main In[12]:8\n┌ Info: Loss [961]: 8.0e-5   Avg displacement in data: 0.00887\n└ @ Main In[12]:8\n┌ Info: Loss [981]: 8.0e-5   Avg displacement in data: 0.00875\n└ @ Main In[12]:8","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"(Image: svg)","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Finally, the FMU is cleaned-up.","category":"page"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"fmiUnload(simpleFMU)","category":"page"},{"location":"examples/simple_hybrid_ME/#Summary","page":"Simple ME-NeuralFMU","title":"Summary","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"Based on the plots, it can be seen that the NeuralFMU is able to adapt the friction model of the realFMU. After 300 runs, the curves do not overlap very well, but this can be achieved by longer training (1000 runs) or a better initialization.","category":"page"},{"location":"examples/simple_hybrid_ME/#Source","page":"Simple ME-NeuralFMU","title":"Source","text":"","category":"section"},{"location":"examples/simple_hybrid_ME/","page":"Simple ME-NeuralFMU","title":"Simple ME-NeuralFMU","text":"[1] Tobias Thummerer, Lars Mikelsons and Josef Kircher. 2021. NeuralFMU: towards structural integration of FMUs into neural networks. Martin Sjölund, Lena Buffoni, Adrian Pop and Lennart Ochel (Ed.). Proceedings of 14th Modelica Conference 2021, Linköping, Sweden, September 20-24, 2021. Linköping University Electronic Press, Linköping (Linköping Electronic Conference Proceedings ; 181), 297-306. DOI: 10.3384/ecp21181297","category":"page"},{"location":"#FMIFlux.jl-Documentation","page":"Introduction","title":"FMIFlux.jl Documentation","text":"","category":"section"},{"location":"#What-is-FMIFlux.jl?","page":"Introduction","title":"What is FMIFlux.jl?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"FMIFlux.jl is a free-to-use software library for the Julia programming language, which offers the ability to setup NeuralFMUs: You can place FMUs (fmi-standard.org) simply inside any feed-forward NN topology and still keep the resulting hybrid model trainable with a standard AD training process.","category":"page"},{"location":"#How-can-I-install-FMIFlux.jl?","page":"Introduction","title":"How can I install FMIFlux.jl?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"open a Julia-Command-Window, activate your preferred environment\ngo to package manager using ] and type add FMIFlux","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> ]\n\n(v.1.5.4)> add FMIFlux","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"If you want to check that everything works correctly, you can run the tests bundled with FMIFlux.jl:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> using Pkg\n\njulia> Pkg.test(\"FMIFlux\")","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Additionally, you can check the version of FMIFlux.jl that you have installed with the status command.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> ]\n(v.1.5.4)> status FMIFlux","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Throughout the rest of the tutorial we assume that you have installed the FMIFlux.jl package and have typed using FMIFlux which loads the package:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> using FMIFlux","category":"page"},{"location":"#How-the-documentation-is-structured?","page":"Introduction","title":"How the documentation is structured?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Having a high-level overview of how this documentation is structured will help you know where to look for certain things. The three main parts of the documentation are :","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"The Examples section gives insight in what is possible with this Library while using short and easily understandable code snippets\nThe Library Functions sections contains all the documentation to the functions provided by this library","category":"page"},{"location":"#What-is-currently-supported-in-FMIFlux.jl?","page":"Introduction","title":"What is currently supported in FMIFlux.jl?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"building and training ME-NeuralFMUs with the default Flux-Front-End\nbuilding and training CS-NeuralFMUs","category":"page"},{"location":"#What-is-under-development-in-FMIFlux.jl?","page":"Introduction","title":"What is under development in FMIFlux.jl?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"different modes for sensitivity estimation\ndocumentation\nmore examples","category":"page"},{"location":"#FMIFlux.jl-Index","page":"Introduction","title":"FMIFlux.jl Index","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"}]
}
